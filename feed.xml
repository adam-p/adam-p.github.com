<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Adam Pritchard's blog and miscellany on adam-p</title><link>https://adam-p.ca/</link><description>Recent content in Adam Pritchard's blog and miscellany on adam-p</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://adam-p.ca/feed.xml" rel="self" type="application/rss+xml"/><item><title>Forwarded Header Sabotage</title><link>https://adam-p.ca/blog/2022/03/forwarded-header-sabotage/</link><pubDate>Wed, 30 Mar 2022 09:37:14 -0400</pubDate><guid>https://adam-p.ca/blog/2022/03/forwarded-header-sabotage/</guid><description>&lt;p>&lt;a href="https://adam-p.ca/blog/2022/03/x-forwarded-for/">We all know by now&lt;/a> that the leftmost values in the &lt;code>X-Forwarded-For&lt;/code> header can be spoofed and only the rightmost IPs – added by your own reverse proxies – can be trusted. The &lt;code>Forwarded&lt;/code> header (&lt;a href="https://datatracker.ietf.org/doc/html/rfc7239">RFC 7239&lt;/a>, 2014) has that same problem, and a new one: If the header is parsed correctly, an attacker can &lt;em>sabotage the whole header&lt;/em>.&lt;/p>
&lt;p>Let’s take a quick trip to understanding how that can happen and how complicated &lt;code>Forwarded&lt;/code> parsing can get. (Think about how you’d parse the header as we go.)&lt;/p>
&lt;h2 id="syntax">
&lt;a class="heading-anchor" href="#syntax">
##
&lt;/a>
Syntax
&lt;/h2>
&lt;p>A simple &lt;code>Forwarded&lt;/code> header might look like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>Forwarded: for=1.1.1.1, For=2.2.2.2
&lt;/code>&lt;/pre>&lt;p>Here’s what a header looks like with an IPv6 value:&lt;/p>
&lt;pre tabindex="0">&lt;code>Forwarded: for=1.1.1.1, FOR="[2001:db8:cafe::17]"
&lt;/code>&lt;/pre>&lt;p>Colons and square brackets are &lt;a href="https://datatracker.ietf.org/doc/html/rfc7230#section-3.2.6">not allowed in a “token”&lt;/a>, so the IPv6 address needs to be quoted. But that means we could have:&lt;/p>
&lt;pre tabindex="0">&lt;code>Forwarded: host="with,comma=equals;semicolon";for=1.1.1.1
&lt;/code>&lt;/pre>&lt;p>So now you can’t just split by comma – you need to be aware of quoted strings as well.&lt;/p>
&lt;p>But characters can also be escaped, so this is also legal:&lt;/p>
&lt;pre tabindex="0">&lt;code>Forwarded: ext="escaped\"quote";for=1.1.1.1
&lt;/code>&lt;/pre>&lt;p>The blessed semicolon-separated parameter names in an entry are “for”, “by”, “host”, and “proto” (case-insensitive). There is allowance for “extensions” using other tokens.&lt;/p>
&lt;p>Some more legal things:&lt;/p>
&lt;ul>
&lt;li>Anything can be escaped, including backslashes: &lt;code>\\&lt;/code>. So don’t just delete all them all.&lt;/li>
&lt;li>Any amount of whitespace around the commas and semicolons.&lt;/li>
&lt;li>There can be multiple instances of the header, and they must be considered a single list, top to bottom.&lt;/li>
&lt;/ul>
&lt;p>And some illegal things:&lt;/p>
&lt;ul>
&lt;li>Can’t have whitespace around the equal sign.&lt;/li>
&lt;li>Can’t have disallowed characters in parameter names (and not quotable).&lt;/li>
&lt;li>Can’t have disallowed characters in a parameter value, if not quoted. (Which mean, for example, that an unquoted IPv6 address is illegal.)&lt;/li>
&lt;li>IPv6 addresses must have square brackets.&lt;/li>
&lt;li>Backslash escaping is only allowed in quoted strings.&lt;/li>
&lt;/ul>
&lt;p>There is only one single library I’ve found that actually correctly parses the header: &lt;a href="https://github.com/lpinca/forwarded-parse">github.com/lpinca/forwarded-parse&lt;/a>.&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> Everything else just does what you were probably thinking after the first couple of steps above:&lt;/p>
&lt;ol>
&lt;li>Split by comma.&lt;/li>
&lt;li>Trim whitespace.&lt;/li>
&lt;li>Split by semicolon.&lt;/li>
&lt;li>Trim the quotes off the value.&lt;/li>
&lt;li>Done.&lt;/li>
&lt;/ol>
&lt;p>Hilariously, this half-assed, RFC-violating parsing is resistant to sabotage and proper parsing is not.&lt;/p>
&lt;h2 id="sabotagehttpswwwyoutubecomwatchvz5rrzdiu1ue">
&lt;a class="heading-anchor" href="#sabotagehttpswwwyoutubecomwatchvz5rrzdiu1ue">
##
&lt;/a>
&lt;a href="https://www.youtube.com/watch?v=z5rRZdiu1UE">Sabotage!&lt;/a>
&lt;/h2>
&lt;p>The &lt;code>Forwarded&lt;/code> header is unique. It is the only header that:&lt;/p>
&lt;ol>
&lt;li>Has untrusted values at the start and trusted values at the end.&lt;/li>
&lt;li>Is official and specified.&lt;/li>
&lt;/ol>
&lt;p>This combination leads to its susceptibility to sabotage, where the whole header – including the trusted part – needs to be discarded because of chicanery in the untrusted part.&lt;/p>
&lt;p>The RFC doesn’t (that I can find) provide any special instructions about salvaging the rest of the header if a single entry (“forwarded-element”) has a syntax error. So, in theory, the whole header needs to be thrown if a spoofer adds, say, &lt;code>f*r=&lt;/code> instead of &lt;code>for=&lt;/code>.&lt;/p>
&lt;p>The sabotage is even more fun with an unclosed double-quote:&lt;/p>
&lt;pre tabindex="0">&lt;code>Forwarded: for="1.1.1.1, for=2.2.2.2, for=3.3.3.3
&lt;/code>&lt;/pre>&lt;p>It’s illegal to have an unclosed quote, so the whole thing is immediately garbage. But even if you wanted to salvage the header… Where do you close the quote? What do you salvage and discard?&lt;/p>
&lt;h2 id="why-is-x-forwarded-for-not-sabotage-able">
&lt;a class="heading-anchor" href="#why-is-x-forwarded-for-not-sabotage-able">
##
&lt;/a>
Why is &lt;code>X-Forwarded-For&lt;/code> not sabotage-able?
&lt;/h2>
&lt;p>Because there’s no spec! People just split by comma, trim, and that’s it. Your trusted reverse proxy will add &lt;code>", 1.1.1.1"&lt;/code> and you don’t really need to care about what comes before that. (Unless you want the leftmost-ish value, but then you’re in the danger zone regardless.)&lt;/p>
&lt;h2 id="mitigations">
&lt;a class="heading-anchor" href="#mitigations">
##
&lt;/a>
Mitigations
&lt;/h2>
&lt;p>“Half-assed, RFC-violating parsing” is the most obvious. If you’re using a rightmost-ish value, you should know if your reverse proxies are going to be quoting things, escaping, etc. – and they probably aren’t. So do a simple comma-splitting and throw away the stuff on the left.&lt;/p>
&lt;p>Doing simple splitting means that you could end up with total garbage in your leftmost values – from spoofing or from valid-but-complicated values. You could probably make your parser more-complex-but-still-not-RFC-compliant by trying to handle quotes and escaping, without discarding everything in the case of bad data. Perhaps your deviation rule could be “no commas allowed in quotes or escaped; they always signal a new entry”. Or just don’t use a leftmost value.&lt;/p>
&lt;p>(Note that differences in parsing at different points – reverse proxies, server, etc. – could result in &lt;a href="https://adam-p.ca/blog/2022/03/x-forwarded-for/">parser mismatch vulnerabilities&lt;/a>.)&lt;/p>
&lt;p>At the reverse proxy level, the obvious mitigation is to discard any existing &lt;code>Forwarded&lt;/code> headers and start fresh, so there are only trusted, well-formed values. If you don’t like the idea of discarding potentially valuable forensic information, maybe your reverse proxy could move the previous header value into some new &lt;code>X-&lt;/code> header before starting fresh.&lt;/p>
&lt;p>A variation on unconditionally discarding the &lt;code>Forwarded&lt;/code> is to check for validity and, if it fails, discard or replace with &lt;code>for=unknown&lt;/code> (which is also per spec). (I &lt;a href="https://github.com/lpinca/forwarded-parse/issues/6">asked lpinca&lt;/a> and this is what he prefers.)&lt;/p>
&lt;p>Another possibility is to have your reverse proxy not discard the existing &lt;code>Forwarded&lt;/code> headers but to add a new one. &lt;em>Maybe&lt;/em> the separate headers could be sanely interpreted separately. But this violates &lt;a href="https://datatracker.ietf.org/doc/html/rfc2616#section-4.2">RFC 2616&lt;/a>, which says “It MUST be possible to combine the multiple header fields into one “field-name: field-value” pair, without changing the semantics of the message”.&lt;/p>
&lt;p>So, I can’t see how it’s possible to adhere to the spec &lt;em>and&lt;/em> retain the existing header value without falling victim to sabotage.&lt;/p>
&lt;h2 id="conclusion">
&lt;a class="heading-anchor" href="#conclusion">
##
&lt;/a>
Conclusion
&lt;/h2>
&lt;p>I wrote a half-assed, RFC-violating &lt;code>Forwarded&lt;/code> parser. Then I decided I wanted it to be “correct” and started looking for better implementations.&lt;/p>
&lt;p>Along the way I found an &lt;a href="https://forum.nginx.org/read.php?29,275880">Nginx forum conversation&lt;/a> from 2017 about adding &lt;code>Forwarded&lt;/code> support. That conversation is interesting and brought the sabotage potential to my attention, but there’s no resolution. It just kind of trails off into hopelessness. (Interestingly, there was strong resistance to discarding the header and thereby losing information.)&lt;/p>
&lt;p>Because my &lt;code>Forwarded&lt;/code> parser is in &lt;a href="https://github.com/realclientip/realclientip-go">a project&lt;/a> that I am hoping will be a reference implementation for getting the “real” client IP, I really wanted the parser itself to be a reference implementation. But, as disappointing as it is, it seems like being spec-compliant is the wrong move. I also can’t assume that a user of the library has the ability to tweak their reverse proxy handling of &lt;code>Forwarded&lt;/code> (I mean, I could state it as a requirement for use of the library, but that’s limiting and error-prone). Documenting the shortcomings seems about as good as it gets.&lt;/p>
&lt;p>(Now I have to figure out how to summarize this in the MDN &lt;code>Forwarded&lt;/code> page update that &lt;a href="https://github.com/mdn/content/pull/13838#issuecomment-1071933350">I’m on the hook for&lt;/a>…)&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Seriously, big kudos to lpinca. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">↩︎&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Symbol and text labels</title><link>https://adam-p.ca/inco/2022/03/symbol-and-text-labels/</link><pubDate>Mon, 28 Mar 2022 10:54:16 -0400</pubDate><guid>https://adam-p.ca/inco/2022/03/symbol-and-text-labels/</guid><description>&lt;p>When I first saw this machine in the doctor’s office (waiting, bored) I thought the duplication of the labels – especially on the power button – was silly. Then I realized that I was being Anglo-centric and that having both enables use by more people, in more places.&lt;/p>
&lt;p>&lt;img src="https://adam-p.ca/img/inco/duplicate-labels.jpg" alt="machine with symbol and text labels">&lt;/p></description></item><item><title>Should you strip the IPv6 zone?</title><link>https://adam-p.ca/blog/2022/03/strip-ipv6-zone/</link><pubDate>Tue, 22 Mar 2022 18:32:38 -0400</pubDate><guid>https://adam-p.ca/blog/2022/03/strip-ipv6-zone/</guid><description>&lt;p>There have recently been three different (but related) contexts where I have asked or been asked that question:&lt;/p>
&lt;ol>
&lt;li>When a reverse proxy is adding the client IP to the &lt;code>X-Forwarded-For&lt;/code> header.&lt;/li>
&lt;li>When the client IP is being used for rate limiting.&lt;/li>
&lt;li>When checking if a client IP is contained in a configured list of ranges/prefixes/CIDRs.&lt;/li>
&lt;/ol>
&lt;p>As I understood more about zones my opinion on this changed. This is an attempt to capture my understanding and where I ended up.&lt;/p>
&lt;h2 id="what-are-ipv6-zones">
&lt;a class="heading-anchor" href="#what-are-ipv6-zones">
##
&lt;/a>
What are IPv6 zones?
&lt;/h2>
&lt;p>Only link-local addresses have zone identifiers&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. The &lt;a href="https://en.wikipedia.org/wiki/IPv6#Link-local_address">Wikipedia definition&lt;/a>&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> of IPv6 link-local address assignment will get us started:&lt;/p>
&lt;blockquote>
&lt;p>All interfaces of IPv6 hosts require a link-local address, which have the prefix fe80::/10. This prefix is combined with a 64-bit suffix, which the host can compute and assign by itself without the presence or cooperation of an external network component like a DHCP server, in a process called &lt;em>link-local address autoconfiguration&lt;/em>.&lt;/p>
&lt;/blockquote>
&lt;p>Link-local addresses are guaranteed to be unique within a “link” – the subnet or network segment the interface is connected to. They have no uniqueness guarantees beyond that, and are not routed beyond the link they belong to.&lt;/p>
&lt;p>Because a single machine might have multiple interfaces on separate links, there might be peers on those separate links with the same link-local address as each other. In order to correctly address those peers, there needs to be an additional qualifier – the zone identifier.&lt;/p>
&lt;p>In addition to a link-local address, each interface gets a zone identifier. If you list the interfaces on your machine, you’ll see link-local addresses like &lt;code>fe80::2934:e0a2:73b1:80da%21&lt;/code> or &lt;code>fe80::9105:d0ed:bbda:9935%eth0&lt;/code> – the part after the percent sign is the zone.&lt;/p>
&lt;p>If you want to connect to the link-local address of another machine you share a link with, it’s not enough to know that its IP is &lt;code>fe80::abcd&lt;/code> – you also need to know which of your interfaces to connect through to get to it. So the zone-qualified address would be like &lt;code>fe80::abcd%eth0&lt;/code>. You then have enough information to connect to that machine.&lt;/p>
&lt;p>It’s important to note that the zone identifier has no meaning outside of the computer that owns the interface. Your IP+zone for a peer machine might be &lt;code>fe80::abcd%eth0&lt;/code>, but the address that machine has for its own interface might be &lt;code>fe80::abcd%wifi0&lt;/code> and its address for your machine would then be &lt;code>fe80::9105:d0ed:bbda:9935%wifi0&lt;/code>.&lt;/p>
&lt;p>Link-local addresses may change on reboot (depending on assignment method and the addresses of other peers on the link). Zones probably won’t change, since they use either the index or name of the interface (or both), but I don’t know if there’s a guarantee of that.&lt;/p>
&lt;p>I’m not sure if zones are directly modifiable, but it’s possible in Linux to change interface names, which should result in a zone change.&lt;/p>
&lt;h2 id="so-should-zones-be-stripped">
&lt;a class="heading-anchor" href="#so-should-zones-be-stripped">
##
&lt;/a>
So should zones be stripped?
&lt;/h2>
&lt;p>As &lt;a href="https://datatracker.ietf.org/doc/html/rfc6874#section-1">RFC 6874&lt;/a> says:&lt;/p>
&lt;blockquote>
&lt;p>It should be noted that zone identifiers have purely local meaning within the node in which they are defined, often being the same as IPv6 interface names. They are completely meaningless for any other node.&lt;/p>
&lt;/blockquote>
&lt;p>That suggests that we should be discarding the zone from addresses that are destined for use anywhere other than the machine that added the zone. But, as we’ve seen, link-local IP addresses are meaningless without the associated zone. So… what’s the right answer?&lt;/p>
&lt;p>Let’s examine the individual contexts I mentioned at the top.&lt;/p>
&lt;h3 id="adding-to-the-x-forwarded-for-header">
&lt;a class="heading-anchor" href="#adding-to-the-x-forwarded-for-header">
###
&lt;/a>
Adding to the &lt;code>X-Forwarded-For&lt;/code> header
&lt;/h3>
&lt;p>(I wrote extensively about the &lt;a href="https://adam-p.ca/blog/2022/03/x-forwarded-for/">hazards of using the &lt;code>X-Forwarded-For&lt;/code> header&lt;/a>. I won’t be repeating it here so we can stay out of the weeds.)&lt;/p>
&lt;p>The main use of the &lt;code>X-Forwarded-For&lt;/code> header is to get the “real” client IP. A secondary use is to see what path a request took.&lt;/p>
&lt;p>The address being appended to the XFF header is the address of the &lt;em>peer&lt;/em> connecting to the machine in question, via its own link-local address. So it’ll look like “remote_fe80_ip%local_zone_id”.&lt;/p>
&lt;p>I think it’s better if the link-local address zone is preserved. Without it, we won’t know which link the address belongs to, so we won’t know which remote machine made the connection.&lt;/p>
&lt;p>Of course, it depends on how the XFF values will actually be used. However, that’s not always known so it seems better to include more information rather than less.&lt;/p>
&lt;h3 id="rate-limiting-by-ip">
&lt;a class="heading-anchor" href="#rate-limiting-by-ip">
###
&lt;/a>
Rate limiting by IP
&lt;/h3>
&lt;p>This is partly an extension of the XFF consideration, as that header is often the source of the IP we use for rate limiting.&lt;/p>
&lt;p>First of all, let’s deal with the possibility of zone spoofing via &lt;code>X-Forwarded-For&lt;/code>. If we include the zone in the key we’re using to rate limit, then the possibility arises of an attacker altering the zone to avoid being rate-limited. I think we can dismiss this concern because if an attacker has the ability to change the zone value, then it will likely be no harder (and probably easier) to change the IP itself.&lt;/p>
&lt;p>If we’re getting the client IP directly from the socket (rather than a header), then we don’t need to worry about spoofing. However, one thing that can go wrong is if the rate-limiting server changes interface names. Then the same client will end up with different “ip%zone” values.&lt;/p>
&lt;p>Which leads us to another potential problem: If the rate-limiting DB is shared between multiple servers, then the same client will have different “ip%zone” values for each server, as they’ll have different interface names/indexes. The rate limit for a single link-local client will effectively be multiplied by the number of servers.&lt;/p>
&lt;p>If we strip the zone, then we lose some specificity, which may result in different machines on different links being rate-limited because they coincidentally have the same link-local IP. While this is true, &lt;a href="https://en.wikipedia.org/wiki/Link-local_address#IPv6">the way link-local address are generated&lt;/a> makes this extremely unlikely. So we can dismiss this as well.&lt;/p>
&lt;p>The shared rate-limiting DB case seems more weighty than the others, so my feeling is that it’s better to strip the zone.&lt;/p>
&lt;h3 id="rangesprefixescidrs">
&lt;a class="heading-anchor" href="#rangesprefixescidrs">
###
&lt;/a>
Ranges/prefixes/CIDRs
&lt;/h3>
&lt;p>I’ll only be considering the “does this prefix contain this IP” use of prefixes (mostly because that’s the use I was looking at in this context).&lt;/p>
&lt;p>This one seems more obvious: If the user includes a zone in a prefix, then the IP prefix should be respected; if there’s no zone in the prefix, then the zone should be stripped from the IP before checking if the prefix contains it.&lt;/p>
&lt;p>To make that more concrete:&lt;/p>
&lt;ul>
&lt;li>With zone: Prefix &lt;code>fe80::%eth0/10&lt;/code> should contain &lt;code>fe80::abcd%eth0&lt;/code> but not &lt;code>fe80::abcd%wifi0&lt;/code>.&lt;/li>
&lt;li>Without zone: Prefix &lt;code>fe80::/10&lt;/code> should contain &lt;code>fe80::abcd%eth0&lt;/code> &lt;em>and&lt;/em> &lt;code>fe80::abcd%wifi0&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>What if the prefix contains a zone but the IP has had the zone already stripped? I don’t think there’s a good answer – neither “contains” or “does not contain” is entirely sane. So that’s another reason not to prematurely strip the zone.&lt;/p>
&lt;h3 id="additional-concerns">
&lt;a class="heading-anchor" href="#additional-concerns">
###
&lt;/a>
Additional concerns
&lt;/h3>
&lt;p>There are other factors involved in all of these considerations.&lt;/p>
&lt;p>The first is that link-local addresses probably shouldn’t be put to any use that requires these considerations. They should probably only be used for low-level automatic network coordination – any direct use of a link-local address would be better served by a &lt;a href="https://en.wikipedia.org/wiki/Unique_local_address">unique local address&lt;/a>. But if you’re writing a general-purpose library or reverse proxy you can’t just say “well, no one should use them” and wash your hands of it.&lt;/p>
&lt;p>Another consideration is the restrictions imposed by your tools and programming language. For example, I recently &lt;a href="https://adam-p.ca/blog/2022/03/go-netip-flaw/">wrote a post&lt;/a> examining Go’s handling of prefixes and IPs with zones. In that case, Go’s address prefix types don’t support zones at all (in some confusing ways). I suspect that zone handling elsewhere is similarly uneven.&lt;/p>
&lt;p>An example of such constraints limiting design is the &lt;a href="https://github.com/caddyserver/caddy/blob/79cbe7bfd06565d0e7ab0717119f78960ed54c08/modules/caddyhttp/reverseproxy/reverseproxy.go#L622-L626">Caddy reverse proxy&lt;/a> stripping the zone &lt;a href="https://github.com/caddyserver/caddy/pull/4507#issuecomment-1075475379">because of&lt;/a> the Go zone-handling limitations.&lt;/p>
&lt;p>Finally, another quote from &lt;a href="https://datatracker.ietf.org/doc/html/rfc6874#section-1">RFC 6874&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>Today, [zone identifiers] are meaningful only when attached to addresses with less than global scope, but it is possible that other uses might be defined in the future.&lt;/p>
&lt;/blockquote>
&lt;p>So even if link-local addresses aren’t important to you now, it doesn’t mean that zones won’t be important to you later. (But, yeah, that’s hand-wavy and not very compelling.)&lt;/p>
&lt;h3 id="conclusions">
&lt;a class="heading-anchor" href="#conclusions">
###
&lt;/a>
Conclusions
&lt;/h3>
&lt;p>Zones should be kept until the point of use, and then the decision to keep or strip them should be based on the specific use of the IP and ramifications of zones to that use.&lt;/p>
&lt;p>This means that reverse proxies should be including the zone in the &lt;code>X-Forwarded-For&lt;/code> header, rate limiters should probably be discarding them, and prefix-contains-IP checks should be based on whether there’s a zone in the prefix. But these are only examples – there are myriad uses of IP addresses, and the particular use will dictate (or at least inform) the fate of the zone.&lt;/p>
&lt;p>Of course, this is all debatable. To see some other other opinions, check out the few comments I got when I asked about this in the &lt;a href="https://old.reddit.com/r/ipv6/comments/tee1gt/should_zone_identifier_be_in_xforwardedfor_ip/">r/ipv6 subreddit&lt;/a>. If you have a differing opinion or know of anyone else having written about this, please let me know.&lt;/p>
&lt;h2 id="appendix-checking-your-own-link-local-addresses-and-zones">
&lt;a class="heading-anchor" href="#appendix-checking-your-own-link-local-addresses-and-zones">
##
&lt;/a>
Appendix: Checking your own link-local addresses and zones
&lt;/h2>
&lt;p>Windows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-no-highlight" data-lang="no-highlight">&lt;span style="display:flex;">&lt;span>$ ipconfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Wireless LAN adapter Wi-Fi:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Connection-specific DNS Suffix . : home
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Link-local IPv6 Address . . . . . : fe80::2934:e0a2:73b1:80da%21
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> IPv4 Address. . . . . . . . . . . : 192.168.1.11
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Subnet Mask . . . . . . . . . . . : 255.255.255.0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Default Gateway . . . . . . . . . : 192.168.1.1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ping fe80::2934:e0a2:73b1:80da%21
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Pinging fe80::2934:e0a2:73b1:80da%21 with 32 bytes of data:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Reply from fe80::2934:e0a2:73b1:80da%21: time&lt;1ms
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ping fe80::2934:e0a2:73b1:80da%nope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Ping request could not find host fe80::2934:e0a2:73b1:80da%nope. Please check the name and try again.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Ubuntu (under WSL1 in this case, so same values):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-no-highlight" data-lang="no-highlight">&lt;span style="display:flex;">&lt;span>$ ip address
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>21: wifi0: &lt;BROADCAST,MULTICAST,UP> mtu 1500 group default qlen 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> link/ieee802.11 64:bc:58:11:a9:f9
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> inet 192.168.1.11/24 brd 192.168.1.255 scope global dynamic
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> valid_lft 244176sec preferred_lft 244176sec
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> inet6 fe80::2934:e0a2:73b1:80da/64 scope link dynamic
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> valid_lft forever preferred_lft forever
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ping fe80::2934:e0a2:73b1:80da%21
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>PING fe80::2934:e0a2:73b1:80da%21(fe80::2934:e0a2:73b1:80da%wifi0) 56 data bytes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>64 bytes from fe80::2934:e0a2:73b1:80da%wifi0: icmp_seq=1 ttl=128 time=0.283 ms
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ping fe80::2934:e0a2:73b1:80da%wifi0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>PING fe80::2934:e0a2:73b1:80da%wifi0(fe80::2934:e0a2:73b1:80da%wifi0) 56 data bytes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>64 bytes from fe80::2934:e0a2:73b1:80da%wifi0: icmp_seq=1 ttl=128 time=0.298 ms
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ping fe80::2934:e0a2:73b1:80da%nope
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ping: fe80::2934:e0a2:73b1:80da%nope: Name or service not known
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>This is not entirely true. Site-local addresses also used zone identifiers, but they were &lt;a href="https://en.wikipedia.org/wiki/Unique_local_address#History">deprecated and went out of use&lt;/a> about 15 years ago. Also, the “zone identifier” is &lt;a href="https://en.wikipedia.org/wiki/IPv6_address#zone_index">sometimes called&lt;/a> a “zone index”. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">↩︎&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Well, one of the definitions. There’s also &lt;a href="https://en.wikipedia.org/wiki/Link-local_address">a whole entry&lt;/a> devoted to link-local addresses. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">↩︎&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>A tiny flaw in Go's netip design</title><link>https://adam-p.ca/blog/2022/03/go-netip-flaw/</link><pubDate>Mon, 21 Mar 2022 18:30:38 -0400</pubDate><guid>https://adam-p.ca/blog/2022/03/go-netip-flaw/</guid><description>&lt;p>Update 2022-03-23: Matt Layher &lt;a href="https://github.com/golang/go/issues/51899">created a Go issue&lt;/a> about this. It’s not getting a lot of traction.&lt;/p>
&lt;hr>
&lt;p>Does this surprise you? (Try it in the &lt;a href="https://go.dev/play/p/4bHXBiBktUH">playground&lt;/a>.)&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-golang" data-lang="golang">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75af00">prefix&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#75af00">netip&lt;/span>&lt;span style="color:#111">.&lt;/span>&lt;span style="color:#75af00">MustParsePrefix&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">"fe80::%zone/10"&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75af00">addr&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#75af00">netip&lt;/span>&lt;span style="color:#111">.&lt;/span>&lt;span style="color:#75af00">MustParseAddr&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">"fe80::1%zone"&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75af00">fmt&lt;/span>&lt;span style="color:#111">.&lt;/span>&lt;span style="color:#75af00">Println&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#75af00">prefix&lt;/span>&lt;span style="color:#111">.&lt;/span>&lt;span style="color:#75af00">Contains&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#75af00">addr&lt;/span>&lt;span style="color:#111">))&lt;/span> &lt;span style="color:#75715e">// ==> false
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Go’s new-as-of-1.18 &lt;a href="https://pkg.go.dev/net/netip">&lt;code>netip&lt;/code> package&lt;/a> is better in every way than the previous &lt;code>net.IP&lt;/code>, etc., but this one design decision will probably burn someone, somewhere, sometime.&lt;/p>
&lt;p>If you pass a prefix with a zone to the older &lt;code>net.ParseCIDR&lt;/code> it returns an error. If you pass a prefix with a zone to the newer &lt;code>netip.ParsePrefix&lt;/code>, it succeeds but silently discards the zone. If you then pass an IP address that is clearly contained by the original prefix – &lt;em>including the zone&lt;/em> – to &lt;code>netip.Prefix.Contains&lt;/code>… it returns false!&lt;/p>
&lt;h2 id="why-is-it-like-this">
&lt;a class="heading-anchor" href="#why-is-it-like-this">
##
&lt;/a>
Why is it like this?
&lt;/h2>
&lt;p>I learned about this from &lt;a href="https://old.reddit.com/r/ipv6/comments/thyhcn/does_it_make_sense_and_is_it_legal_to_have_a_zone/i1by8n5/">a Reddit comment&lt;/a> by &lt;a href="https://github.com/mdlayher">Matt Layher&lt;/a>&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> who worked on &lt;code>netip&lt;/code> (or the original &lt;code>inet.af/netaddr&lt;/code>):&lt;/p>
&lt;blockquote>
&lt;p>For what it’s worth, I helped work on the library that ultimately became Go’s net/netip and we decided we would remove zones in our CIDR prefix parser because we didn’t find any documented usage of a a CIDR like “fe80::%eth0/64” in the wild.&lt;/p>
&lt;/blockquote>
&lt;p>Which is fair, but I don’t think the resulting behaviour is ideal.&lt;/p>
&lt;h2 id="what-do-the-docs-say">
&lt;a class="heading-anchor" href="#what-do-the-docs-say">
##
&lt;/a>
What do the docs say?
&lt;/h2>
&lt;p>The &lt;a href="https://pkg.go.dev/net/netip@go1.18#Prefix.Contains">documentation&lt;/a> for &lt;code>netip.Prefix.Contains&lt;/code> does make clear the behaviour (emphasis added):&lt;/p>
&lt;blockquote>
&lt;p>Contains reports whether the network p includes ip.&lt;/p>
&lt;p>An IPv4 address will not match an IPv6 prefix. A v6-mapped IPv6 address will not match an IPv4 prefix. A zero-value IP will not match any prefix. &lt;strong>If ip has an IPv6 zone, Contains returns false, because Prefixes strip zones.&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;p>It’s good that it’s documented, but… how many people are going to read the doc for that method? Most people who use it are going to know what it means for a prefix (or CIDR) to “contain” an IP address. And many of us will already be familiar with the older &lt;code>net.IPMask.Contains&lt;/code>, which has the one-sentence &lt;a href="https://pkg.go.dev/net@go1.18#IPNet.Contains">documentation&lt;/a>: “Contains reports whether the network includes ip.” And the &lt;a href="https://pkg.go.dev/net/netip@go1.18#ParsePrefix">doc&lt;/a> for &lt;code>netip.ParsePrefix&lt;/code> says nothing about discarding the zone.&lt;/p>
&lt;h2 id="why-do-i-care-about-this-fringe-thing-that-no-one-uses">
&lt;a class="heading-anchor" href="#why-do-i-care-about-this-fringe-thing-that-no-one-uses">
##
&lt;/a>
Why do I care about this fringe thing that no one uses?
&lt;/h2>
&lt;p>I’m writing a &lt;a href="https://github.com/realclientip/realclientip-go">library&lt;/a> that will take a configured list of prefixes/CIDRs/ranges, parse them, and then later check if incoming IPs are contained by them. And whether the IP is contained or not could lead to security-relevant decisions, so the accuracy is important.&lt;/p>
&lt;p>With the older &lt;code>net&lt;/code> package, if the user tried to configure the library to use &lt;code>"fe80::/10%zone"&lt;/code>, the parsing would fail and there would be an immediate error. If I switch to using &lt;code>netip&lt;/code>, the parsing will succeed but then the &lt;code>Contains&lt;/code> checks will return false and the resulting behaviour will be wrong. (The ramifications of that will depend on how the library is being used. It could mean rate-limiting a link-local IP. It could mean using a link-local IP for an access control check where it should instead be an external IP.)&lt;/p>
&lt;p>So even though the Go/netip/netaddr team didn’t find any instance of a link-local-with-zone-prefix “in the wild”, I still need to code (defensively) for the possibility of it.&lt;/p>
&lt;p>To be safe I’m going to have to force the &lt;code>netip&lt;/code> code to behave like the &lt;code>net&lt;/code> code: return an error from the prefix parsing code if there’s a percent sign.&lt;/p>
&lt;h2 id="bonus-ipv4-mapped-ipv6-handling-has-also-changed">
&lt;a class="heading-anchor" href="#bonus-ipv4-mapped-ipv6-handling-has-also-changed">
##
&lt;/a>
Bonus: IPv4-mapped IPv6 handling has also changed
&lt;/h2>
&lt;p>As hinted at in the &lt;code>netip.Prefix.Contains&lt;/code> doc I quoted above…&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-golang" data-lang="golang">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75af00">prefix&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#75af00">netip&lt;/span>&lt;span style="color:#111">.&lt;/span>&lt;span style="color:#75af00">MustParsePrefix&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">"1.0.0.0/8"&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Let's check that it's working as expected
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#75af00">addr&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#75af00">netip&lt;/span>&lt;span style="color:#111">.&lt;/span>&lt;span style="color:#75af00">MustParseAddr&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">"1.1.1.1"&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75af00">fmt&lt;/span>&lt;span style="color:#111">.&lt;/span>&lt;span style="color:#75af00">Println&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#75af00">prefix&lt;/span>&lt;span style="color:#111">.&lt;/span>&lt;span style="color:#75af00">Contains&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#75af00">addr&lt;/span>&lt;span style="color:#111">))&lt;/span> &lt;span style="color:#75715e">// ==> true
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Now let's try the "IPv4-mapped IPv6" representation of the same address
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#75af00">addr&lt;/span> &lt;span style="color:#111">=&lt;/span> &lt;span style="color:#75af00">netip&lt;/span>&lt;span style="color:#111">.&lt;/span>&lt;span style="color:#75af00">MustParseAddr&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">"::ffff:1.1.1.1"&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75af00">fmt&lt;/span>&lt;span style="color:#111">.&lt;/span>&lt;span style="color:#75af00">Println&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#75af00">addr&lt;/span>&lt;span style="color:#111">)&lt;/span> &lt;span style="color:#75715e">// ==> "::ffff:1.1.1.1"
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#75af00">fmt&lt;/span>&lt;span style="color:#111">.&lt;/span>&lt;span style="color:#75af00">Println&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#75af00">prefix&lt;/span>&lt;span style="color:#111">.&lt;/span>&lt;span style="color:#75af00">Contains&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#75af00">addr&lt;/span>&lt;span style="color:#111">))&lt;/span> &lt;span style="color:#75715e">// ==> false!
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// But with the older net.IP and net.NetIP...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#75af00">_&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#75af00">cidr&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#75af00">_&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#75af00">net&lt;/span>&lt;span style="color:#111">.&lt;/span>&lt;span style="color:#75af00">ParseCIDR&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">"1.0.0.0/8"&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75af00">ip&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#75af00">net&lt;/span>&lt;span style="color:#111">.&lt;/span>&lt;span style="color:#75af00">ParseIP&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">"::ffff:1.1.1.1"&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75af00">fmt&lt;/span>&lt;span style="color:#111">.&lt;/span>&lt;span style="color:#75af00">Println&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#75af00">ip&lt;/span>&lt;span style="color:#111">)&lt;/span> &lt;span style="color:#75715e">// ==> "1.1.1.1"
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#75af00">fmt&lt;/span>&lt;span style="color:#111">.&lt;/span>&lt;span style="color:#75af00">Println&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#75af00">cidr&lt;/span>&lt;span style="color:#111">.&lt;/span>&lt;span style="color:#75af00">Contains&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#75af00">ip&lt;/span>&lt;span style="color:#111">))&lt;/span> &lt;span style="color:#75715e">// ==> true!
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>(Try it in the &lt;a href="https://go.dev/play/p/ANR5tJEDohN">playground&lt;/a>.)&lt;/p>
&lt;p>The older &lt;code>net&lt;/code> code would convert IPv4-mapped IPv6 addresses to IPv4 addresses, with the result that they would be contained by IPv4 CIDRs. The new &lt;code>netip&lt;/code> code does &lt;em>not&lt;/em> convert to IPv4, and the resulting address is &lt;em>not&lt;/em> contained by an IPv4 prefix.&lt;/p>
&lt;p>I haven’t yet thought about this enough to form a strong opinion, but it’s good to know.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Who super helpfully answered my Reddit question and I’m totally not taking a swipe at him. To be clear, I still think &lt;code>netip&lt;/code> is great and will be using it wherever I can make 1.18 the minimum Go version. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">↩︎&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>The perils of the “real” client IP</title><link>https://adam-p.ca/blog/2022/03/x-forwarded-for/</link><pubDate>Fri, 04 Mar 2022 13:20:10 -0500</pubDate><guid>https://adam-p.ca/blog/2022/03/x-forwarded-for/</guid><description>&lt;h2 id="summary">
&lt;a class="heading-anchor" href="#summary">
##
&lt;/a>
Summary
&lt;/h2>
&lt;p>This post ended up being incredibly &lt;del>long&lt;/del> comprehensive. I’m afraid that many people won’t read enough to get everything that’s important, so here are the key points:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>When deriving the “real client IP address” from the &lt;code>X-Forwarded-For&lt;/code> header, use the rightmost IP in the list.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The leftmost IP in the XFF header is commonly considered to be “closest to the client” and “most real”, but it’s trivially spoofable. Don’t use it for anything even close to security-related.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>When choosing the rightmost XFF IP, make sure to use the &lt;em>last&lt;/em> instance of that header.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Using special “true client IPs” set by reverse proxies (like &lt;code>X-Real-IP&lt;/code>, &lt;code>True-Client-IP&lt;/code>, etc.) &lt;em>can&lt;/em> be good, but it depends on the a) how the reverse proxy actually sets it, b) whether the reverse proxy sets it if it’s already present/spoofed, and c) how you’ve configured the reverse proxy (sometimes).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Any header not specifically set by your reverse proxy cannot be trusted. For example, you &lt;em>must not&lt;/em> check the &lt;code>X-Real-IP&lt;/code> header if you’re not behind Nginx or something else that always sets it, because you’ll be reading a spoofed value.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A lot of rate limiter implementations are using spoofable IPs and are vulnerable to rate limiter escape and memory exhaustion attacks.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;em>If you use the “real client IP” anywhere in your code or infrastructure, you need to go check right now how you’re deriving it.&lt;/em>&lt;/p>
&lt;p>This is all explained in detail below, so keep reading. It’s a weird, scary, bumpy ride.&lt;/p>
&lt;h2 id="introduction">
&lt;a class="heading-anchor" href="#introduction">
##
&lt;/a>
Introduction
&lt;/h2>
&lt;p>The state of getting the “real client IP” using &lt;code>X-Forwarded-For&lt;/code> and other HTTP headers is terrible. It’s done incorrectly, inconsistently, and the result is used inappropriately. This leads to security vulnerabilities in a variety of projects, and will certainly lead to more in the future.&lt;/p>
&lt;p>After thinking about rate limiters for a while, I started worrying about their IPv6 handling. I &lt;a href="https://adam-p.ca/blog/2022/02/ipv6-rate-limiting/">wrote a post&lt;/a> detailing how bad IPv6 rate limiting can and does lead to rate limiter escape and memory exhaustion. Then I moved on to worrying about how rate limiters determine what IP to rate-limit when they’re behind a load balancer (or any reverse proxy). As you’ll see, the situation is bad.&lt;/p>
&lt;p>But this isn’t just about rate limiters. If you ever touch code that looks at the &lt;code>X-Forwarded-For&lt;/code> header, or if you use someone else’s code that uses or gives you the “real client IP”, then you absolutely need to be savvy and wary. This post will help you get there.&lt;/p>
&lt;p>&lt;strong>NOTE:&lt;/strong> Portions of this are redacted as I’m trying to disclose responsibly to the affected projects. Those portions will be added in as that projects choose to make the issues public. (So check back later!)&lt;/p>
&lt;h2 id="it-cant-be-that-hard-to-get-the-real-client-ip-right">
&lt;a class="heading-anchor" href="#it-cant-be-that-hard-to-get-the-real-client-ip-right">
##
&lt;/a>
It can’t be that hard to get the real client IP, right?
&lt;/h2>
&lt;p>There are many reasons why web services are interested in the IP address of their clients: geographical stats, geo-targeting, auditing, rate-limiting, abuse-blocking, session history, etc.&lt;/p>
&lt;p>When a client directly connects to a server, the server can see the IP address of the client. If the client connects through one or more proxies (of any kind: forward, reverse, load balancer, API gateway, TLS offloading, IP access control, etc.), then the server only directly sees the IP address of the final proxy used by the client connection.&lt;/p>
&lt;p>In order to pass the original IP address on to the server, there are several headers in common use:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For">&lt;code>X-Forwarded-For&lt;/code>&lt;/a> is a list of comma-separated IPs that gets appended to by each traversed proxy&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. The idea is that the first IP (added by the first proxy) is the true client IP. Each subsequent IP is another proxy along the path. The last proxy’s IP is &lt;em>not&lt;/em> present (because proxies don’t add their own IPs, and because it connects directly to the server so its IP will be directly available anyway). We’re going to talk about this a lot, so it’ll be abbreviated to “XFF”.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Forwarded">&lt;code>Forwarded&lt;/code>&lt;/a> is the most official but seemingly least-used header. We look at it in more detail &lt;a href="#rfc-7239-forwarded-http-extension-june-2014">below&lt;/a>, but it’s really just a fancier version of XFF that has the same problems that we’re about to discuss.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>There are also special single-IP headers like &lt;code>X-Real-IP&lt;/code> (Nginx), &lt;code>CF-Connecting-IP&lt;/code> (Cloudflare), or &lt;code>True-Client-IP&lt;/code> (Cloudflare and Akamai). We’ll talk more about these below, but they’re not the main focus of this post.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="pitfalls">
&lt;a class="heading-anchor" href="#pitfalls">
##
&lt;/a>
Pitfalls
&lt;/h2>
&lt;p>Before talking about how to use XFF properly, we’ll talk about the many ways that using &lt;code>X-Forwarded-For&lt;/code> can go wrong.&lt;/p>
&lt;h3 id="headers-are-untrustworthy">
&lt;a class="heading-anchor" href="#headers-are-untrustworthy">
###
&lt;/a>
Headers are untrustworthy
&lt;/h3>
&lt;p>First of all, and most importantly, you must always be aware that &lt;em>any XFF IPs that were added (or appear to have been added) by any proxy not controlled by you are completely unreliable&lt;/em>. Any proxy could have added, removed, or modified the header any way it wants. The client as well could have initially set the header to anything it wants to get the spoof-ball rolling. For example, if you make this request to an AWS load balancer&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>…&lt;/p>
&lt;pre tabindex="0">&lt;code>curl -X POST https://my.load.balanced.domain/login -H "X-Forwarded-For: 1.2.3.4, 11.22.33.44"
&lt;/code>&lt;/pre>&lt;p>…your server behind the load balancer will get this:&lt;/p>
&lt;pre tabindex="0">&lt;code>X-Forwarded-For: 1.2.3.4, 11.22.33.44, &lt;actual client IP>
&lt;/code>&lt;/pre>&lt;p>And this:&lt;/p>
&lt;pre tabindex="0">&lt;code>curl -X POST https://my.load.balanced.domain/login -H "X-Forwarded-For: oh, hi,,127.0.0.1,,,,"
&lt;/code>&lt;/pre>&lt;p>…will give you this:&lt;/p>
&lt;pre tabindex="0">&lt;code>X-Forwarded-For: oh, hi,,127.0.0.1,,,,, &lt;actual client IP>
&lt;/code>&lt;/pre>&lt;p>As you can see, everything already present is just passed through, unchanged and unvalidated. The final, actual IP is just appended to whatever is already there.&lt;/p>
&lt;p>(In addition to curl’ing and custom clients, there is also at least one &lt;a href="https://chrome.google.com/webstore/detail/x-forwarded-for-header/hkghghbnihliadkabmlcmcgmffllglin">Chrome extension&lt;/a> that lets you set the XFF header in browser requests. But &lt;em>how&lt;/em> you can set the header doesn’t really matter to us here, only that an attacker can do it.)&lt;/p>
&lt;h3 id="multiple-headers">
&lt;a class="heading-anchor" href="#multiple-headers">
###
&lt;/a>
Multiple headers
&lt;/h3>
&lt;p>According to the &lt;a href="https://datatracker.ietf.org/doc/html/rfc2616#section-4.2">HTTP/1.1 RFC (2616)&lt;/a>&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>:&lt;/p>
&lt;blockquote>
&lt;p>Multiple message-header fields with the same field-name MAY be present in a message if and only if the entire field-value for that header field is defined as a comma-separated list [i.e., #(values)]. It MUST be possible to combine the multiple header fields into one “field-name: field-value” pair, without changing the semantics of the message, by appending each subsequent field-value to the first, each separated by a comma. The order in which header fields with the same field-name are received is therefore significant to the interpretation of the combined field value, and thus a proxy MUST NOT change the order of these field values when a message is forwarded.&lt;/p>
&lt;/blockquote>
&lt;p>That applies to XFF, as it is a comma-separated list. This can make getting the rightmost (or even leftmost) IP error-prone.&lt;/p>
&lt;p>For example, Go has three ways to get a header value:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://pkg.go.dev/net/http#Header.Get">&lt;code>http.Header.Get(headerName)&lt;/code>&lt;/a> returns the first header value as a string.&lt;/li>
&lt;li>&lt;a href="https://pkg.go.dev/net/http#Header.Values">&lt;code>http.Header.Values(headerName)&lt;/code>&lt;/a> returns a slice (array) of strings with the values of all instances of the header &lt;code>headerName&lt;/code>. (&lt;code>headerName&lt;/code> is canonicalized before lookup.)&lt;/li>
&lt;li>&lt;code>http.Header&lt;/code> is a &lt;code>map[string][]string&lt;/code> and can be accessed directly. (The map keys are canonicalized header names.) This is similar to using &lt;code>Values&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>So here’s the attack:&lt;/p>
&lt;ol>
&lt;li>Eve makes a request with &lt;em>two&lt;/em> spoofed XFF headers.&lt;/li>
&lt;li>Your reverse proxy adds Eve’s true IP to the end of the &lt;em>second&lt;/em> XFF header, per the RFC requirements.&lt;/li>
&lt;li>You call &lt;code>req.Header.Get("X-Forwarded-For")&lt;/code> and get the first header. You split it up and take the rightmost.&lt;/li>
&lt;li>You have chosen a spoofed IP. You treat it as trustworthy. Bad things result.&lt;/li>
&lt;/ol>
&lt;p>Unlike Go, Twisted’s method for getting a single header value &lt;a href="https://github.com/twisted/twisted/blob/ebb2d360070e468981377b917e3a728ff4e6c7f6/src/twisted/web/http.py#L1068">returns the &lt;em>last&lt;/em> value&lt;/a>. (Why is there no standard, common, accepted behaviour for this?) This avoids the above attack, but it can cause a different (less likely) problem: If you’re using the rightmost-ish algorithm (described &lt;a href="#algorithms">below&lt;/a>), you need to go backwards from the right looking for the first untrusted IP. But what if one of your reverse proxies has added a new header instead of appending (a valid thing to do, per the RFC)? Now the IP that you want is nowhere to be found in the last header – it’s full of trusted reverse proxy IPs and the real IP is in a previous instance of the XFF header.&lt;/p>
&lt;p>There might be a subtle, hypothetical attack possible here:&lt;/p>
&lt;ol>
&lt;li>You have (at least) two reverse proxies that you trust.&lt;/li>
&lt;li>The second of those reverse proxies doesn’t like super long headers, so it creates a new one rather than appending if the XFF header is too long.&lt;/li>
&lt;li>Eve knows this. And she wants to hide her IP from you.&lt;/li>
&lt;li>Eve spoofs a long XFF in her request to you.&lt;/li>
&lt;li>Your first reverse proxy adds her true IP to the XFF header.&lt;/li>
&lt;li>Your second reverse proxy doesn’t like how long that header is, so it creates a new one. The header value is the IP of the first reverse proxy.&lt;/li>
&lt;li>Your server software gets the last header and it has only a single IP, belonging to your first reverse proxy.&lt;/li>
&lt;li>What does your logic do? Use that IP? Treat it as special because it’s private/trusted? Panic because it’s impossible that this IP should be trusted?&lt;/li>
&lt;/ol>
&lt;p>Note that when I tested with a server behind AWS ALB I found that ALB had already concatenated the XFF headers. So that’s good. I have no idea if other reverse proxies do the same, but I bet there’s no real consistency.&lt;/p>
&lt;p>The best thing to do is merge all of the XFF headers yourself.&lt;/p>
&lt;p>(It is worthwhile asking – and checking – to make sure reverse proxies append to the correct header, because appending to the wrong header would wreck the trustworthiness of taking the rightmost. I have only checked AWS ALB and Cloudflare, and they’re doing it right. If anyone discovers something doing it wrong, please let me know.)&lt;/p>
&lt;p>[2022-03-04: I created a &lt;a href="https://github.com/golang/go/issues/51493">Go issue&lt;/a> arguing for a change to the behaviour of &lt;code>http.Header.Get&lt;/code>. Not with any real expectation of a change, but we’ll see.]&lt;/p>
&lt;h3 id="private-ips">
&lt;a class="heading-anchor" href="#private-ips">
###
&lt;/a>
Private IPs
&lt;/h3>
&lt;p>Even in completely non-malicious scenarios, any of the XFF IPs – but especially the leftmost – may be a &lt;a href="https://en.wikipedia.org/wiki/Private_network">private/internal IP address&lt;/a>. If the client first connects to an internal proxy, it may add the private IP of the client to the XFF header. This address is never going to be useful to you.&lt;/p>
&lt;h3 id="splitting-the-ips">
&lt;a class="heading-anchor" href="#splitting-the-ips">
###
&lt;/a>
Splitting the IPs
&lt;/h3>
&lt;p>Because &lt;code>X-Forwarded-For&lt;/code> isn’t an official standard, there’s no formal spec for it. Most examples show the IP addresses comma-space (&lt;code>", "&lt;/code>) separated, but the space isn’t strictly required. (For example, the &lt;a href="https://datatracker.ietf.org/doc/html/rfc2616#section-4.2">HTTP/1.1 RFC&lt;/a> says that headers &lt;em>like&lt;/em> XFF are simply “comma separated”.) &lt;em>Most&lt;/em> of the code I looked at splits by just comma and then trims the value, but at least &lt;a href="https://github.com/go-chi/httprate/blob/463924d478ea0c19de7265b97371a59a7ebf5fdd/httprate.go#L33">one I found&lt;/a> looks for comma-space.&lt;/p>
&lt;p>When testing, it looks to me like AWS ALB uses comma-space when adding an IP, but Cloudflare uses only a comma.&lt;/p>
&lt;h3 id="unencrypted-data-is-always-untrustworthy">
&lt;a class="heading-anchor" href="#unencrypted-data-is-always-untrustworthy">
###
&lt;/a>
Unencrypted data is always untrustworthy
&lt;/h3>
&lt;p>This should go without saying, but if you’re receiving HTTP-not-S requests, then anyone could have modified the headers before they got to you. This is worth mentioning because an interloper can’t mess with the “rightmost-ish” approach (described below) because they can’t mess with the IP of the final connection from the internet to your reverse proxy or server.&lt;/p>
&lt;p>So just encrypt your traffic, okay?&lt;/p>
&lt;h3 id="other-headers-x-client-ip-true-client-ip-might-be-present-and-spoofed">
&lt;a class="heading-anchor" href="#other-headers-x-client-ip-true-client-ip-might-be-present-and-spoofed">
###
&lt;/a>
Other headers (&lt;code>X-Client-IP&lt;/code>, &lt;code>True-Client-IP&lt;/code>) might be present and spoofed
&lt;/h3>
&lt;p>Some reverse proxies remove any unexpected or unwanted headers, but some (like AWS ALB) don’t. So an attacker can set headers like &lt;code>X-Client-IP&lt;/code> and &lt;code>True-Client-IP&lt;/code> straight through to your server. You need to not get fooled into using them if your reverse proxy didn’t specifically set them for you.&lt;/p>
&lt;h3 id="trying-to-learn-about-x-forwarded-for">
&lt;a class="heading-anchor" href="#trying-to-learn-about-x-forwarded-for">
###
&lt;/a>
Trying to learn about &lt;code>X-Forwarded-For&lt;/code>
&lt;/h3>
&lt;p>Trying to educate yourself about XFF is, unfortunately, also difficult.&lt;/p>
&lt;p>MDN Web Docs are usually the gold standard for stuff like this, but &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For">the page about XFF&lt;/a> doesn’t mention these risks at all; it says “the right-most IP address is the IP address of the most recent proxy and the left-most IP address is the IP address of the originating client” with no caveat. The &lt;a href="https://en.wikipedia.org/wiki/X-Forwarded-For">Wikipedia entry&lt;/a> is much better: “Since it is easy to forge an X-Forwarded-For field the given information should be used with care. The right-most IP address is always the IP address that connects to the last proxy, which means it is the most reliable source of information.”&lt;/p>
&lt;p>[2022-03-09: Created &lt;a href="https://github.com/mdn/content/issues/13703">an issue&lt;/a> for the MDN documentation. 2022-03-19: I rewrote the page, PR’d it, and the change is live now. You can see a PDF of the &lt;a href="https://adam-p.ca/misc/MDN-XFF.pdf">original page here&lt;/a>. Now to fix the &lt;code>Forwarded&lt;/code> page…]&lt;/p>
&lt;p>Other sources are similarly variable. Some say nothing whatsoever about the possibility of the header being spoofed or the presence of private addresses (&lt;a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html">1&lt;/a>, &lt;a href="https://techcommunity.microsoft.com/t5/iis-support-blog/how-to-use-x-forwarded-for-header-to-log-actual-client-ip/ba-p/873115">2&lt;/a>, &lt;a href="https://www.geeksforgeeks.org/http-headers-x-forwarded-for/">3&lt;/a>, &lt;a href="https://developers.cloudflare.com/fundamentals/get-started/http-request-headers/">4&lt;/a>, &lt;a href="https://www.keycdn.com/blog/x-forwarded-for-cdn">5&lt;/a>). Others do a pretty good job of mentioning the risks (&lt;a href="https://totaluptime.com/kb/prevent-x-forwarded-for-spoofing-or-manipulation/">6&lt;/a>, &lt;a href="https://docs.fastly.com/signalsciences/faq/real-client-ip-addresses/#x-forwarded-for-header-configuration">7&lt;/a>, &lt;a href="https://datatracker.ietf.org/doc/html/rfc7239#section-8.1">8&lt;/a>), but sometimes you have to read pretty deeply to get to the warnings.&lt;/p>
&lt;h2 id="avoiding-those-pits">
&lt;a class="heading-anchor" href="#avoiding-those-pits">
##
&lt;/a>
Avoiding those pits
&lt;/h2>
&lt;p>Let’s make a few baseline statements:&lt;/p>
&lt;ol>
&lt;li>Using an IP in the private address space as the “real” client IP is never the right choice&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/li>
&lt;li>Using a value that’s not actually an IP address is never the right choice.&lt;/li>
&lt;li>In the absence of chicanery, the leftmost non-private, non-invalid IP is the closest we can come to the “real” client IP. (Henceforth, “leftmost-ish”.)&lt;/li>
&lt;li>The only client IP that we can &lt;em>trust&lt;/em> is the first one added by a (reverse) proxy that we control. (Henceforth, “rightmost-ish”.)&lt;/li>
&lt;/ol>
&lt;p>The leftmost-ish is usually going to be the most “real”, while the rightmost-ish is going to be the most trustworthy. So which IP should you use? It depends on what you’re going to do with it.&lt;/p>
&lt;p>If you’re going to do something security-related, you need to use the IP you trust – the rightmost-ish. The obvious example here is rate-limiting. If you use the leftmost-ish IP for this, an attacker can just spoof a different XFF prefix value with each request and &lt;em>completely avoid being limited&lt;/em>.&lt;/p>
&lt;p>Additionally, they might be able to exhaust your server memory by forcing you to store too many individual entries – one for each fake IP. It may seem hard to believe that storing IP addresses in memory could lead to exhaustion – especially if they’re stored in a cache with finite time-to-live, but keep in mind:&lt;/p>
&lt;ul>
&lt;li>The attacker won’t be limited to the 4 billion IPv4 addresses. They can use all the bazillion IPv6 addresses, if the limiter isn’t &lt;a href="https://adam-p.ca/blog/2022/02/ipv6-rate-limiting/">smart about prefixes&lt;/a>.&lt;/li>
&lt;li>Since many limiters don’t check for valid IPs, an attacker can use any random string it wants.&lt;/li>
&lt;li>Also note that these strings can be &lt;em>big&lt;/em>; for example, Go’s &lt;a href="https://pkg.go.dev/net/http#pkg-constants">default header block size limit&lt;/a> is 1MB. That means a single random-string “IP” can be almost 1MB. That means adding 1MB of memory usage &lt;em>per request&lt;/em>.&lt;/li>
&lt;/ul>
&lt;p>It still won’t be feasible for all attackers and configurations, but it shouldn’t be dismissed without consideration.&lt;/p>
&lt;p>Or an attacker can force you to rate-limit/block the IP addresses of other users. They can supply a real – but not their – IP address, and you’ll eventually be fooled into rate-limiting it. (And if you’re using the “real” IP for abuse reports, you might end up complaining about the wrong person.)&lt;/p>
&lt;p>The downside to using the rightmost-ish IP for rate-limiting is that you might block a proxy IP that’s not actually a source of abuse but is just used by a bunch of different clients and you would have realized that if you’d just used the leftmost-ish instead. Yeah, well. That doesn’t seem super likely, and it’s still infinitely more acceptable than allowing attackers to trivially bypass your rate limiter and crash your server.&lt;/p>
&lt;p>If you’re doing something not obviously security-related… Think hard about your use case. Let’s say you just want to do an &lt;a href="https://en.wikipedia.org/wiki/Internet_geolocation">IP-geolocation&lt;/a> lookup for your stats. &lt;em>Probably&lt;/em> the leftmost-ish IP is what you want. The vast majority of your users won’t be doing any header spoofing, and the geolocation of random internet proxies are no good to you, so you’ll &lt;em>probably&lt;/em> get the best results with the IP closest to the user.&lt;/p>
&lt;p>On the other hand, you might want to consider how many internet-proxy-using users you expect to have. Probably few enough that it won’t hurt your stats if you geolocate the wrong thing. And is there a way an attacker could hurt you by purposely skewing your geo stats? Probably not, but take a moment to really think about it.&lt;/p>
&lt;p>So &lt;em>be careful&lt;/em> when writing your “GetRealClientIP(request)” function. Make sure it has a big warning comment about how it should be used. Or maybe write two functions: “GetUntrustworthyRealClientIP(request)” and “GetTrustworthyButLessRealClientIP(request)”. Which are horrible names. Maybe just pass a flag instead. Anyway, the point is to prevent the callers of your functions from having any confusion about the nature of the result.&lt;/p>
&lt;p>Also be careful when using the results of that function. It’s easy to write code that gets the leftmost-ish IP to do some geo lookup and then later decide you also need to do rate limiting… so you might as well use the same “realClientIP” variable! Oops. This might be a good time &lt;a href="https://www.joelonsoftware.com/2005/05/11/making-wrong-code-look-wrong/">to make wrong code look wrong&lt;/a>.&lt;/p>
&lt;p>And remember that the final proxy IP – or the address of the client if it’s connecting directly – is &lt;em>not&lt;/em> in the XFF header. You need to look at your request connection information for that. (&lt;code>http.Request.RemoteAddr&lt;/code> in Go, the &lt;code>REMOTE_ADDR&lt;/code> environment variable for many CGI servers, etc.)&lt;/p>
&lt;h3 id="algorithms">
&lt;a class="heading-anchor" href="#algorithms">
###
&lt;/a>
Algorithms
&lt;/h3>
&lt;p>When reading this, remember that the final proxy IP is not in the XFF list – it’s the &lt;code>RemoteAddr&lt;/code>. Also note that &lt;code>RemoteAddr&lt;/code> might have the form &lt;code>ip:port&lt;/code>, depending on your platform (like it does in Go) – sure sure to only use the IP part.&lt;/p>
&lt;h4 id="first-collect-all-of-the-ips">
&lt;a class="heading-anchor" href="#first-collect-all-of-the-ips">
####
&lt;/a>
First: collect all of the IPs
&lt;/h4>
&lt;p>Make a single list of all the IPs in all of the &lt;code>X-Forwarded-For&lt;/code> headers.&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> Also have the &lt;code>RemoteAddr&lt;/code> available.&lt;/p>
&lt;h4 id="second-decide-what-your-security-needs-are">
&lt;a class="heading-anchor" href="#second-decide-what-your-security-needs-are">
####
&lt;/a>
Second: decide what your security needs are
&lt;/h4>
&lt;p>Default to using the rightmost-ish approach. Only use the leftmost-ish if you have to, and make sure you do so carefully.&lt;/p>
&lt;h4 id="leftmost-ish-the-closest-to-the-real-ip-but-utterly-untrustworthy">
&lt;a class="heading-anchor" href="#leftmost-ish-the-closest-to-the-real-ip-but-utterly-untrustworthy">
####
&lt;/a>
Leftmost-ish: the closest to the “real IP”, but utterly untrustworthy
&lt;/h4>
&lt;p>If your server is directly connected to the internet, there might be an XFF header or there might not be (depending on whether the client used a proxy). If there &lt;em>is&lt;/em> an XFF header, pick the leftmost IP address that is a &lt;em>valid, non-private&lt;/em> IPv4 or IPv6 address. If there is no XFF header, use the &lt;code>RemoteAddr&lt;/code>.&lt;/p>
&lt;p>If your server is behind one or more reverse proxies, pick the leftmost XFF IP address that is a &lt;em>valid, non-private&lt;/em> IPv4 or IPv6 address. (If there’s no XFF header, you need to fix your network configuration problem &lt;em>right now&lt;/em>.)&lt;/p>
&lt;p>And never forget the security implications!&lt;/p>
&lt;h4 id="rightmost-ish-the-only-useful-ip-you-can-trust">
&lt;a class="heading-anchor" href="#rightmost-ish-the-only-useful-ip-you-can-trust">
####
&lt;/a>
Rightmost-ish: the only useful IP you can trust
&lt;/h4>
&lt;p>If your server is directly connected to the internet, the XFF header cannot be trusted, period. Use the &lt;code>RemoteAddr&lt;/code>.&lt;/p>
&lt;p>If your server is behind one or more reverse proxies and not directly accessible from the internet, you need to know either the IP addresses of those reverse proxies or the number of them that the request will pass through. We’ll call these the “trusted proxy IPs” and “trusted proxy count”. (Using “trusted proxy IPs” is preferable, for reasons described in the &lt;a href="#network-architecture-changes">“network architecture changes” section&lt;/a>.)&lt;/p>
&lt;p>The trusted proxy IPs or trusted proxy count will tell you how far from the right of the XFF header you need to check before you find the first IP that doesn’t belong to one of your reverse proxies. This IP was added by your first trusted proxy and is therefore the only IP you can trust. Use it.&lt;/p>
&lt;p>(Notice that I’m not saying “valid, non-private IP” here. It is tempting to do so, just to be extra safe, and I won’t blame you if you do, but if you can’t trust your own reverse proxy to add the proper IP, then you have a bigger problem.)&lt;/p>
&lt;p>Again, if you’re behind one or more reverse proxies and there’s no XFF header, you need to immediately figure out how people are connecting to your server directly.&lt;/p>
&lt;h5 id="tentative-variation-rightmost-non-private-ip">
&lt;a class="heading-anchor" href="#tentative-variation-rightmost-non-private-ip">
#####
&lt;/a>
Tentative variation: rightmost non-private IP
&lt;/h5>
&lt;p>If all of your reverse proxies are in the same private IP space as your server, I &lt;em>think&lt;/em> it’s okay to use the rightmost non-private IP rather than using “trusted proxy IPs” or “trusted proxy count”. This is equivalent to adding all private IP ranges to your “trusted proxy IPs” list.&lt;/p>
&lt;p>An example where this &lt;em>doesn’t&lt;/em> work is if you’re behind an external reverse proxy service, like Cloudflare – it’s not in your private address space.&lt;/p>
&lt;h2 id="falling-into-those-pits">
&lt;a class="heading-anchor" href="#falling-into-those-pits">
##
&lt;/a>
Falling into those pits
&lt;/h2>
&lt;p>Let’s look at real-world examples!&lt;/p>
&lt;p>Warning: I got a little carried away here. I was only intending to look at a couple of projects that I was familiar with, but the hit-rate of dangerous-use-of-leftmost was so high that I just kept searching. (And there were some interesting and educational aspects even when it was done right.)&lt;/p>
&lt;p>(If a tool or service isn’t mentioned here, it’s because I either didn’t look at it or couldn’t find enough information about it. I included all successes as well as failures.)&lt;/p>
&lt;h3 id="cloudflare-nginx-apache">
&lt;a class="heading-anchor" href="#cloudflare-nginx-apache">
###
&lt;/a>
Cloudflare, Nginx, Apache
&lt;/h3>
&lt;p>Let’s start with some good news.&lt;/p>
&lt;p>&lt;a href="https://support.cloudflare.com/hc/en-us/articles/206776727-Understanding-the-True-Client-IP-Header">Cloudflare adds&lt;/a> the &lt;code>CF-Connecting-IP&lt;/code> header to all requests that pass through it; it adds &lt;code>True-Client-IP&lt;/code> as a synonym for Enterprise users who require backwards compatibility. The value for these headers is a single IP address. The &lt;a href="https://developers.cloudflare.com/fundamentals/get-started/http-request-headers/">fullest description&lt;/a> of these headers that I could find makes it &lt;em>sound&lt;/em> like they are just using the leftmost XFF IP, but the example was sufficiently incomplete that I tried it out myself. Happily, it looks like they’re actually using the rightmost-ish.&lt;/p>
&lt;p>Nginx offers a not-enabled-by-default module that &lt;a href="https://nginx.org/en/docs/http/ngx_http_realip_module.html">adds the &lt;code>X-Real-IP&lt;/code> header&lt;/a>. This is also a single IP. When properly and fully configured&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>, it also uses the rightmost IP that isn’t on the “trusted” list. So, the rightmost-ish IP. Also good.&lt;/p>
&lt;p>Similarly, when configured to look at &lt;code>X-Forwarded-For&lt;/code>, Apache’s &lt;a href="https://httpd.apache.org/docs/trunk/mod/mod_remoteip.html">mod_remoteip&lt;/a> picks the rightmost untrusted IP to set into &lt;code>REMOTE_ADDR&lt;/code>.&lt;/p>
&lt;h3 id="akamai">
&lt;a class="heading-anchor" href="#akamai">
###
&lt;/a>
Akamai
&lt;/h3>
&lt;p>Akamai does very wrong things, but at least warns about it. Here is &lt;a href="https://community.akamai.com/customers/s/article/Difference-Between-Akamai-True-Client-IP-header-and-Default-X-Forwarded-For">the documentation&lt;/a> about how it handles &lt;code>X-Forwarded-For&lt;/code> and &lt;code>True-Client-IP&lt;/code> (original emphasis):&lt;/p>
&lt;blockquote>
&lt;p>&lt;code>X-Forwarded-For&lt;/code> header is the default header proxies use to report the end user IP that is requesting the content. However, this header is often overwritten by other proxies and is also overwritten by Akamai parent servers and thus are not very reliable.&lt;/p>
&lt;p>The &lt;code>True-Client-IP&lt;/code> header sent by Akamai does not get overwritten by proxy or Akamai servers and will contain the IP of the client when sending the request to the origin.&lt;/p>
&lt;p>&lt;code>True-Client-IP&lt;/code> is a self provisioned feature enabled in the Property Manager.&lt;/p>
&lt;p>&lt;em>Note that if the &lt;code>True-Client-IP&lt;/code> header is already present in the request from the client it will not be overwritten or sent twice. It is not a security feature.&lt;/em>&lt;/p>
&lt;p>The connecting IP is appended to &lt;code>X-Forwarded-For&lt;/code> header by proxy server and thus it can contain multiple IPs in the list with comma as separator. &lt;code>True-Client-IP&lt;/code> contains only one IP. If the the end user uses proxy server to connect to Akamai edge server, &lt;code>True-Client-IP&lt;/code> is the first IP from in &lt;code>X-Forwarded-For&lt;/code> header. If the end user connects to Akamai edge server directly, &lt;code>True-Client-IP&lt;/code> is the connecting public IP seen by Akamai.&lt;/p>
&lt;/blockquote>
&lt;p>The relevant bits are “&lt;code>True-Client-IP&lt;/code> is the first IP from in &lt;code>X-Forwarded-For&lt;/code> header” and “if the True-Client-IP header is already present in the request from the client it will not be overwritten”. So &lt;code>True-Client-IP&lt;/code> is either the leftmost XFF IP or keeps the original value spoofed by the client. Just the worst possible thing.&lt;/p>
&lt;p>However, there is also the sentence “It is not a security feature.” Well, that’s certainly true. Does that warning make it okay? What’s the chance that there aren’t a ton of Akamai users out using &lt;code>True-Client-IP&lt;/code> for security-related purposes?&lt;/p>
&lt;p>(I’m not sure how to interpret the above when it says that the XFF header is “overwritten by Akamai parent servers”. Does it mean “appended to” when it says “overwritten”? Or is Akamai actually blowing away the existing header value? That would be against the spirit of XFF.)&lt;/p>
&lt;h3 id="fastly">
&lt;a class="heading-anchor" href="#fastly">
###
&lt;/a>
Fastly
&lt;/h3>
&lt;p>Fastly adds the &lt;a href="https://developer.fastly.com/reference/http/http-headers/Fastly-Client-IP/">&lt;code>Fastly-Client-IP&lt;/code>&lt;/a> header with a single IP value. I &lt;em>think&lt;/em> it’s using the rightmost-ish XFF IP:&lt;/p>
&lt;blockquote>
&lt;p>Essentially, &lt;code>Fastly-Client-IP&lt;/code> is the non-Fastly thing that is making the request to Fastly.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;em>However&lt;/em>:&lt;/p>
&lt;blockquote>
&lt;p>The value is not protected from modification at the edge of the Fastly network, so if a client sets this header themselves, we will use it. If you want to prevent this [you need to do some additional configuration].&lt;/p>
&lt;/blockquote>
&lt;p>So, by default &lt;code>Fastly-Client-IP&lt;/code> is trivially spoofable. Again, it seems highly likely that there are a lot of people using its default behaviour for security-related purposes and making themselves vulnerable to attack.&lt;/p>
&lt;h3 id="azure">
&lt;a class="heading-anchor" href="#azure">
###
&lt;/a>
Azure
&lt;/h3>
&lt;p>Azure Front Door adds the &lt;code>X-Azure-ClientIP&lt;/code> and &lt;code>X-Azure-SocketIP&lt;/code> headers. They are &lt;a href="https://docs.microsoft.com/en-us/azure/frontdoor/front-door-http-headers-protocol">described like so&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>&lt;code>X-Azure-ClientIP&lt;/code>: Represents the client IP address associated with the request being processed. For example, a request coming from a proxy might add the &lt;code>X-Forwarded-For&lt;/code> header to indicate the IP address of the original caller.&lt;/p>
&lt;p>&lt;code>X-Azure-SocketIP&lt;/code>: Represents the socket IP address associated with the TCP connection that the current request originated from. A request’s client IP address might not be equal to its socket IP address because the client IP can be arbitrarily overwritten by a user.&lt;/p>
&lt;/blockquote>
&lt;p>So, &lt;code>X-Azure-ClientIP&lt;/code> is the leftmost-ish XFF IP and &lt;code>X-Azure-SocketIP&lt;/code> is the rightmost-ish.&lt;/p>
&lt;p>That’s reasonably good, but I think it could be a lot clearer. The only warning about &lt;code>X-Azure-ClientIP&lt;/code> is a subtle hint in the description of the &lt;em>other&lt;/em> header. I also hand-wavingly feel that the name of the less-secure header is more appealing than the more-secure one, and is probably leading many people into the wrong choice.&lt;/p>
&lt;h3 id="go-chichi">
&lt;a class="heading-anchor" href="#go-chichi">
###
&lt;/a>
go-chi/chi
&lt;/h3>
&lt;p>Chi is a Go HTTP router and provides a &lt;a href="https://github.com/go-chi/chi/blob/c97bc988430d623a14f50b7019fb40529036a35a/middleware/realip.go#L42-L55">RealIP middleware&lt;/a> and a &lt;a href="https://github.com/go-chi/httprate/blob/463924d478ea0c19de7265b97371a59a7ebf5fdd/httprate.go#L28-L37">rate limiter&lt;/a>. The RealIP middleware has this comment:&lt;/p>
&lt;blockquote>
&lt;p>You should only use this middleware if you can trust the headers passed to you (in particular, the two [three, actually] headers this middleware uses), for example because you have placed a reverse proxy like HAProxy or nginx in front of Chi. If your reverse proxies are configured to pass along arbitrary header values from the client, or if you use this middleware without a reverse proxy, malicious clients will be able to make you very sad (or, depending on how you’re using RemoteAddr, vulnerable to an attack of some sort).&lt;/p>
&lt;/blockquote>
&lt;p>Which is a pretty good warning, right? Almost.&lt;/p>
&lt;p>Let’s take this opportunity to talk about abusing &lt;code>X-Real-IP&lt;/code>, &lt;code>True-Client-IP&lt;/code>, etc. For example, AWS ALB “[passes] along arbitrary header values from the client” and, indeed, if you don’t realize that you will end up “very sad”. Because a request like this…&lt;/p>
&lt;pre tabindex="0">&lt;code>curl -X POST https://my.load.balanced.domain/login -H "X-Forwarded-For:1.1.1.1" -H "X-Real-IP:2.2.2.2" -H "True-Client-IP:3.3.3.3"
&lt;/code>&lt;/pre>&lt;p>…results in your server getting these headers:&lt;/p>
&lt;pre tabindex="0">&lt;code>X-Forwarded-For: 1.1.1.1, &lt;actual client IP>
X-Real-IP: 2.2.2.2
True-Client-IP: 3.3.3.3
&lt;/code>&lt;/pre>&lt;p>&lt;code>chi.middleware.RealIP&lt;/code>’s logic goes like: “use the &lt;code>True-Client-IP&lt;/code>; if that doesn’t exist, use the &lt;code>X-Real-IP&lt;/code>; if that doesn’t exist, use &lt;code>X-Forwarded-For&lt;/code>”. So it falls victim to header spoofing.&lt;/p>
&lt;p>But, as we’ve learned, the &lt;code>chi.middleware.RealIP&lt;/code> warning also isn’t good enough when it comes to &lt;code>X-Forwarded-For&lt;/code> &lt;em>because you can never, ever trust all of it&lt;/em>&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. In the &lt;code>RealIP&lt;/code> code, the logic I just paraphrased actually ends with “use the leftmost XFF IP address”. And we’re now suitably scared of using the leftmost XFF IP. (It also does not check that the leftmost “IP” is valid and non-private.)&lt;/p>
&lt;p>So &lt;code>chi.middleware.RealIP&lt;/code> falls firmly into the “only safe for non-security use” category. And you &lt;em>must&lt;/em> be aware of its header preference order and what your reverse proxy does or doesn’t set and let through. In short, it’s hard to recommend it.&lt;/p>
&lt;p>Chi’s rate limiter has identical logic for obtaining the IP address and doesn’t have the same warning. So that’s bad, for the reasons we’ve discussed – the non-XFF headers could be spoofed, the XFF header could be spoofed, the IP can be garbage, the rate limiter could be bypassed, your memory could be exploded. The best way to use it is to not use its “real IP” logic and instead write your own “GetTrustworthyClientIP(request)” and pass that to its &lt;a href="https://github.com/go-chi/httprate/blob/463924d478ea0c19de7265b97371a59a7ebf5fdd/README.md?plain=1#L61">“rate limit by arbitrary keys”&lt;/a> feature.&lt;/p>
&lt;p>Both RealIP and httprate are both using Go’s &lt;code>http.Header.Get&lt;/code> to get the XFF header. As &lt;a href="#multiple-headers">discussed above&lt;/a>, this means that switching to taking the rightmost-ish IP wouldn’t be sufficient, as an attacker could force the wrong header to be used.&lt;/p>
&lt;p>Chi’s rate limiter is also the one instance I found of the XFF list being split by comma-space instead of just comma. I think that’s wrong.&lt;/p>
&lt;p>[2022-03-03: Disclosed to maintainer via email. 2022-03-04: Maintainer requested that I &lt;a href="https://github.com/go-chi/chi/issues/711">make an issue&lt;/a>.]&lt;/p>
&lt;h3 id="didiptollbooth">
&lt;a class="heading-anchor" href="#didiptollbooth">
###
&lt;/a>
didip/tollbooth
&lt;/h3>
&lt;p>The &lt;a href="https://github.com/didip/tollbooth">Tollbooth HTTP rate limiter&lt;/a> is better, but you still need to be aware of what it’s doing in order to use it properly.&lt;/p>
&lt;p>Its &lt;a href="https://github.com/didip/tollbooth/blob/d2340101f440011dd593e8b6787f5b1a437c2516/README.md#L68-L70">README&lt;/a> says the order in which it looks for the “real” client IP address is…&lt;/p>
&lt;blockquote>
&lt;p>By default it’s: “RemoteAddr”, “X-Forwarded-For”, “X-Real-IP”&lt;br>
If your application is behind a proxy, set “X-Forwarded-For” first.&lt;/p>
&lt;/blockquote>
&lt;p>Strangely, that default order isn’t actually the default everywhere. If you call &lt;a href="https://github.com/didip/tollbooth/blob/e1a6b41b35e95810a7dce638996b15f136424ed2/limiter/limiter.go#L21">&lt;code>limiter.New()&lt;/code>&lt;/a> that &lt;em>is&lt;/em> the default. But if you call &lt;a href="https://github.com/didip/tollbooth/blob/2e5b779d07015e6a4386c6cc1a36612c17d54eb5/tollbooth.go#L33">&lt;code>tollbooth.NewLimiter()&lt;/code>&lt;/a> – “a convenience function to limiter.New” – the order is &lt;code>"X-Forwarded-For", "X-Real-IP", "RemoteAddr"&lt;/code>. Which is an important difference!&lt;/p>
&lt;p>It doesn’t make clear that it doesn’t support &lt;code>CF-Connecting-IP&lt;/code>, &lt;code>True-Client-IP&lt;/code>, or any other arbitrary header – &lt;a href="https://github.com/didip/tollbooth/blob/2e5b779d07015e6a4386c6cc1a36612c17d54eb5/libstring/libstring.go#L20-L56">it just silently skips them&lt;/a> if you add them (returning empty string, leading to over-limiting).&lt;/p>
&lt;p>This is a general problem with Tollbooth – it “fails open”: if it can’t find an IP to use &lt;a href="https://github.com/didip/tollbooth/blob/2e5b779d07015e6a4386c6cc1a36612c17d54eb5/tollbooth.go#L51-L55">it doesn’t rate limit&lt;/a>. So if the user of the library misconfigures it (by trying to use an unsupported header, or even making a letter-case mistake in a supported on), it won’t rate limit at all and will give no indication of this. In a &lt;a href="https://github.com/didip/tollbooth/pull/99#issuecomment-1059328777">comment on the PR to fix this&lt;/a> I give my opinion on how it should be have, but this is a pretty tricky problem.&lt;/p>
&lt;p>Tollbooth also suffers from the &lt;a href="#multiple-headers">“multiple headers”&lt;/a> problem – &lt;a href="https://github.com/didip/tollbooth/blob/2e5b779d07015e6a4386c6cc1a36612c17d54eb5/libstring/libstring.go#L23">it uses&lt;/a> Go’s &lt;code>r.Header.Get&lt;/code>. So, even though it’s counting from the rightmost, which is good, it’s doing so with the first header, which is bad.&lt;/p>
&lt;p>Speaking of counting from the rightmost… If Tollbooth is configured to count very far from the right (like, &lt;code>lmt.SetForwardedForIndexFromBehind(1000)&lt;/code>), then it will effectively take the leftmost IP (it &lt;a href="https://github.com/didip/tollbooth/blob/2e5b779d07015e6a4386c6cc1a36612c17d54eb5/libstring/libstring.go#L42-L45">uses a minimum index of zero&lt;/a>). In that case, it should be doing &lt;a href="#leftmost-ish-the-closest-to-the-real-ip-but-utterly-untrustworthy">“leftmost-ish”&lt;/a> IP validation.&lt;/p>
&lt;p>There are two more things that bug me about tollbooth’s design. The first is that the ostensible default order has Go’s &lt;code>http.Request.RemoteAddr&lt;/code> first. My understanding is that that field should never be empty (for an HTTP server, rather than client), so the rest of the list is guaranteed to be ignored. So why have a list? Also, if your server is behind a reverse proxy, &lt;code>RemoteAddr&lt;/code> will be your proxy’s IP, which is useless.&lt;/p>
&lt;p>The second thing that bugs me is going to get its very own section…&lt;/p>
&lt;p>[2022-03-03: Disclosed to maintainer via email. 2022-03-04: Maintainer &lt;a href="https://github.com/didip/tollbooth/pull/99">created a PR&lt;/a> to fix it. Ongoing discussion there.]&lt;/p>
&lt;h3 id="a-default-list-of-places-to-look-for-the-client-ip-makes-no-sense">
&lt;a class="heading-anchor" href="#a-default-list-of-places-to-look-for-the-client-ip-makes-no-sense">
###
&lt;/a>
A default list of places to look for the client IP makes no sense
&lt;/h3>
&lt;p>Where you should be looking for the “real” client IP is very specific to your network architecture and use case. A default configuration encourages blind, naïve use and will result in incorrect and potentially dangerous behaviour more often than not.&lt;/p>
&lt;p>If you’re using Cloudflare you want &lt;code>CF-Connecting-IP&lt;/code>. If you’re using &lt;code>ngx_http_realip_module&lt;/code>, you want &lt;code>X-Real-IP&lt;/code>. If you’re behind AWS ALB you want the rightmost-ish &lt;code>X-Forwarded-For&lt;/code> IP. If you’re directly connected to the internet, you want &lt;code>RemoteAddr&lt;/code> (or equivalent). And so on.&lt;/p>
&lt;p>There’s &lt;em>never&lt;/em> a time when you’re okay with just falling back across a big list of header values that have nothing to do with your network architecture. That’s going to bite you.&lt;/p>
&lt;p>Even Tollbooth defaulting to using the rightmost XFF IP can be problematic. If your server is behind two layers of reverse proxies, then you’ll be looking at the IP of your first proxy instead of the client’s IP.&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup> (What will probably happen is that you will rate-limit your proxy almost immediately, nothing gets through, and then you fix your config. But it still would have been better to be forced to think about the correct configuration in the first place.)&lt;/p>
&lt;p>So, even though I know it’s not very user friendly, I don’t think that rate-limiting libraries should have any default at all, and instead should &lt;em>require&lt;/em> explicit configuration.&lt;/p>
&lt;h3 id="ululelimiter">
&lt;a class="heading-anchor" href="#ululelimiter">
###
&lt;/a>
ulule/limiter
&lt;/h3>
&lt;p>Another Go rate limiter middleware. By default it doesn’t look at the XFF header, but if enabled it &lt;a href="https://github.com/ulule/limiter/blob/9ba030af6009ca2bc9a285dbd815bc7a76de724a/network.go#L31-L56">uses the leftmost XFF IP&lt;/a>. The option is called “TrustForwardHeader”, but &lt;em>you can never trust the XFF header&lt;/em>. So it falls victim to rate limit escape, etc.&lt;/p>
&lt;p>It also uses Go’s &lt;code>http.Header.Get&lt;/code>, so if it switches to rightmost-ish it will need to change how it gets the XFF header.&lt;/p>
&lt;p>When &lt;code>TrustForwardHeader&lt;/code> is true it first looks for XFF and then falls through to &lt;code>X-Real-IP&lt;/code> and finally uses &lt;code>RemoteAddr&lt;/code>. But as we just saw, “a default list of places to look for the client IP makes no sense”.&lt;/p>
&lt;p>It returns &lt;code>net.ParseIP(ip)&lt;/code> rather than just the raw string. This seems good at first, but &lt;code>net.ParseIP&lt;/code> &lt;a href="https://pkg.go.dev/net#ParseIP">returns nil&lt;/a> if the parse fails. So in the case of a garbage string, ulule/limiter doesn’t check for the failure, returns nil, and then, &lt;a href="https://go.dev/play/p/8bgVfOy4Lkw">as far as I can tell&lt;/a>, uses &lt;code>"&lt;nil>"&lt;/code> as the “IP” key. (I’m surprised that it doesn’t panic, but I don’t think it does.) The way this logic works makes memory exhaustion more difficult, but it might be achievable using valid IPv6 addresses.&lt;/p>
&lt;p>[2022-03-04: Disclosed to maintainer via email. 2022-03-05: &lt;a href="https://github.com/ulule/limiter/pull/181">PR&lt;/a> has been created with fixes (mostly documentation warnings). It’s public, so I’m un-redacting this. 2022-03-17: The PR was merged. &lt;a href="https://github.com/ulule/limiter/pull/181#issuecomment-1059827400">I don’t love&lt;/a> the changes, but it sounds like the next major version will address the shortcomings.]&lt;/p>
&lt;h3 id="sethvargogo-limiter">
&lt;a class="heading-anchor" href="#sethvargogo-limiter">
###
&lt;/a>
sethvargo/go-limiter
&lt;/h3>
&lt;p>This is yet another Go rate limiter middleware. If its &lt;code>httplimit.IPKeyFunc&lt;/code> is configured to look at the &lt;code>X-Forwarded-For&lt;/code> header (which is given as an example in its comment), it will &lt;a href="https://github.com/sethvargo/go-limiter/blob/d0c1f4df450eb46623c6f3afe9f675d40da52cf9/httplimit/middleware.go#L40-L59">&lt;em>use the whole header&lt;/em>&lt;/a> as the rate limit key. That’s almost worse than taking the leftmost IP.&lt;/p>
&lt;p>The way to work around this would be to avoid its “real IP” logic and create your own &lt;a href="https://github.com/sethvargo/go-limiter/blob/67fff5ee8978ea6218af4d1354bb842096a5b543/httplimit/middleware.go#L38">&lt;code>KeyFunc&lt;/code>&lt;/a> that extracts the correct IP for your network architecture.&lt;/p>
&lt;p>If the library can’t find the configured header(s), it falls through to &lt;code>RemoteAddr&lt;/code>. But, again, I don’t think default fallbacks are good.&lt;/p>
&lt;p>The library also uses Go’s &lt;code>http.Header.Get()&lt;/code>.&lt;/p>
&lt;p>[2022-03-04: Disclosed to maintainer via email. 2022-03-05: Maintainer indicated by email that I could un-redact this.]&lt;/p>
&lt;!--redact-start-->
&lt;h3 id="redacted">
&lt;a class="heading-anchor" href="#redacted">
###
&lt;/a>
[REDACTED]
&lt;/h3>
&lt;p>Pending disclosure&lt;/p>
&lt;!--redact-end-->
&lt;h3 id="lets-encrypt">
&lt;a class="heading-anchor" href="#lets-encrypt">
###
&lt;/a>
Let’s Encrypt
&lt;/h3>
&lt;p>It &lt;a href="https://github.com/letsencrypt/boulder/blob/ab79f96d7bfc94be7d009e2aa2007c51ddf16f31/web/context.go#L95">looks like&lt;/a> Let’s Encrypt is using Nginx with &lt;code>X-Real-IP&lt;/code>. If it’s configuration is good (I don’t think the config files are in GitHub), then it should be using rightmost-ish.&lt;/p>
&lt;!--redact-start-->
&lt;h3 id="redacted-1">
&lt;a class="heading-anchor" href="#redacted-1">
###
&lt;/a>
[REDACTED]
&lt;/h3>
&lt;p>Pending disclosure&lt;/p>
&lt;!--redact-end-->
&lt;h3 id="jetty">
&lt;a class="heading-anchor" href="#jetty">
###
&lt;/a>
Jetty
&lt;/h3>
&lt;p>It looks like the Jetty web server &lt;a href="https://github.com/eclipse/jetty.project/blob/cb127793e5d8b5c5730b964392a9a905ba49191d/jetty-server/src/main/java/org/eclipse/jetty/server/ForwardedRequestCustomizer.java#L814-L821">uses the leftmost&lt;/a> XFF IP address. I haven’t dug far enough in to see what it’s used for (or how it’s exposed), but that’s a dangerous start.&lt;/p>
&lt;h3 id="express">
&lt;a class="heading-anchor" href="#express">
###
&lt;/a>
Express
&lt;/h3>
&lt;p>Express is a NodeJS web framework. The default configuration ignores the XFF header, but it’s possible to &lt;a href="https://expressjs.com/en/guide/behind-proxies.html">configure it&lt;/a> to use the leftmost or a rightmost-ish XFF IP. The setting that uses the leftmost has this warning:&lt;/p>
&lt;blockquote>
&lt;p>When setting to true, it is important to ensure that the last reverse proxy trusted is removing/overwriting all of the following HTTP headers: X-Forwarded-For, X-Forwarded-Host, and X-Forwarded-Proto otherwise it may be possible for the client to provide any value.&lt;/p>
&lt;/blockquote>
&lt;p>So that’s pretty good. (But still a footgun that I’m sure someone will fall victim to.)&lt;/p>
&lt;h3 id="traefik">
&lt;a class="heading-anchor" href="#traefik">
###
&lt;/a>
Traefik
&lt;/h3>
&lt;p>Traefik is a “cloud native network stack”. Its &lt;a href="https://doc.traefik.io/traefik/middlewares/http/ratelimit/#ipstrategydepth">rate limiter&lt;/a> is configured to use the “trusted proxy count” version of rightmost-ish. So that’s good.&lt;/p>
&lt;h3 id="phplist">
&lt;a class="heading-anchor" href="#phplist">
###
&lt;/a>
phpList
&lt;/h3>
&lt;p>phpList is an “open source newsletter and email marketing software”. It &lt;a href="https://github.com/phpList/phplist3/blob/927ce341068d16f35566eaaddf2b9ace639885a4/public_html/lists/admin/lib.php#L2423-L2445">uses the leftmost XFF IP&lt;/a>. I can’t quite tell what it’s used for, but it’s &lt;a href="https://github.com/phpList/phplist3/blob/927ce341068d16f35566eaaddf2b9ace639885a4/public_html/lists/admin/index.php#L296">something surrounding login&lt;/a>.&lt;/p>
&lt;h3 id="iis">
&lt;a class="heading-anchor" href="#iis">
###
&lt;/a>
IIS
&lt;/h3>
&lt;p>I could find anything to suggest that Microsoft IIS processes the XFF header, but an official support blog post entitled &lt;a href="https://techcommunity.microsoft.com/t5/iis-support-blog/how-to-use-x-forwarded-for-header-to-log-actual-client-ip/ba-p/873115">“How to use X-Forwarded-For header to log actual client IP address?”&lt;/a> says:&lt;/p>
&lt;blockquote>
&lt;p>If you see multiple IP addresses in X-Forwarded-For column, it means the client went through more than one network device. Each network device adds their own IP to the end of the value. The left-most IP address is the actual client IP address. Others belong to network devices the client go through.&lt;/p>
&lt;/blockquote>
&lt;p>Which is a &lt;em>woefully incomplete&lt;/em> statement. I fear for the 97,641 people who read that post.&lt;/p>
&lt;h3 id="tor">
&lt;a class="heading-anchor" href="#tor">
###
&lt;/a>
Tor
&lt;/h3>
&lt;p>Tor is an anonymity network. They have &lt;a href="https://gitlab.torproject.org/tpo/anti-censorship/rdsys/-/issues/80">recently realized&lt;/a> that they have a control server that is both directly connected to the internet &lt;em>and&lt;/em> behind a reverse proxy and are using XFF to give them trustworthy IPs, so they’re vulnerable to spoofing. It looks like they’re working towards limiting the public-ness of the interface, or doing better verification of CDN connections, or both.&lt;/p>
&lt;p>(Because I looked into it, I’ll mention that it looks like they’re &lt;em>not&lt;/em> falling victim to the &lt;a href="#multiple-headers">“multiple headers”&lt;/a> pitfall. It looks like they use Twisted and call &lt;code>request.getHeader&lt;/code> to get the XFF value. The &lt;a href="https://github.com/twisted/twisted/blob/ebb2d360070e468981377b917e3a728ff4e6c7f6/src/twisted/web/http.py#L1068">Twisted source&lt;/a> for that method indicates that it returns the &lt;em>last&lt;/em> matching header. That could cause problems if you need the Nth-from-the-right header, but I think it’s fine in this case.)&lt;/p>
&lt;h3 id="gorillahandlersproxyheaders">
&lt;a class="heading-anchor" href="#gorillahandlersproxyheaders">
###
&lt;/a>
gorilla/handlers.ProxyHeaders
&lt;/h3>
&lt;p>[Section added 2022-03-27. u/Genesis2001 &lt;a href="https://old.reddit.com/r/golang/comments/toynmv/rfc_get_the_real_client_ip_the_right_ways/i28ibey/">asked about this&lt;/a> on Reddit, so I looked at the code and figured I should add some comments here.]&lt;/p>
&lt;p>&lt;a href="https://github.com/gorilla">Gorilla&lt;/a> is a Go web toolkit. It’s most known for its router, &lt;a href="https://github.com/gorilla/mux">gorilla/mux&lt;/a>. It has a &lt;a href="https://pkg.go.dev/github.com/gorilla/handlers#ProxyHeaders">&lt;code>ProxyHeaders&lt;/code> middleware&lt;/a> for handling XFF (that is intended for general consumption, not just for gorilla/mux users).&lt;/p>
&lt;p>&lt;code>ProxyHeaders&lt;/code> (&lt;a href="https://github.com/gorilla/handlers/blob/v1.5.1/proxy_headers.go#L43">source&lt;/a>) is deficient in a number of ways, but at least it has a warning that the user’s first reverse proxy must strip out the headers being checked before adding them back in. So it’s good that it has that warning, but that requirement means that a) it won’t be usable for a lot of users, and b) it will be misused by a lot of users.&lt;/p>
&lt;p>Let’s touch on the problems that are legitimately mitigated by stripping the headers:&lt;/p>
&lt;ul>
&lt;li>It’s taking the leftmost XFF and &lt;code>Forwarded&lt;/code> values.&lt;/li>
&lt;li>It’s checking &lt;code>X-Forwarded-For&lt;/code> and then &lt;code>X-Real-IP&lt;/code> and then &lt;code>Forwarded&lt;/code>. So it has the &lt;a href="#a-default-list-of-places-to-look-for-the-client-ip-makes-no-sense">“default list”&lt;/a> problem.&lt;/li>
&lt;li>It’s using &lt;code>X-Forwarded-Host&lt;/code> to replace &lt;code>r.Host&lt;/code>. So that’s a new spoofable thing.&lt;/li>
&lt;li>It’s using &lt;code>X-Forwarded-Proto&lt;/code>-then-&lt;code>X-Forwarded-Scheme&lt;/code>-then-&lt;code>Fowarded&lt;/code> to replace &lt;code>r.URL.Scheme&lt;/code>. Another new spoofable thing.&lt;/li>
&lt;/ul>
&lt;p>It’s &lt;a href="https://github.com/gorilla/handlers/blob/d453effd20e6817a8acfd9d278dadd21371da0ba/proxy_headers.go#L74">using comma-space&lt;/a> to parse &lt;code>X-Forwarded-For&lt;/code>, contrary to RFC 2616. So it can set &lt;code>r.RemoteAddr&lt;/code> to an IP like “1.1.1.1,10.1.1.1”, etc. It’s also not trimming the result, and I &lt;em>think&lt;/em> the LWS rules of RFC 2616 mean that there can be spaces before the comma, so &lt;code>ProxyHeaders&lt;/code> can also end up with strings like &lt;code>"&lt;space>1.1.1.1&lt;space>"&lt;/code>.&lt;/p>
&lt;p>It also doesn’t support any single-IP headers besides &lt;code>X-Real-IP&lt;/code>, which limits its general utility.&lt;/p>
&lt;p>(And I keep wondering why stripping XFF at the first reverse proxy makes sense. If you have that much control – e.g., if you’re using Nginx – you should instead just set &lt;code>X-Real-IP&lt;/code> and let XFF behave the way it’s intended to.)&lt;/p>
&lt;p>[2022-03-27: Created &lt;a href="https://github.com/gorilla/handlers/issues/227">issue&lt;/a> and &lt;a href="https://github.com/gorilla/handlers/pull/228">pull request&lt;/a>.]&lt;/p>
&lt;h3 id="quickies">
&lt;a class="heading-anchor" href="#quickies">
###
&lt;/a>
Quickies
&lt;/h3>
&lt;p>[2022-03-29: Added this section.]&lt;/p>
&lt;p>I keep finding more examples of code making mistakes. They help to drive home the points I’m trying to make, but they would bloat this post if I gave a section to every one. Instead I’m going to write some short bullets.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/sebest/xff">sebest/xff&lt;/a>: Claims to be a &lt;code>Forwarded&lt;/code> (RFC 7239) parser but is instead an XFF parser. It uses a leftmost-non-private algorithm, but doesn’t document the risks (though there is a 6-year-old &lt;a href="https://github.com/sebest/xff/issues/6">issue&lt;/a>). The private IP ranges are incomplete. (&lt;a href="https://github.com/sebest/xff/issues/14">Created&lt;/a> &lt;a href="https://github.com/sebest/xff/issues/15">issues&lt;/a>.)&lt;/li>
&lt;li>&lt;a href="https://github.com/pbojinov/request-ip">pbojinov/request-ip&lt;/a>: Falls back between XFF and many single-IP headers. Uses leftmost for XFF. Has no warning about dangers. Claims to support &lt;code>Forwarded&lt;/code>, but just returns the whole value. Has issues and PRs to address some of this, but project seems abandoned.&lt;/li>
&lt;li>&lt;a href="https://github.com/mo7zayed/reqip">mo7zayed/reqip&lt;/a>: Based on the previous one, with all the same shortcomings.&lt;/li>
&lt;li>&lt;a href="https://github.com/stanvit/go-forwarded">stanvit/go-forwarded&lt;/a>: Takes the absolute rightmost, which is better than leftmost but still not right for all configurations. (Created &lt;a href="https://github.com/stanvit/go-forwarded/issues/2">an issue&lt;/a>.)&lt;/li>
&lt;/ul>
&lt;h2 id="advanced-and-theoretical-pitfalls-and-attacks">
&lt;a class="heading-anchor" href="#advanced-and-theoretical-pitfalls-and-attacks">
##
&lt;/a>
Advanced and theoretical pitfalls and attacks
&lt;/h2>
&lt;p>I’ve talked a lot about two attacks on rate limiters: avoiding being limited and exhausting server memory. I’ve done this because rate limiters are what led me to this topic and because causing a map of IPs to fill memory was an obvious danger in many implementations.&lt;/p>
&lt;p>But rate limiters are only one “security-related” use of &lt;code>X-Forwarded-For&lt;/code>, and there are more, cooler possibilities for badness! They’re harder to find or reproduce, but they should be fun to speculate on…&lt;/p>
&lt;h3 id="server-behind-reverse-proxy-_and_-directly-connectable">
&lt;a class="heading-anchor" href="#server-behind-reverse-proxy-_and_-directly-connectable">
###
&lt;/a>
Server behind reverse proxy &lt;em>and&lt;/em> directly connectable
&lt;/h3>
&lt;p>This was briefly mentioned in the “algorithms” section, but is worth repeating.&lt;/p>
&lt;p>Generally speaking, if your server is behind one or more reverse proxies, there are one or more rightmost IPs in the XFF header that you can trust. The “rightmost-ish” algorithm is predicated on that. But if your server can &lt;em>also&lt;/em> be connected to directly from the internet, that is no longer true.&lt;/p>
&lt;p>With some experimentation, an attacker can craft an XFF header to look exactly like the one you expect from your reverse proxy:&lt;/p>
&lt;ol>
&lt;li>Attacker gets her IP limited/blocked by your server.&lt;/li>
&lt;li>Attacker crafts XFF header so that the rightmost of it has different IPs in the private space, and different counts of those IPs.&lt;/li>
&lt;li>Continue until the limit/block unexpectedly disappears.&lt;/li>
&lt;/ol>
&lt;p>Now you’re using an untrusted XFF IP and don’t realize it. Rate limiter escape, memory exhaustion, etc.&lt;/p>
&lt;p>One way to mitigate this is to check the &lt;code>RemoteAddr&lt;/code> to make sure it belongs to your reverse proxy before you try to use the XFF header.&lt;/p>
&lt;h3 id="re-fronting-attack">
&lt;a class="heading-anchor" href="#re-fronting-attack">
###
&lt;/a>
Re-fronting attack
&lt;/h3>
&lt;p>Thanks to Ryan Gerstenkorn for sending me his &lt;a href="https://blog.ryanjarv.sh/2022/03/16/bypassing-wafs-with-alternate-domain-routing.html">blog post&lt;/a> about this.&lt;/p>
&lt;p>If a) your backend is in-house or otherwise externally accessible, and b) it’s fronted by a CDN, and c) you trust the IP addresses/ranges of your CDN, then you may be vulnerable to another class of attack.&lt;/p>
&lt;p>With AWS CloudFront, it’s possible for an attacker to create a distribution that points to your origin. Now requests are coming to your origin from trusted IPs, but from a distribution not owned by you. But the real beauty/horror of this is that the attacker can use Lambda@Edge to modify the &lt;code>Host&lt;/code> header so that you can’t tell that a different hostname was used to access your origin, and can also modify the &lt;code>X-Forwarded-For&lt;/code> header to be whatever the attacker wants.&lt;/p>
&lt;p>So your “trusted” reverse proxy IPs become untrusted and can lie to you about the client IP. This can be used to bypass your rate limiter, IP-based access control, etc.&lt;/p>
&lt;p>The proper way to address this is to also verify that it’s &lt;em>your&lt;/em> CDN distribution talking to you. This will usually involve a shared secret or client certificate.&lt;/p>
&lt;p>Note that Gerstenkorn verified that this works for AWS CloudFront. I checked Cloudflare and found that it doesn’t work there: Cloudflare’s “Transform Rules” won’t let you “set” the XFF header, and if you “delete” the header, only the pre-existing header is deleted and a new one is added with the actual IP. And it’s similar when attempting to leverage Workers – the actual client IP is still appended to the XFF header after any other manipulation.&lt;/p>
&lt;p>Always do strong verification of your CDN! Maybe there are other Cloudflare headers that are important to you and aren’t as protected as XFF. Or maybe you’re using some other CDN. And this general class of attacks might apply to third-party WAFs, etc., depending on how they’re configured.&lt;/p>
&lt;h3 id="many-trusted-reverse-proxy-ips">
&lt;a class="heading-anchor" href="#many-trusted-reverse-proxy-ips">
###
&lt;/a>
Many trusted reverse proxy IPs
&lt;/h3>
&lt;p>This is more of a “difficulty” than a pitfall or attack, but this is as good a place as any to fit it in.&lt;/p>
&lt;p>In the rightmost-ish algorithm, the “trusted proxy IPs” list might be very large and might involve ranges rather than specific IPs. For example:&lt;/p>
&lt;ul>
&lt;li>There might be a pool of reverse proxies that feed into your server.&lt;/li>
&lt;li>That pool might scale out with load. So your trusted IPs will have to include whatever range that pool uses.&lt;/li>
&lt;li>You might use a large external service, like Cloudflare. So you’ll need to “trust” all of their &lt;a href="https://www.cloudflare.com/ips">very large set of IPs&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>Especially in that last case – and &lt;em>especially&lt;/em> if you’re accepting IPv6 connections – you can’t just have a big list of single IPs that you check. You’ll need to check a list of ranges.&lt;/p>
&lt;h3 id="network-architecture-changes">
&lt;a class="heading-anchor" href="#network-architecture-changes">
###
&lt;/a>
Network architecture changes
&lt;/h3>
&lt;p>So, you’ve set everything up perfectly. Your configuration is exactly right and you’re picking the correct “client IP” every time. Great. It runs quietly and flawlessly for so long that you forget all about it.&lt;/p>
&lt;p>And then you change your network architecture.&lt;/p>
&lt;p>The scenarios with the less-bad result are when you’re using the rightmost-ish approach and you add a new level of reverse proxy.&lt;/p>
&lt;ul>
&lt;li>You were accepting connections directly from the internet, so you were using &lt;code>RemoteAddr&lt;/code> for rate-limiting. Then you added a load balancer. Now you’re rate-limiting your load balancer.&lt;/li>
&lt;li>You were using a single reverse proxy. You were using the rightmost XFF IP – the one that gets added by that proxy – for rate-limiting. Then you added another level of reverse proxy. Now you’re rate-limiting one of your reverse proxies (whichever is first in the chain).&lt;/li>
&lt;li>You had a complex setup of internal reverse proxies. You were rate-limiting by rightmost-ish XFF IP, with your whole internal IP range on the “trusted proxy” list. Then you added Cloudflare in front of it all. Now you’re rate-limiting Cloudflare.&lt;/li>
&lt;/ul>
&lt;p>Those are “less bad” because they don’t introduce security flaws, but they’re still going to result in a near-complete inability to process requests.&lt;/p>
&lt;p>The “much more bad” scenarios tend to occur when removing reverse proxy levels, and introduce vulnerabilities that you won’t notice.&lt;/p>
&lt;ul>
&lt;li>You were using a single reverse proxy. You were using the rightmost XFF IP – the one that gets added by that proxy – for rate-limiting. Then you decide that you don’t need that extra proxy level and instead you connect your server directly to the internet. But now no part of the XFF is trustworthy and you’re vulnerable to spoofing.&lt;/li>
&lt;li>You were using Cloudflare in front of AWS ALB and relying on its &lt;code>CF-Connecting-IP&lt;/code> header. Then you decide to save some money, drop Cloudflare, and now have requests go directly to ALB. So now you’re vulnerable to &lt;code>CF-Connecting-IP&lt;/code> spoofing.&lt;/li>
&lt;li>You were using two levels of reverse proxy in front of your server. You were using the rightmost-ish XFF IP with a “trusted proxy count” of 2 for rate-limiting – so you were always taking the second-from-the-right IP. You decide to remove a level of reverse proxy. Now you’re vulnerable to spoofing because the second-from-the-right XFF IP is no longer trustable.&lt;/li>
&lt;/ul>
&lt;p>Depending on your logic, it might take an attack before you notice that you’re vulnerable.&lt;/p>
&lt;p>The takeaway here is pretty obvious – “when network architecture changes, configuration that depends on that architecture also needs to change” – but it can be very difficult to remember to update every dependent config file when you tweak something that “should” work and does appear to work. (Maybe there are fancy network-management tools that automatically push config changes in response to architecture changes? Would be cool.)&lt;/p>
&lt;p>The “trusted proxy count” variation of the rightmost-ish algorithm is especially brittle to adding or removing reverse proxies. The “trusted proxy count” (especially with a big internal range) or “rightmost non-private IP” variations can better cope with changing the number of internal proxies (as long as you’re not going to zero).&lt;/p>
&lt;h4 id="even-worse-_third-party_-network-architecture-changes">
&lt;a class="heading-anchor" href="#even-worse-_third-party_-network-architecture-changes">
####
&lt;/a>
Even worse: &lt;em>Third-party&lt;/em> network architecture changes
&lt;/h4>
&lt;p>Take a look at the &lt;a href="https://www.cloudflare.com/ips">Cloudflare IP list&lt;/a>. Notice down at the bottom that there have been two times where Cloudflare removed IPs from the list.&lt;/p>
&lt;p>Imagine you had those IPs on your trusted list. Imagine you didn’t realize they were removed. &lt;em>Now who owns those IPs?&lt;/em> Whoever it is can put whatever they want in the XFF, pass it on to your trusted proxy chain, and you’ll use it as the “real” IP. Rate limit escape and memory exhaustion.&lt;/p>
&lt;p>…The answer to that question is that Cloudflare still owns the IPs (I &lt;a href="https://search.arin.net/rdap/?query=104.30.0.1">checked&lt;/a> &lt;a href="https://search.arin.net/rdap/?query=199.27.128.1">ARIN&lt;/a>). But the point isn’t about Cloudflare and those particular IP ranges. &lt;em>Any&lt;/em> CDN or reverse proxy service with a trusted IP list could change their list and cause problems.&lt;/p>
&lt;p>(Edit: A reader shared &lt;a href="https://adam-p.ca/img/blog/cloudflare-ip-range-removal-email.png">the email Cloudflare sent&lt;/a> when they most recently changed their IP set. He pointed out that even though Cloudflare still owns the IPs, they should be considered untrusted. The email mentions that there’s &lt;a href="https://api.cloudflare.com/#cloudflare-ips-properties">an API&lt;/a> to get Cloudflare’s IPs, which is good. Anyway, my original point wasn’t just about Cloudflare, so it remains unchanged.)&lt;/p>
&lt;h3 id="x-forwarded-for-parser-mismatch">
&lt;a class="heading-anchor" href="#x-forwarded-for-parser-mismatch">
###
&lt;/a>
&lt;code>X-Forwarded-For&lt;/code> parser mismatch
&lt;/h3>
&lt;p>This is inspired by &lt;a href="https://bishopfox.com/blog/json-interoperability-vulnerabilities">JSON interoperability vulnerabilities&lt;/a>. These occur when different levels of code or architecture interpret JSON in different ways. So if the JSON parser at one level deals with, say, duplicate object keys by taking the first key and another level deals with it by taking the last key, you can have a problem. (E.g., an attacker passes a &lt;code>"username"&lt;/code> value along with a matching password, but then also passes another &lt;code>"username"&lt;/code> value. If your auth check uses the first username and the business logic uses the second, you’re going to access the wrong user data.)&lt;/p>
&lt;p>If there’s one thing that’s certainly true of the XFF header it’s that there’s an abundance of ways of “interpreting” it. Let’s recap some of them and add a fun new one:&lt;/p>
&lt;ul>
&lt;li>Which position? Leftmost? Rightmost? Nth-from-rightmost? And so on.&lt;/li>
&lt;li>What value is acceptable? Don’t validate at all? Only IPs? Only non-private IPs?&lt;/li>
&lt;li>How to split up the list of IPs? Comma? Comma-space?&lt;/li>
&lt;li>How to handle multiple XFF headers?&lt;/li>
&lt;li>New: How to handle weird characters? What if there’s a null byte? Or some other control character? What if there’s a UTF-8 sequence?&lt;/li>
&lt;/ul>
&lt;p>&lt;em>Any&lt;/em> difference in the answers to any of those question marks can result in a mismatch between parsers.&lt;/p>
&lt;p>I wish I had a slam-dunk example scenario for this, but I don’t. Here are some hand-wavy ones:&lt;/p>
&lt;ul>
&lt;li>You block access to your service to requests from, say, Antarctica. You have a reverse proxy at one level that grabs an XFF IP and checks that. At another level, you have a reverse proxy that grabs a different XFF IP and collects geolocation statistics. You get confused about why you seem to have users connected from Antarctica. (One of them is doing it wrong, but this isn’t enough to tell you which.)&lt;/li>
&lt;li>At one reverse proxy level, you check a user’s incoming IP address against your DB to make sure it’s acceptable for that user. At another reverse proxy level, you update that DB. If there’s a mismatch, you’ll end up too permissive, too restrictive, or both.&lt;/li>
&lt;li>More generally… At one reverse proxy level you use the XFF header to determine the client’s IP. Allowing the request to proceed is an attestation that the client IP is acceptable for further processing. At a later reverse proxy level, the client IP is again derived from the XFF header and treated as trusted data because it is implicitly attested to by the previous level.&lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup> A difference between the two levels in XFF parsing introduces a vulnerability.&lt;/li>
&lt;/ul>
&lt;h2 id="rfc-7239-forwarded-http-extension-june-2014">
&lt;a class="heading-anchor" href="#rfc-7239-forwarded-http-extension-june-2014">
##
&lt;/a>
RFC 7239: Forwarded HTTP Extension, June 2014
&lt;/h2>
&lt;p>After considering comma-vs-comma-space and other parser mismatches, you’re probably thinking, “There should be an RFC to concretely specify this.” And there is, kind of.&lt;/p>
&lt;p>&lt;a href="https://datatracker.ietf.org/doc/html/rfc7239">RFC 7239&lt;/a> specifies the &lt;code>Forwarded&lt;/code> header. Its purpose is to replace and improve &lt;code>X-Forwarded-For&lt;/code>. The big thing it addresses is that &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For">&lt;code>X-Forwarded-For&lt;/code>&lt;/a> (a list of client and proxy IP addresses), &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-Host">&lt;code>X-Forwarded-Host&lt;/code>&lt;/a> (the hostname requested by the client), &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-Proto">&lt;code>X-Forwarded-Proto&lt;/code>&lt;/a> (the protocol used by the client; e.g., “https”), and &lt;a href="https://datatracker.ietf.org/doc/html/rfc7239#section-5.1">&lt;code>X-Forwarded-By&lt;/code>&lt;/a> (the proxy IP address) are all separate-but-related headers. It becomes very easy to for an intermediary proxy to mess up the relationship between them. To address this, the &lt;code>Forwarded&lt;/code> header includes all that information in a single list.&lt;/p>
&lt;p>Does the &lt;code>Forwarded&lt;/code> header fix the security issues that &lt;code>X-Forwarded-For&lt;/code> has? Not even a little. It can be misused in exactly the same ways that XFF can.&lt;/p>
&lt;p>Does it have wide adoption? Not that I can see. It gets mentioned in documentation occasionally, but I don’t remember it actually being checked in any of the code I read.&lt;/p>
&lt;p>Okay, does the RFC at least make clear how it should be used and not be used? Well, there is &lt;a href="https://datatracker.ietf.org/doc/html/rfc7239#section-8.1">this section&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;h5 id="81--header-validity-and-integrity">
&lt;a class="heading-anchor" href="#81--header-validity-and-integrity">
#####
&lt;/a>
8.1. Header Validity and Integrity
&lt;/h5>
&lt;p>The “Forwarded” HTTP header field cannot be relied upon to be correct, as it may be modified, whether mistakenly or for malicious reasons, by every node on the way to the server, including the client making the request.&lt;/p>
&lt;p>One approach to ensure that the “Forwarded” HTTP header field is correct is to verify the correctness of proxies and to whitelist them as trusted. This approach has at least two weaknesses. First, the chain of IP addresses listed before the request came to the proxy cannot be trusted. Second, unless the communication between proxies and the endpoint is secured, the data can be modified by an attacker with access to the network.&lt;/p>
&lt;/blockquote>
&lt;p>And that’s it.&lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>&lt;/p>
&lt;p>That warning is strictly true, but it’s not very helpful and could be clearer. Would you read those five sentences and then think, “Now I thoroughly understand the danger! It’s perfectly clear to me how to use this header in a secure manner.”? I wouldn’t.&lt;/p>
&lt;p>I feel like it should be the responsibility of this RFC not only to specify how to &lt;em>create&lt;/em> the header but also how to correctly &lt;em>consume&lt;/em> it.&lt;/p>
&lt;p>(Bonus: The RFC adds a variation on IPv6 parsing – quotation marks: “Note that as “:” and “[]” are not valid characters in “token”, IPv6 addresses are written as “quoted-string”.” E.g., &lt;code>"[2001:db8:cafe::17]:4711"&lt;/code>.)&lt;/p>
&lt;h2 id="conclusions">
&lt;a class="heading-anchor" href="#conclusions">
##
&lt;/a>
Conclusions
&lt;/h2>
&lt;p>Well, that was exhausting. I didn’t start out with the intention of writing a dissertation on this godforsaken header.&lt;/p>
&lt;p>I have avoided giving &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For#syntax">this definition&lt;/a> until now, just in case anyone skims the article and sees it, but I think we’re ready for it:&lt;/p>
&lt;pre tabindex="0">&lt;code>X-Forwarded-For: &lt;client>, &lt;proxy1>, &lt;proxy2>
&lt;/code>&lt;/pre>&lt;p>That’s what you’ll see on basically every page that describes the header. Is it any wonder that misuse of &lt;code>X-Forwarded-For&lt;/code> is so prevalent?&lt;/p>
&lt;p>Let’s summarize some of the things we’ve learned, the wisdom we’ve gained, and the opinions we’ve formed:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Danger on the left, trust on the right. There are, maybe, some situations where the leftmost-ish XFF IP can be used, but only very carefully. If there’s any doubt, use the rightmost-ish.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Any header – or any part of any header – that wasn’t set by your own reverse proxy is fundamentally untrustworthy.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Setting a special header to a helpful value but also just letting it through if it’s already set is a terrible idea. (See: Akamai, Fastly.)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If there’s no universally sane default, there should be no default. And “get the real IP” is a case where there’s no sane default. (See: Chi, Tollbooth.)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Defaults should &lt;em>not&lt;/em> be insecure. There shouldn’t need to be extra configuration to avoid shooting yourself in the foot. (See: Fastly.)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If you know a function or value is dangerous (spoofable, etc.), put that in your documentation for it in big red letters. Don’t just coyly hint at it. (See: Azure, etc.)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Good specifications (i.e., RFCs) should tell you how to consume a value, not just how to produce it. And if there are different ways to consume that value that make sense in different situations, it should give you the necessary information – with sufficient clarity – to help you make that choice. A reference implementation would also help. [2022-03-24: I &lt;a href="https://github.com/realclientip/realclientip-go">wrote a library&lt;/a> that I hope will become that reference implementation.]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Inconsistency in security implementations is bad. Pick a tool or cloud service that I didn’t cover here to check for XFF behaviour. Can you guess beforehand what you’ll find with any certainty? You can’t, and that’s bad.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If you’re creating a security tool, product, or feature, you need to have as thorough an understanding of the problem space as possible. Even if you start with solid off-the-shelf components (like a token bucket library and expirable cache for a rate limiter), you still need to be fully aware of, for example, the nature and meaning of a header that’s being used as input. Otherwise you run the risk of introducing a new vulnerability (or two, etc.).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If you’re taking untrusted input – like a header value – and putting it into an ever-growing, pretty-long-lived, in-memory structure (like a rate limiter map), you need to be incredibly careful. (Especially if that structure isn’t &lt;em>behind&lt;/em> a rate limiter and instead &lt;em>is&lt;/em> the rate limiter.)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Whenever possible, read the code for your dependencies. It’s hard and a ton of work, but bad security surprises can be worse.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>I have avoided saying that you should only use the rightmost-ish XFF IP and never, ever the leftmost. But, seriously, just don’t use it.&lt;/p>
&lt;h2 id="discussion">
&lt;a class="heading-anchor" href="#discussion">
##
&lt;/a>
Discussion
&lt;/h2>
&lt;p>Comment and discuss at &lt;a href="https://news.ycombinator.com/item?id=30570053">Hacker News&lt;/a>.&lt;/p>
&lt;p>There have been some interesting comments on HN and Reddit. I’ll share some tidbits here.&lt;/p>
&lt;h3 id="rusts-headermapget-returns-first">
&lt;a class="heading-anchor" href="#rusts-headermapget-returns-first">
###
&lt;/a>
Rust’s &lt;code>HeaderMap::get&lt;/code> returns first
&lt;/h3>
&lt;p>HN commenter scottlamb &lt;a href="https://news.ycombinator.com/item?id=30571412">pointed out&lt;/a> that &lt;a href="https://docs.rs/http/0.2.6/http/header/struct.HeaderMap.html#method.get">Rust’s method to return a single header&lt;/a> value also returns the value of the first such header. The commenter checked and discovered that they were using it wrong.&lt;/p>
&lt;h3 id="aws-elbalb-has-an-option-to-make-xff-even-worse">
&lt;a class="heading-anchor" href="#aws-elbalb-has-an-option-to-make-xff-even-worse">
###
&lt;/a>
AWS ELB/ALB has an option to make XFF even worse
&lt;/h3>
&lt;p>HN commenter nickjj brought the AWS ELB/ALB &lt;a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/x-forwarded-headers.html#x-forwarded-for">“client port preservation”&lt;/a> option to my attention. If enabled, the client port number is appended to the IP added to XFF. Turning that option on will a) violate the de facto standard form of the header, and b) mess up a lot of IP parsing code.&lt;/p>
&lt;p>(And if the IP suddenly starts failing, then what? Does the rate limiter logic keep moving to the right until it finds a good IP? If done wrong, that could lead to using untrusted values. Does the rate limiter instead give up? And do what? Fail open? Fail closed? Panic? In a comment on the didip/tollbooth PR for this &lt;a href="https://github.com/didip/tollbooth/pull/99#issuecomment-1059328777">I talk more about this&lt;/a>.)&lt;/p>
&lt;h3 id="consider-a-custom-single-ip-header-if-you-can">
&lt;a class="heading-anchor" href="#consider-a-custom-single-ip-header-if-you-can">
###
&lt;/a>
Consider a custom single-IP header, if you can
&lt;/h3>
&lt;p>HN commenter terom &lt;a href="https://news.ycombinator.com/item?id=30571542">said&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>highly recommended to just override the entire XFF header with a single value at the appropriate point in your stack, if at all possible&lt;/p>
&lt;/blockquote>
&lt;p>Which is good advice and I didn’t really say in the post. If you have the ability to use one of the “good” single-IP headers, or add your own at your first proxy, that’s much better than messing around with XFF.&lt;/p>
&lt;p>(The reason I didn’t really talk about the custom header is like: I was mostly writing for people who are trying to use what’s available rather than doing a lot of proxy tinkering. Or something.)&lt;/p>
&lt;h3 id="gos-nethttphttputilreverseproxy-xff-behaviour-being-re-examined">
&lt;a class="heading-anchor" href="#gos-nethttphttputilreverseproxy-xff-behaviour-being-re-examined">
###
&lt;/a>
Go’s &lt;code>net/http/httputil.ReverseProxy&lt;/code> XFF behaviour being re-examined
&lt;/h3>
&lt;p>Right now, &lt;a href="https://pkg.go.dev/net/http/httputil#ReverseProxy">&lt;code>httputil.ReverseProxy&lt;/code>&lt;/a> appends the client IP to the XFF header. It looks like &lt;a href="https://github.com/golang/go/issues/50465">they are considering&lt;/a> either replacing the existing XFF header by default or adding options to append to, overwrite, or preserve the existing header.&lt;/p>
&lt;p>My gut feeling is that the initial more-knobs-to-turn suggestion in the issue is better than the limited-and-awkward thing it seems to be turning into. (I guess I’ll &lt;a href="https://github.com/golang/go/issues/50465#issuecomment-1059987276">express my opinion&lt;/a> there.)&lt;/p>
&lt;h3 id="thoughts-on-overwriting-the-xff-header">
&lt;a class="heading-anchor" href="#thoughts-on-overwriting-the-xff-header">
###
&lt;/a>
Thoughts on overwriting the XFF header
&lt;/h3>
&lt;p>After being prompted to look at &lt;a href="https://github.com/golang/go/issues/50465">&lt;code>httputil.ReverseProxy&lt;/code>&lt;/a> and &lt;a href="https://github.com/caddyserver/caddy/pull/4507">Caddy&lt;/a>, I started thinking more about overwriting the &lt;code>X-Forwarded-For&lt;/code> header (which they both do). I had previously given the idea only a footnote&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>, but it deserves more consideration.&lt;/p>
&lt;p>The idea is this: In a multi-reverse-proxy scenario, the first proxy replaces any existing XFF header(s) with one containing only &lt;code>RemoteAddr&lt;/code>. All subsequent proxies (configured to trust the previous proxies) then append to the fresh XFF header.&lt;/p>
&lt;p>This approach has an obvious nice property: There are no untrusted values in the XFF list. You can’t possibly choose a spoofed value. But there are also aspects that I don’t like.&lt;/p>
&lt;p>First of all, I think that it teaches bad XFF hygiene and introduces the possibility of mistakes leading to spoofing vulnerability. Because the XFF header is being replaced by the first proxy, the instructions for use become “use the leftmost”. But what if you then swap out your first proxy that doesn’t remove the XFF and instead appends to it? Spoofed!&lt;/p>
&lt;p>Secondly, there’s no configuration-simplicity gain with this approach. Trusted proxies still need to be configured for all proxies but the first. If you’re doing that, you can use the rightmost-ish approach. And the rightmost-ish approach is more robust: Is your XFF overwritten? It works! Is your XFF list appended to? It works! Is your XFF list mostly spoofed? It works! And you’re helping people to understand the right way to think about the XFF header.&lt;/p>
&lt;p>Thirdly, there are still legitimate uses for the leftmost-ish XFF IP (albeit with a lot of warnings attached). If your only mode of operation is to overwrite the XFF header, then you utterly deny those use cases.&lt;/p>
&lt;p>Finally, hand-wavingly, I think this violates the spirit of the headers. XFF is supposed to be a comma-separated list of all the IPs involved. Which overwriting it negates. I think that, instead, &lt;code>X-Real-IP&lt;/code> should be used, set by the first proxy. No need to hijack XFF for this effectively-single-IP purpose.&lt;/p>
&lt;p>However, I wouldn’t fight to the death against overwriting the XFF. It’s still an improvement over the dismal widespread-ness of append-and-use-leftmost.&lt;/p>
&lt;h3 id="envoys-xff-documentation-is-really-something">
&lt;a class="heading-anchor" href="#envoys-xff-documentation-is-really-something">
###
&lt;/a>
Envoy’s XFF documentation is really something
&lt;/h3>
&lt;p>HN commenter jrockway &lt;a href="https://news.ycombinator.com/item?id=30571219">pointed me&lt;/a> at the Envoy Proxy &lt;a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#x-forwarded-for">documentation for XFF use&lt;/a>. It’s not exactly generally educational, but I think it’s a really good effort at making sure that Envoy users don’t shoot themselves in the foot.&lt;/p>
&lt;h3 id="real-world-examples-of-doing-it-wrong">
&lt;a class="heading-anchor" href="#real-world-examples-of-doing-it-wrong">
###
&lt;/a>
Real-world examples of doing it wrong
&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://esd.io/blog/flask-apps-heroku-real-ip-spoofing.html">“Flask apps on Heroku susceptible to IP spoofing”&lt;/a> (2013-05-12). Via &lt;a href="https://news.ycombinator.com/item?id=30574577">eli on HN&lt;/a>. It was using the leftmost.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>“For many years, a very prominent computer science journal used XFF for guarding access — if you set it to an IP of some well-known universities, you’d be able to download all you want.” (&lt;a href="https://news.ycombinator.com/item?id=30572614">HN&lt;/a>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>“I remember the source code for a certain simple web app would check if X-Forwarded-For matched a certain IP as a way of granting admin powers. I spoofed it. It was sort of the first “hack” I ever did.” (&lt;a href="https://old.reddit.com/r/programming/comments/t7lxeb/the_perils_of_the_real_client_ip_or_all_the_wrong/hzka9ma/">Reddit&lt;/a>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>“This was really helpful, I feel good after fixing something I didn’t know was even a problem 🤗” (&lt;a href="https://old.reddit.com/r/programming/comments/t7lxeb/the_perils_of_the_real_client_ip_or_all_the_wrong/hzkmxie/">Reddit&lt;/a>)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="acknowledgements">
&lt;a class="heading-anchor" href="#acknowledgements">
##
&lt;/a>
Acknowledgements
&lt;/h2>
&lt;p>Thanks to &lt;a href="https://github.com/rod-hynes">Rod Hynes&lt;/a> for proofreading and providing feedback. All mistakes are mine, of course.&lt;/p>
&lt;p>Thanks to &lt;a href="https://psiphon.ca">Psiphon Inc.&lt;/a> for giving me the time to work on this. And employing me.&lt;/p>
&lt;h2 id="todo">
&lt;a class="heading-anchor" href="#todo">
##
&lt;/a>
TODO
&lt;/h2>
&lt;ul>
&lt;li>Nodejs collapses XFF &lt;a href="https://nodejs.org/api/http.html#messageheaders">https://nodejs.org/api/http.html#messageheaders&lt;/a> &lt;a href="https://old.reddit.com/r/programming/comments/t7lxeb/the_perils_of_the_real_client_ip_or_all_the_wrong/hzkg18l/">https://old.reddit.com/r/programming/comments/t7lxeb/the_perils_of_the_real_client_ip_or_all_the_wrong/hzkg18l/&lt;/a>&lt;/li>
&lt;li>add note about single-value header Get being combined list&lt;/li>
&lt;li>all projects: if deciding to use leftmost, check for valid/non-private&lt;/li>
&lt;li>finish reference implementation&lt;/li>
&lt;li>probably add some diagrams&lt;/li>
&lt;li>rethink hyphenating rate-limit* (right now I’m not doing it for nouns but am doing it for verbs, and I can’t decide what’s right)&lt;/li>
&lt;li>&lt;a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html#http-463-issues">AWS ALB returns 463&lt;/a> if there are more than 30 XFF IP addresses&lt;/li>
&lt;/ul>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>…Except for anonymizing proxies that specifically hide the client IP. But that’s not an important difference for us here. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">↩︎&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>I’m picking on AWS ALB mostly because that’s what I’m testing with. It’s also pretty popular and probably not unusual. But please append this caveat every time I mention it: “This is AWS ALB’s behaviour by default, or as I understand it.” &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">↩︎&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>I believe this is inherited unchanged into &lt;a href="https://datatracker.ietf.org/doc/html/rfc7540">HTTP/2&lt;/a>. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">↩︎&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>A redditor &lt;a href="https://old.reddit.com/r/programming/comments/t7lxeb/the_perils_of_the_real_client_ip_or_all_the_wrong/hzits6h/">points out&lt;/a> that if you’re running a server for internal usage, and you want to know the originating internal IP, then you &lt;em>do&lt;/em> want an IP in the private address space. Which is true. So if that scenario applies to you, modify the algorithm accordingly (i.e., remove the “non-private” checks). &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">↩︎&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>In the leftmost-ish approach, the IP you need might not be in the first XFF header. In the rightmost-ish approach, it might not be in the last XFF header. &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">↩︎&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Seek a guide &lt;a href="https://docs.fastly.com/signalsciences/faq/real-client-ip-addresses/#nginx---http_realip_module">elsewhere&lt;/a>. &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">↩︎&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Okay, it’s &lt;em>possible&lt;/em> for your trusted proxy to blow away the existing XFF value and start fresh. But a) that’s not how XFF is supposed to work, b) you’re losing potentially useful information, and c) you achieve the same thing by using the rightmost-ish algorithm. &lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">↩︎&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>“Rightmost” is not “rightmost-ish”. &lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">↩︎&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>Of course, the attestation reverse proxy should have set the &lt;code>X-Client-IP&lt;/code> or otherwise passed on the IP it verified. But it’s not hard to imagine such a mistake. &lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">↩︎&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>There is one more sentence elsewhere, but it doesn’t add anything: “With the header field format described in this document, it is possible to know what information belongs together, &lt;em>as long as the proxies are trusted&lt;/em>.” (Emphasis added.) &lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">↩︎&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>The scary state of IPv6 rate-limiting</title><link>https://adam-p.ca/blog/2022/02/ipv6-rate-limiting/</link><pubDate>Sun, 20 Feb 2022 13:44:57 -0500</pubDate><guid>https://adam-p.ca/blog/2022/02/ipv6-rate-limiting/</guid><description>&lt;p>IPv6 rate-limiting is scarily half-baked right now. If you run a server that does any kind of IP-based rate-limiting, consider not enabling IPv6 if possible. If you do use IPv6, check how your rate-limiter actually handles it.&lt;/p>
&lt;h2 id="four-billion-is-a-pretty-small-number">
&lt;a class="heading-anchor" href="#four-billion-is-a-pretty-small-number">
##
&lt;/a>
Four billion is a pretty small number
&lt;/h2>
&lt;p>Most IPv4 rate-limiters will block individual addresses as they exceed the limit. That’s mostly okay, because there are only 4 billion IPv4 addresses. That means a) they are given out with some frugality, and b) it doesn’t take much memory to block a large proportion of them. If you and 1000 of your closest friends launch a brute-force or credential-stuffing login attack, any server will have no problem rate-limiting all of you.&lt;/p>
&lt;p>But IPv6 is a very different matter.&lt;/p>
&lt;h2 id="a-gazillion-ips">
&lt;a class="heading-anchor" href="#a-gazillion-ips">
##
&lt;/a>
A gazillion IPs
&lt;/h2>
&lt;p>When you ask your ISP for an IPv6 assignment, you get &lt;em>at least&lt;/em> a &lt;a href="https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing#IPv6_CIDR_blocks">/64&lt;/a> block – 2⁶⁴ assignable addresses. &lt;a href="https://www.ripe.net/publications/docs/ripe-690">RIPE suggests&lt;/a> giving a /56 prefix (2⁷² addresses == 256 /64 blocks) to home users and a /48 prefix (2⁸⁰ addresses == 65,536 /64 blocks) to businesses (or “If you want a simple addressing plan use a /48 for each end-user”). &lt;a href="https://datatracker.ietf.org/doc/html/rfc6177">RFC 6177&lt;/a> agrees with this guidance, as does &lt;a href="https://blog.apnic.net/2017/07/10/isps-simplifying-customer-ipv6-addressing-part-2/">APNIC&lt;/a>.&lt;/p>
&lt;p>Searching for ISPs’ IPv6 prefix delegation policies shows that /64&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> and /56 are pretty common. Internode in Australia &lt;a href="https://www.internode.on.net/about/our_network/ipv6/">assigns /56 blocks&lt;/a> to residential and business customers. In the US, Charter Spectrum also &lt;a href="https://www.reddit.com/r/ipv6/comments/i1b7nk/charter_spectrum_and_ipv6_with_prefix_delegation/">gives /56s&lt;/a>. Cogent lets users &lt;a href="https://www.cogentco.com/files/docs/customer_service/faq/ipq_na.txt">request up to /48&lt;/a>.&lt;/p>
&lt;p>So, it’s safe to assume that an attacker can obtain at least a /56 and probably a /48. It’s also prudent to assume that a determined attacker can utilize all of the addresses at their disposal. And there is at least one &lt;a href="https://github.com/blechschmidt/freebind">tool that does exactly that&lt;/a> – “freebind: IPv4 and IPv6 address rate limiting evasion tool”.&lt;/p>
&lt;h2 id="whats-the-right-way-to-rate-limit-a-gazillion-ips">
&lt;a class="heading-anchor" href="#whats-the-right-way-to-rate-limit-a-gazillion-ips">
##
&lt;/a>
What’s the right way to rate-limit a gazillion IPs?
&lt;/h2>
&lt;p>This &lt;a href="https://serverfault.com/a/863511/476142">StackOverflow answer&lt;/a> outlines the best approach I’ve found:&lt;/p>
&lt;blockquote>
&lt;p>The best algorithm is to start blocking separate addresses. Then when multiple addresses are blocked in the same /64 you block the whole /64. Repeat that for bigger aggregates.&lt;/p>
&lt;p>Prefixes are usually given out on nibble boundaries (multiples of 4, or one hexadecimal digit). So you might want to scale from /64 to /60, /56, /52, and /48. A /48 is usually the largest prefix given to a single site.&lt;/p>
&lt;p>Depending how careful you want to be you can skip from /64 straight to /56 and /48.&lt;/p>
&lt;/blockquote>
&lt;p>A comment on that answer has a useful addition:&lt;/p>
&lt;blockquote>
&lt;p>You can implement this gradual aggregation approach in a fairly simple way. Track separate rate limits at the /64, /56, and /48 level all the time. Use higher limits for higher levels. That way there is no aggregation logic at all. It’s just three separate limits based on different keys.&lt;/p>
&lt;/blockquote>
&lt;p>(Fun fact: If I google for &lt;a href="https://www.google.com/search?q=ipv6+rate+limiting">“ipv6 rate limiting”&lt;/a> (in a private browsing window), the “featured snippet” at the top is a link to the “rate limiting evasion tool” that I mentioned above. The first normal result is to that SO question. And note that it has only 6 votes and a single answer with only 10 votes. Are people just not thinking/talking about the problem? Or am I searching for the wrong thing?)&lt;/p>
&lt;h2 id="how-are-real-rate-limiters-actually-doing-it">
&lt;a class="heading-anchor" href="#how-are-real-rate-limiters-actually-doing-it">
##
&lt;/a>
How are real rate limiters actually doing it?
&lt;/h2>
&lt;p>Let’s start with &lt;a href="https://support.cloudflare.com/hc/en-us/articles/115001635128-Configuring-Cloudflare-Rate-Limiting">Cloudflare&lt;/a>, since it’s nice and clear:&lt;/p>
&lt;blockquote>
&lt;p>Once an individual IPv4 address or IPv6 /64 IP range exceeds a rule threshold, further requests to the origin web server are blocked&lt;/p>
&lt;/blockquote>
&lt;p>That’s pretty good, though it’s missing some of the nuance of the algorithm above. If there’s a large non-malicious site (apartment complex, school, business, etc.) behind the /64, the blocking might be over-aggressive. If an attacker has an assignment larger than /64, they might have between 256 and 65,536 /64s at their disposal. The large end of that range is getting big.&lt;/p>
&lt;p>AWS WAF supports IPv6 for rules, inspection, and reporting, but doesn’t specify how it implements rate-limiting for IPv6. Concerningly, it has a &lt;a href="https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html">really small limit&lt;/a> on the number of IPs it can rate-limit at once: “AWS WAF can block up to 10,000 IP addresses. If more than 10,000 IP addresses send high rates of requests at the same time, AWS WAF will only block 10,000 of them.” Unless their IPv6-limiting algorithm is smart, it would be easy for an attacker to ensure they have more blockable units (IPs or /64s) than the limiter can hold. And that means that it would effectively be completely unlimited.&lt;/p>
&lt;p>(This raises the question of what the limit on the number of blocked IPs is for other services. I found no such limit mentioned for anything else.)&lt;/p>
&lt;p>I also couldn’t figure out what IPv6 strategy Google Cloud Armor uses, but &lt;a href="https://cloud.google.com/armor/docs/security-policy-overview">it says this&lt;/a> about its configurable rules: “Both IPv4 and IPv6 source addresses are supported, but IPv6 addresses must have subnet masks no larger than /64.” So maybe its rate-limiting is also /64-based, like Cloudflare? Or maybe that’s reading too much into a statement that’s only tangentially related.&lt;/p>
&lt;p>Let’s Encrypt &lt;a href="https://letsencrypt.org/docs/rate-limits/">limits account creations by /48&lt;/a>, because &lt;a href="https://github.com/letsencrypt/boulder/blob/b5b5033136427c988e20ca11f1f7471563f90616/sa/sa.go#L224-L227">“it’s not uncommon for one person to have a /48 to themselves”&lt;/a>. That seems very.. cautious. On the one hand, I like how aggressive it is, but on the other hand… there could be 65,536 home or business networks (/64s) in a single rate-limited /48. I feel like this is too coarse-grained for general use.&lt;/p>
&lt;p>A year ago, &lt;a href="https://hackerone.com/reports/1154003">after a vulnerability report&lt;/a>, Nextcloud changed from limiting IPv6 by individual addresses (/128) to limiting by /64. (There also is/was no size-limiting of the IP cache, &lt;a href="https://github.com/nextcloud/server/tree/master/lib/private/Security/RateLimiting/Backend">that I can see&lt;/a>.)&lt;/p>
&lt;p>I also looked at a couple of Go HTTP rate-limiting libraries – &lt;a href="https://github.com/didip/tollbooth">github.com/didip/tollbooth&lt;/a> and &lt;a href="https://github.com/go-chi/httprate">github.com/go-chi/httprate&lt;/a>. Neither distinguishes between IPv4 and IPv6 and simply does per-IP blocking. So that’s bad. And neither has a size limit on the IPs in its limiter cache (only a time limit), so an attacker can consume all available memory, I think.&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;p>(Fun fact: Even a terabyte drive can only store 2³⁶ IPv6 addresses. So you’d need about 270 million such disks to store the IP addresses accessible to a single /64 home user. Or 18 trillion disks for a /48.)&lt;/p>
&lt;h2 id="how-many-blockable-units-is-too-many-for-an-attacker">
&lt;a class="heading-anchor" href="#how-many-blockable-units-is-too-many-for-an-attacker">
##
&lt;/a>
How many “blockable units” is too many for an attacker?
&lt;/h2>
&lt;p>If a rate limiter is blocking by single IP addresses, then that’s the “blockable unit”&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. If it’s blocking by /64, then that’s the “blockable unit”. And so on. The rate limiter effectively “allows” an attacker to have a certain number of blockable units at her disposal depending on the limiting strategy used.&lt;/p>
&lt;p>The obvious extremes: An attacker having a single blockable unit is acceptable (and unavoidable). An attacker having 2⁶⁴ blockable units is way too many.&lt;/p>
&lt;p>But what if the attacker has 256 blockable units (blocking on /64, attacker has /56)? Or 65,536 blockable units (blocking on /64, attacker has /48)?&lt;/p>
&lt;p>Let’s (charitably) assume that AWS WAF’s limit of blocking “10,000 IP addresses” applies to /64s for IPv6. If that’s true, then allowing an attacker 65,636 is too many. (To state the obvious, an attacker could cycle through her /64s and never be limited at all.)&lt;/p>
&lt;p>Do other WAFs have a size limit that they’re not publishing? It seems likely, but not certain. Cloudflare, for example, prides itself on &lt;a href="https://blog.cloudflare.com/cloudflare-blocks-an-almost-2-tbps-multi-vector-ddos-attack/">withstanding the largest attacks&lt;/a> and is surely concerned about state-level attackers with access to at least a /32 prefix – 4 billion /64s. It would take about 40 GB of storage to keep track of that many prefixes (2³² * (8 bytes per prefix + overhead)). That’s not impossible for a big box of RAM, and certainly not for disk, of course (but disk feels a bit slow for this use case). Perhaps Cloudflare is comfortable with blocking that many addresses.&lt;/p>
&lt;p>A big box of RAM dedicated to this purpose might be expensive for a smaller operator, but maybe using disk is more acceptable. If we’re talking about Nextcloud running on someone’s NAS box, then /32 attacks are surely outside of the threat model.&lt;/p>
&lt;p>What about 256 blockable units? That’s… probably okay?&lt;/p>
&lt;p>So, I don’t have a great answer to the question of how many blockable units is too many. What’s your comfort level? What’s your threat model?&lt;/p>
&lt;p>And what about an attack that is both distributed &lt;em>and&lt;/em> can utilize the full IP space? What &lt;em>multiple&lt;/em> of 65,536 (or 256) are you comfortable with?&lt;/p>
&lt;h2 id="conclusions">
&lt;a class="heading-anchor" href="#conclusions">
##
&lt;/a>
Conclusions
&lt;/h2>
&lt;p>I really like the idea of IPv6. I work for a company that would (probably) benefit from widespread IPv6 adoption (so that we’re, uh, harder to block). But as I said in the title: If you need to rate-limit access to something, avoid enabling IPv6 for now. The state of IPv6 rate-limiting just seems too immature.&lt;/p>
&lt;p>But what if you have no choice? If you’re using a web application firewall, try to talk to the vendor about what it actually does. (And then &lt;a href="mailto:pritchard.adam@gmail.com">let me know what they say&lt;/a>!) If you’re doing the rate-limiting yourself, look closely at what your code is doing, because there’s a very good chance that it’s doing it inadequately.&lt;/p>
&lt;p>For a quick fix, block IPv6 /64s rather than individual IPs. It might not be perfect, but it’s 2⁶⁴ times better.&lt;/p>
&lt;p>I remain hopeful that this situation can improve rapidly. Good algorithms tend to get adopted quickly once they become available in a consumable format, and this isn’t likely a very complex case. (Yes, I am tempted to implement something myself, but this isn’t a problem I personally have right now so I wouldn’t actually use my own code, which is never a good starting point.)&lt;/p>
&lt;h2 id="postscript">
&lt;a class="heading-anchor" href="#postscript">
##
&lt;/a>
Postscript
&lt;/h2>
&lt;p>The state of this seems so obviously sketchy that I think I must be missing something important. I am still an IPv6 neophyte. Please correct me if I have gotten anything wrong.&lt;/p>
&lt;hr>
&lt;p>Edit 2022-02-21: I &lt;a href="https://old.reddit.com/r/ipv6/comments/sx8ko7/the_scary_state_of_ipv6_ratelimiting_please_let/">posted this to /r/ipv6&lt;/a> and there are some good contrary comments there. I particularly like &lt;a href="https://www.reddit.com/r/ipv6/comments/sx8ko7/the_scary_state_of_ipv6_ratelimiting_please_let/hxsbfuh/">this one&lt;/a> that talks about IPv6 being better than IPv4 for rate limiting, since providers will generally have a single IPv6 prefix themselves and give out prefixes in a consistent manner, rather than the scattered, different-IP-each-reboot world of IPv4. The comments also talk a lot more about “bycatch” (over-blocking), which I didn’t really. But I still don’t feel they’re worried enough about how providers and libraries have actually implemented rate limiting at this point in time.&lt;/p>
&lt;hr>
&lt;p>Edit 2022-02-22:&lt;/p>
&lt;p>A coworker pointed out that the way I did the prefixed-IP-canonicalization in my PRs was overly complicated and can be achieved with the stdlib like &lt;code>ipv6.Mask(net.CIDRMask(56, 128)).String()&lt;/code>. I had tried various approaches with the stdlib and didn’t come up with one that worked, but I guess I missed that one. Embarrassing.&lt;/p>
&lt;p>I did some searching for fail2ban+ipv6. Their &lt;a href="https://github.com/fail2ban/fail2ban/issues/1123">IPv6 support master plan&lt;/a> is interesting and relevant. &lt;a href="https://github.com/fail2ban/fail2ban/issues/1123#issuecomment-123746872">For example&lt;/a>: “I am not sure we will land/release 1 [per-IP blocking] alone since, as was stated, it could immediately be exploited by an attacker to cause resources exhaustion/DoS. May be only if treatment of IPv6 addresses would be made optional with a big fat warning on possible ramifications.” Even though it looks like &lt;a href="https://github.com/fail2ban/fail2ban/pull/1374">per-IP IPv6 support&lt;/a> was &lt;a href="https://www.ctrl.blog/entry/fail2ban-ipv6.html">added in 0.10&lt;/a>.&lt;/p>
&lt;p>Reading through all of the comments on that issues suggests that fail2ban still only uses a per-IP strategy to block IPv6. And are aware it’s insufficient. And stopped discussing it a year and a half ago.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Some ISPs also give a small multiple of /64s. But I feel like that case isn’t significantly different from a single /64 for our purposes. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">↩︎&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>After writing this I realized that I’d better be part of the change I want to see, so I submitted PRs to &lt;a href="https://github.com/didip/tollbooth/pull/98">tollbooth&lt;/a> and &lt;a href="https://github.com/go-chi/httprate/pull/10">httprate&lt;/a>. Both have been accepted. But it’s unlikely that the only two rate-limiting libraries I checked are the only two with this problem, so I don’t think this changes the overall point of this post. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">↩︎&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>To be clear, I’m making this term up for convenience of discussion. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">↩︎&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Git Submodule vs Subtree</title><link>https://adam-p.ca/blog/2022/02/git-submodule-subtree/</link><pubDate>Thu, 17 Feb 2022 21:35:37 -0500</pubDate><guid>https://adam-p.ca/blog/2022/02/git-submodule-subtree/</guid><description>&lt;p>Every now and then I need to make a choice between using git submodules or subtrees (or nothing), or I get asked about them by coworkers. This is infrequent enough that I forget some of the details each time and need to refresh my memory. So I wrote up these notes to share with my coworkers and to help my future self. Hopefully they’re of some use to others as well.&lt;/p>
&lt;p>Disclaimer: My experience still isn’t large. I’ve only used each once or twice. And this isn’t a manual for using those commands/tools – it’s just a concise aid for choosing between them.&lt;/p>
&lt;p>TL;DR: Subtree is better if you basically want to forget you have external code, or only infrequently update it; submodule is better if you &lt;em>don’t&lt;/em> want to forget that the code is external, and/or if you maybe want to edit and push it.&lt;/p>
&lt;p>I’d seen a lot of complaining about submodule during research, and had the vague idea that subtree was “better”, but I’ve come to realize that submodule has its place.&lt;/p>
&lt;p>When you use subtree, you’re basically copying a remote code base into your file structure. The auto-commit comment at the moment you do it will record the remote commit hash, but otherwise there’s no indication anywhere in the repo that a) the subtree happened, b) what the remote repo was, or c) what the commit of the remote repo was.&lt;/p>
&lt;p>From then on, any changes to the subtree code will be treated just like changes anywhere else in the repo. Any operations to &lt;code>git subtree push&lt;/code> and &lt;code>git subtree pull&lt;/code> the code does weird git directory slicing (which I’ve used before to create a new repo from the subdirectory of an existing repo, retaining commit history for the files in that directory). It’s okay, but clunky.&lt;/p>
&lt;p>This is in sharp contrast with submodule, where the remote repo code is &lt;em>not&lt;/em> copied. Instead, the remote repo URL is recorded, and its location in the file structure, and the remote repo commit to use. When you clone the repo, you have to separately clone the submodule dependency(ies) (&lt;code>git submodule init&lt;/code>+&lt;code>git submodule update&lt;/code> or &lt;code>git clone --recurse-submodules&lt;/code>). The submodule becomes a separate repo in the file structure, with its own &lt;code>.git&lt;/code> dir. For example, if you do &lt;code>git status&lt;/code> in the submodule directory, it tells you the status of the submodule code and not the outer repo. If you update the submodule repo directory, it changes the submodule’s tracked commit in the outer repo, and you commit that in the outer repo.&lt;/p>
&lt;p>Treating the submodule as a separate entity is easy. You can just do all your usual branching, committing, pushing (the remote is the subrepo remote, not the outer remote).&lt;/p>
&lt;p>Cloning the subtree is easier because there’s no separate step, but that’s not a big deal. For vendoring 3rd party code, submodule won’t actually make a copy, so that’s not okay&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> (unless you fork the 3rd party code first and submodule the fork).&lt;/p>
&lt;p>So the main decision factors are like:&lt;/p>
&lt;ul>
&lt;li>Use &lt;strong>subtree&lt;/strong> when you just want to copy code from an external repo once, or maybe with occasional pulls.&lt;/li>
&lt;li>Use &lt;strong>submodule&lt;/strong> when you want to make your relationship to the external repo really explicit, or if you intend to make changes to the submodule code from within the context of your repo and push to the external repo.&lt;/li>
&lt;/ul>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Depending on your vendoring policy and the dependency management system. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">↩︎&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Diving into Go's HTTP server timeouts</title><link>https://adam-p.ca/blog/2022/01/golang-http-server-timeouts/</link><pubDate>Sat, 15 Jan 2022 00:00:00 +0000</pubDate><guid>https://adam-p.ca/blog/2022/01/golang-http-server-timeouts/</guid><description>&lt;p>Recently, I was adding timeouts to a Go HTTP server and ended up exploring how the different settings and approaches act and interact. I’m going to publish my notes here, along with the code I used for testing. Hopefully this will help someone else (or myself) in the future.&lt;/p>
&lt;p>The timeout testing client can be found here: &lt;a href="https://github.com/adam-p/httptimeout">github.com/adam-p/httptimeout&lt;/a>. There is a server in the examples directory that you can make requests to.&lt;/p>
&lt;p>I link to it below, but I’m going to recommend here that you read Filippo Valsorda’s post &lt;a href="https://blog.cloudflare.com/exposing-go-on-the-internet/">“So you want to expose Go on the Internet”&lt;/a>. It’s essential, but I didn’t find it had enough quite enough detail about timeouts, hence the below examination.&lt;/p>
&lt;hr>
&lt;p>There are two different, overlapping levels of timeout in our HTTP server:&lt;/p>
&lt;ol>
&lt;li>Read, write, and idle timeouts on the http.Server&lt;/li>
&lt;li>The ServeHTTP timeout (this middleware)&lt;/li>
&lt;/ol>
&lt;p>The &lt;a href="https://pkg.go.dev/net/http#Server">http.Server timeouts&lt;/a> are overlapping and somewhat confusing (to me &lt;a href="https://github.com/golang/go/issues/35626">and others&lt;/a>) so I’ll test and detail how they work (or seem to). (Another important but insufficiently thorough reference is the Cloudflare post &lt;a href="https://blog.cloudflare.com/exposing-go-on-the-internet/#timeouts">“So you want to expose Go on the Internet”&lt;/a>.)&lt;/p>
&lt;ul>
&lt;li>IdleTimeout: “IdleTimeout is the maximum amount of time to wait for the next request when keepalives are enabled. If IdleTimeout is zero, the value of ReadTimeout is used.” Not relevant to request timeouts.&lt;/li>
&lt;li>ReadTimeout: “The maximum duration for reading the entire request, including the body.” It’s implemented in net/http by calling SetReadDeadline immediately after Accept.&lt;/li>
&lt;li>ReadHeaderTimeout: “ReadHeaderTimeout is the amount of time allowed to read request headers.” Implemented as above.&lt;/li>
&lt;li>WriteTimeout: “WriteTimeout is the maximum duration before timing out writes of the response. It is reset whenever a new request’s header is read.” This effectively covers the lifetime of the ServeHTTP handler stack.&lt;/li>
&lt;/ul>
&lt;p>Observations:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The documentation makes a big deal out of ReadHeaderTimeout allowing for per-request timeouts based on the headers. “The connection’s read deadline is reset after reading the headers and the Handler can decide what is considered too slow for the body.” “Because ReadTimeout does not let Handlers make per-request decisions on each request body’s acceptable deadline or upload rate, most users will prefer to use ReadHeaderTimeout.” But since http.Request doesn’t provide access to the underlying net.Conn, I don’t see a way to set a connection deadline from the handler level. (Perhaps it intends the per-request timeout to be done via mw/context rather than via conn deadlines.)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Our TLS terminates at the load balancer, so mention of different TLS behaviour you might see doesn’t apply.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The zero values mean no timeout. These shouldn’t be used for anything but toy servers.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A timeout during header or body read means that there’s no response to the client. This is unfortunate but expected.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A timeout during header read means that there’s no server log written for the request. This is even more unfortunate but also not unexpected. The handler stack (including logging middleware) is not set up until the headers are read.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>http.Server timeouts do not themselves cancel the request context. However, if a body read follows the timeout, the resulting error will &lt;a href="https://github.com/golang/go/blob/24239120bfbff9ebee8e8c344d9d3a8ce460b686/src/net/http/server.go#L740">trigger a context cancellation&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A ReadTimeout during body read results in a log with status of 503. This is initially somewhat surprising. The timeout interrupts the read connection, then the failed read attempt cancels the request context, then the http.TimeoutHandler (discussed below) receives the signal of that cancellation and &lt;a href="https://github.com/golang/go/blob/24239120bfbff9ebee8e8c344d9d3a8ce460b686/src/net/http/server.go#L3392">sends the 503 response&lt;/a>.&lt;/p>
&lt;p>This is okay, but I’d prefer more control over it. (This might be a bigger problem later, when we try to handle “context canceled” with more nuance.)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The previous point illustrates (I think) that the read and write channels of the connection are severed by the timeouts separately (the response can be written even though the read is interrupted).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ReadHeaderTimeout by itself works as expected. The header read is deadlined, but nothing else is.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ReadTimeout by itself works as expected. The timeout is shared between the header read and body read.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ReadHeaderTimeout and ReadTimeout together:&lt;/p>
&lt;ul>
&lt;li>If set to the same value, behaviour is indistinguishable from just ReadTimeout being set.&lt;/li>
&lt;li>If ReadHeaderTimeout is a different value from ReadTimeout:
&lt;ul>
&lt;li>If the header read is too slow, then ReadHeaderTimeout is used.&lt;/li>
&lt;li>If the body read is too slow, then ReadTimeout is used. The time allowed for the body read is the total ReadTimeout minus the time spent reading headers. (As in the ReadTimeout-by-itself case.)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>I haven’t figured out (in a reasonable amount of time) a way to emulate/implement a slow read. I don’t know how to stream the response and read of it.&lt;/p>
&lt;ul>
&lt;li>But if the WriteTimeout is set to 1ns the client gets EOF immediately.&lt;/li>
&lt;li>A sleep longer than WriteTimeout before writing the response results in the client getting no data, but the client still takes the sleep-time to disconnect rather than the timeout-time, which seems very strange to me.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.cloudflare.com/exposing-go-on-the-internet/#timeouts">One of the sources&lt;/a> led me to believe that ReadHeaderTimeout+WriteTimeout would cover the whole request (“ReadHeaderTimeout … covers up to the request headers … WriteTimeout normally covers the time from the end of the request header read to the end of the response write (a.k.a. the lifetime of the ServeHTTP)”). What actually happens is that the header read timeout is correct, the write timeout is correct, but there’s no body read timeout. So the request can spend forever reading the body but when it goes to write the response the write connection has deadlined.&lt;/p>
&lt;p>I believe that what’s happening is that the WriteTimeout is reset every time a read happens, so it’s not actually starting as long as there’s a body read. (The documentation says WriteTimeout “is reset whenever a new request’s header is read.” But that doesn’t seem to be exactly accurate.)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Otherwise rough testing with combinations of the read timeouts with WriteTimeout suggests they behave as expected (no interaction).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>In addition to the http.Server timeouts we use a timeout middleware, which is basically a wrapper around &lt;a href="https://pkg.go.dev/net/http#TimeoutHandler">http.TimeoutHandler&lt;/a>. Here are some observations when the timeout middleware is in play and has a timeout shorter than the connection timeouts:&lt;/p>
&lt;ul>
&lt;li>Unsurprisingly, the timeout mw’s timeout doesn’t start ticking until the handler stack is set up, so not until after the headers are read.&lt;/li>
&lt;li>http.TimeoutHandler uses “503 Service Unavailable” as its timeout response. It seems like “408 Request Timeout” would be a more semantically appropriate response. We could intercept the response write to change that code, but it would get hack-y to distinguish between http.TimeoutHandler returning 503 and, say, our Ping endpoint returning it intentionally. Additionally, returning a 5xx error means that our clients will automatically retry the request, which is a good thing (probably).
&lt;ul>
&lt;li>We could also use a copy of http.TimeoutHandler (~220 lines) to return whatever value we want.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>It may seem silly to worry about sending a response to the client when its connection is so degraded that it probably can’t read it. But: a) the timeout response might be a lot smaller than whatever the client is trying to send, b) the client’s down pipe might be faster than its up pipe, and c) the timeout might actually be due to our server taking too long to the process, rather than a problem with the client.&lt;/li>
&lt;li>Whether the client receives the timeout mw 503 response depends on what it’s doing. (My test client that gets interrupted writing slowly can’t read the response, but if it’s trying to read when the timeout happens the response is received okay.)&lt;/li>
&lt;li>A slow body read is interrupted by the mw timeout with an “i/o timeout” error. I believe this is due to the request context being canceled by the timeout.&lt;/li>
&lt;li>A long time.Sleep isn’t magically interrupted, unsurprisingly. But selecting on ctx.Done and time.After ends early due to the context cancellation.&lt;/li>
&lt;li>There are &lt;a href="https://cs.opensource.google/go/go/+/refs/tags/go1.18beta1:src/net/http/server.go;l=3392-3402;bpv=0">two cases&lt;/a> when TimeoutHandler returns 503. The first is, of course, when the deadline it set on the context fires (it could have been set somewhere else, in theory). The other is if the context was canceled for some other reason (such as the client leaving). They are distinguishable from the client side because there’s no response body in the latter case.&lt;/li>
&lt;/ul>
&lt;p>Note that it is important that the timeout mw have a shorter timeout than the http.Server timeouts. We want the client to receive a response, if possible, rather than just having its connection severed.&lt;/p>
&lt;p>This is not as simple as it might seem. The handler mw timeout must be shorter than either the WriteTimeout or the time remaining to the ReadTimeout after header reading. But at the handler level we don’t know how long the header read took, except that it took less than ReadHeaderTimeout. So our mw timeout should be &lt;code>min(WriteTimeout, ReadTimeout-ReadHeaderTimeout)&lt;/code>.&lt;/p>
&lt;p>…Except that calculation ends up feeling very unnatural in practice. Instead, it makes more sense to first choose the desired handler timeout, then set the http.Server connection timeouts based on that. I think that it’s reasonable to use 0.5x the timeout for ReadHeaderTimeout and 1x the timeout for ReadTimeout and WriteTimeout.&lt;/p>
&lt;p>We certainly can’t rely on the timeout mw while reading headers (because there is no middleware at that point), but it’s possible that body read and response write timeouts are redundant. Severing the connection seems safer than cancelling the context and hoping something checks it, so we’ll set the other timeouts anyway.&lt;/p>
&lt;hr>
&lt;h2 id="addendum">
&lt;a class="heading-anchor" href="#addendum">
##
&lt;/a>
Addendum
&lt;/h2>
&lt;h3 id="lets-work-through-the-timeout-math">
&lt;a class="heading-anchor" href="#lets-work-through-the-timeout-math">
###
&lt;/a>
Let’s work through the timeout math
&lt;/h3>
&lt;p>Let’s say we want, generally, a 10-second request timeout. So we set TimeoutHandler’s timeout to 10 seconds.&lt;/p>
&lt;p>We need to pick a ReadHeaderTimeout that is basically independent from that (because the handler timeout doesn’t start until &lt;em>after&lt;/em> the header read is complete). It seems reasonable to pick 5 seconds.&lt;/p>
&lt;p>As discussed above, we prefer the ReadTimeout to be longer than the handler timeout, so the client has a chance of getting the response. Because ReadTimeout ticks away during the header read, the calculation for this is something like:&lt;/p>
&lt;pre tabindex="0">&lt;code>ReadTimeout := handler_timeout + ReadHeaderTimeout + wiggle_room
e.g.,
= 10s + 5s + 200ms
&lt;/code>&lt;/pre>&lt;p>So even if the header read takes 4.9s, we are still left with 10.3s for the body read – slightly longer than the handler timeout.&lt;/p>
&lt;p>WriteTimeout covers from the end of the reads until the end of writing. If there’s no body to read, this is the whole post-header request time. So, we want it to be &lt;code>hander_timeout + wiggle_room&lt;/code>, so something like 10.2s.&lt;/p>
&lt;p>IdleTimeout… is independent of any of this stuff. It seems common to set it to a couple of minutes.&lt;/p>
&lt;h3 id="aws-observations">
&lt;a class="heading-anchor" href="#aws-observations">
###
&lt;/a>
AWS observations
&lt;/h3>
&lt;p>Using an AWS load balancer in front of your Go server muddies the behaviour of some of these timeouts, but doesn’t completely obviate them.&lt;/p>
&lt;p>ALB seems to buffer all the incoming headers, so ReadHeaderTimeout does nothing. ALB’s timeout for reading headers appears to be 60 seconds.&lt;/p>
&lt;p>ALB doesn’t seem to have a body-read timeout (or at least not one shorter than a couple of minutes). It does seem to be buffering some of the incoming body, since the client can still send some data after the backend server has given up the connection. About 30 seconds after the server drops the connection, the load balance responds with 502 Bad Gateway.&lt;/p>
&lt;p>I didn’t test the write timeout, but I bet there isn’t one.&lt;/p>
&lt;p>The ALB idle timeout seems to be 60 seconds.&lt;/p></description></item><item><title>The Ethics of Driving Speed in Travel Time Estimation</title><link>https://adam-p.ca/blog/2021/11/ethics-travel-time/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://adam-p.ca/blog/2021/11/ethics-travel-time/</guid><description>&lt;p>How should travel time be estimated? What are the ethical implications of the approach taken?&lt;/p>
&lt;p>You enter your destination into your maps app. It finds a few likely routes. It determines the distance of each pretty easily. It checks traffic conditions along the routes. But we don’t yet have a travel time estimate. Time equals distance divided by speed, adjusted for traffic.&lt;/p>
&lt;p>What travel speed (traffic notwithstanding) does the app use?&lt;/p>
&lt;ol>
&lt;li>The speed limit along the route.&lt;/li>
&lt;li>The speed at which the app user typically drives along the route.&lt;/li>
&lt;li>The speed at which other motorists typically drive along the route.&lt;/li>
&lt;/ol>
&lt;p>We’ll set aside option #2 for now. There are many cases where the user has never made the trip that they’re planning. We’ll discuss an expanded version of it below.&lt;/p>
&lt;p>The problem with picking between the other two options is that a) most people drive above the speed limit&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> and b) doing so is breaking the law.&lt;/p>
&lt;p>If we use the speed limit method and our user drives the speed limit, our estimate is good. But most people drive faster than the speed limit, so our estimated time will typically be too high. The user will leave earlier than they need to, drive faster than we estimated, and arrive earlier than they intended to.&lt;/p>
&lt;p>If we use the typical speed method and our user drives the typical speed, our estimate is good. But if our user is in the minority of law-abiding motorists, we underestimate their travel time and the user arrives late.&lt;/p>
&lt;p>So there’s the quandary: The speed limit method is incorrect for most people and the typical speed method encourages illegal behaviour and punishes law-abiding drivers (by making them late).&lt;/p>
&lt;p>I think that the speed limit approach is more ethically palatable, because it encourages legal behaviour and because being early is almost always better than being late. But giving travel time estimates that are usually incorrect is extremely unappealing – your maps app isn’t very good if one of your fundamental features is usually wrong.&lt;/p>
&lt;p>So what do maps apps do and what should they do? To be clear, I know nothing about this domain. I own a car and a cell phone and that’s the extent of my qualifications. But we can have fun thinking about it…&lt;/p>
&lt;p>One approach could be to split the difference. Give a time estimate in between the speed limit and the speed people usually drive. This doesn’t feel great because a) it’s still usually going to assume an illegal speed, and b) it’s still likely to be incorrect for many people. Nevertheless, I feel like it’s probably a good approach. It still errs on the side of being early, is closer to being correct for more people than the speed limit approach, and will result in a smaller speeding fine if followed closely.&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;p>It’s also worth mentioning that below a certain speeding threshold, the probability of getting a ticket approaches zero. (Though this claim is confounded by automated systems, like speeding cameras.)&lt;/p>
&lt;p>Another approach is to reintroduce option #2 from above, with some extra magic sprinkled on top. The app may not have seen the user drive the exact desired route before, but it has likely seen the user drive similar roads with similar speed limits and can make a very good guess about how fast the user will actually drive.&lt;/p>
&lt;p>This is a very nice approach. It results in the most accurate predictions for the most users. And it largely allows the app developers/ethicists (probably one and the same) to wash their hands of the charge that they’re requiring people to break the law to avoid being late. If the user was law-abiding, they’d get law-abiding directions!&lt;/p>
&lt;p>(For the brief amount of time before there’s any data about the user… probably keep it clean by giving a speed-limit estimate.)&lt;/p>
&lt;p>I don’t know what apps really do. After messing around with Google Maps for a while I managed to &lt;a href="https://goo.gl/maps/8bjzqbrAkTg1QXh56">find a route&lt;/a>&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> that seems to encourage breaking the speed limit.&lt;/p>
&lt;p>&lt;img src="https://adam-p.ca/img/blog/401-travel-time.png" alt="Google maps travel time estimate showing 9 minutes for 16.4 kilometers">&lt;/p>
&lt;p>16.4 kilometers in 9 minutes is 109 kilometers per hour, and the speed limit on highway 401 is 100 km/h. But… if that 9 minutes is rounded down from 9.9 minutes, then the speed drops to 99.4 km/h. And it did take me about 10 tries to find a route that exceeded the limit. (I hunted in the Toronto area because I know the speed limits, but there are probably much better, longer, traffic-free stretches elsewhere that would be better experiments. Except now it’s snowy everywhere that I know, and that will also surely factor into estimates.)&lt;/p>
&lt;p>Is it okay for a maps app to encourage us to ever break the legal speed limit, even if our previous behaviour – or the behaviour of others on the same stretch of road – indicates that we likely will anyway?&lt;/p>
&lt;p>Even seemingly mundane automated systems can have ethical impacts. As consumers of such systems we need to be cognizant of what behaviours such systems are pushing us towards (and away from), and we should get in the habit of consciously and explicitly asking ourselves how we’re being influenced.&lt;/p>
&lt;p>For those of us who are developers of such systems, we need to make a habit of consciously stepping back and thinking about the impact of our design decisions on our users. What’s optimal might not always be what’s ethical. And for any non-trivial ethical question, it should be discussed with others. It’s difficult to see the ethical traps in one’s own design and even harder to find better ways out of them – the perspective of others is invaluable.&lt;/p>
&lt;p>Disclaimer: As I said above, I have no domain knowledge here. I did some googling to see if there was discussion or papers about this and found nothing, but it’s entirely likely I wasn’t searching for the right words.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>This might not be true everywhere, but it sure is where I live. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">↩︎&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>There’s also a conversation worth having about the immorality of breaking the law to speed. And if the moral violation is lesser or greater depending on how badly you exceed the limit, or if being in sin is a binary state. Not a conversation I particularly want to have here, with myself, though. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">↩︎&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Using a &lt;a href="https://www.google.ca/maps/dir/43.8049634,-79.133491/43.8671479,-78.9525895/@43.7902468,-79.1002391,12.33z/data=!4m2!4m1!3e0">longer link&lt;/a> in case that shortened one breaks. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">↩︎&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Instructions on the ceiling</title><link>https://adam-p.ca/inco/2021/11/ftnu/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://adam-p.ca/inco/2021/11/ftnu/</guid><description>&lt;p>&lt;a href="https://fromtheneckupmassage.com/">From the Neck Up&lt;/a> does neck-up massage, including intra-oral. When the RMT has their fingers in your mouth it’s pretty hard to give directions. So there are hand signs for communicating with the RMT. And, since you’re laying on your back during the procedure, the guide to the signs are posted on the ceiling.&lt;/p>
&lt;p>&lt;img src="https://adam-p.ca/img/inco/ftnu.jpg" alt="ceiling sign">&lt;/p></description></item><item><title>Timing attack mitigation must exclude network</title><link>https://adam-p.ca/blog/2021/11/constant-time-network/</link><pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate><guid>https://adam-p.ca/blog/2021/11/constant-time-network/</guid><description>&lt;p>TL;DR: When trying to prevent timing attacks (e.g., against login username enumeration) by making a request take constant time, make sure you exclude the network read and write time. If you don’t, an attacker can slow down their request to bypass it.&lt;/p>
&lt;p>I’ll be covering some background and contextual information here. If you don’t need it, skip to “Exclude network time from constant-time limiting”.&lt;/p>
&lt;h2 id="what-is-a-timing-attack">
&lt;a class="heading-anchor" href="#what-is-a-timing-attack">
##
&lt;/a>
What is a “timing attack”?
&lt;/h2>
&lt;p>Briefly, a timing attack (in this context) is when an attacker observes the time it takes for a server to handle a request to glean some information about the validity of the input they tried. The typical target for this attack is the login request, and in that context there are – unsurprisingly – two pieces of information that can be attacked: username and password.&lt;/p>
&lt;p>Here’s a typical login flow, which we’ll reference below:&lt;/p>
&lt;ol>
&lt;li>Read the request from the client.&lt;/li>
&lt;li>Parse the request. Check for basic validity.&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>If the check fails, go to step 6, indicating a 400 response.&lt;/li>
&lt;/ul>
&lt;ol start="3">
&lt;li>Look up the username in the DB. Retrieve the hashed&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> password.&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>If the username is not found, go to step 6, indicating a 401 response.&lt;/li>
&lt;/ul>
&lt;ol start="4">
&lt;li>Hash the incoming password and compare against the stored one.&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>If the password does not match, go to step 6, indicating a 401 response.&lt;/li>
&lt;/ul>
&lt;ol start="5">
&lt;li>Do other work to set up the login session.&lt;/li>
&lt;li>Write the response to the client.&lt;/li>
&lt;/ol>
&lt;h3 id="testingdiscoveringenumerating-usernames">
&lt;a class="heading-anchor" href="#testingdiscoveringenumerating-usernames">
###
&lt;/a>
Testing/discovering/enumerating usernames
&lt;/h3>
&lt;p>The time that step #4 takes can be used to test for the existence of a username. If the username isn’t found in the DB, the response will come a little faster than if the username is found and then a password hash-and-compare occurs. So an attacker can try out usernames and watch the response time to find out when one exists in the database.&lt;/p>
&lt;p>(Note that multiple requests for a single username may be required to nail down the subtle timing differences.)&lt;/p>
&lt;p>An attacker might use this for testing variations on a single username to target a particular user, or might use huge common username lists or even brute force to enumerate all or nearly all of your users.&lt;/p>
&lt;h4 id="why-do-i-care">
&lt;a class="heading-anchor" href="#why-do-i-care">
####
&lt;/a>
Why do I care?
&lt;/h4>
&lt;p>Maybe you don’t. Maybe you respond with different information depending on whether the username or password is incorrect, because that’s more user-friendly. Maybe your site/service is innocuous, there’s no sensitive information, or everything is public. Maybe you’re confident that your web application firewall or CAPTCHA or other mitigations will prevent this kind of attack.&lt;/p>
&lt;p>&lt;em>I&lt;/em> care because &lt;a href="https://psiphon.ca">I work on a tool&lt;/a> that’s questionably legal in many countries. Users have a habit of putting their real name in their username or reusing usernames across multiple sites, many of which will connect that username to their real identity. So I – and we – think it best that we limit username testing as best we can (and encourage users to &lt;a href="https://psiphon.ca/en/faq.html#psicash-pseudonym">use pseudonyms&lt;/a>).&lt;/p>
&lt;h3 id="testing-passwords">
&lt;a class="heading-anchor" href="#testing-passwords">
###
&lt;/a>
Testing passwords
&lt;/h3>
&lt;p>A very bad and wrong way of checking for a password match is to do a simple string comparison on the plaintext password. First of all, you shouldn’t be storing plaintext passwords in your DB. Secondly, and relevant to timing attacks, doing a simple string comparison will result in different times taken depending on how many characters in the string match. The string comparison is likely doing a length equality check and then doing a character-by-character equality check, so it’s going to return false early as soon as there’s a mismatch.&lt;/p>
&lt;p>This is a solved problem. Use a library with a proper password-hashing algorithm, and use its constant-time equality function. (And don’t just binary-compare the hashes, since the time that takes might also leak something.)&lt;/p>
&lt;h3 id="other-types-of-requests">
&lt;a class="heading-anchor" href="#other-types-of-requests">
###
&lt;/a>
Other types of requests
&lt;/h3>
&lt;p>“Forgot my password” requests are similarly vulnerable. Typically, the user enters a username or email address, then there’s a lookup to see if the account exists and maybe whether the email address is confirmed, then a token gets generated and stored, then the recovery email is sent. As with the login flow, there are processing differences depending on whether the username or email is found or not, which means timing differences that can be used to discover if the input exists in the DB.&lt;/p>
&lt;p>The same considerations about caring mentioned above apply here as well.&lt;/p>
&lt;h2 id="mitigating-timing-attacks">
&lt;a class="heading-anchor" href="#mitigating-timing-attacks">
##
&lt;/a>
Mitigating timing attacks
&lt;/h2>
&lt;h3 id="preventing-automated-requests">
&lt;a class="heading-anchor" href="#preventing-automated-requests">
###
&lt;/a>
Preventing automated requests
&lt;/h3>
&lt;p>Using CAPTCHAs, rate limiting, or a web application firewall can help prevent automated requests that are attempting to enumerate your users. They won’t generally help with targeted username testing.&lt;/p>
&lt;p>You should certainly employ these kinds of measures, but my opinion is that they should be used alongside other mitigations.&lt;/p>
&lt;h3 id="randomizing-response-times-not-recommended">
&lt;a class="heading-anchor" href="#randomizing-response-times-not-recommended">
###
&lt;/a>
Randomizing response times (not recommended)
&lt;/h3>
&lt;p>If a random sleep is added to the processing of sensitive requests, then the response timing becomes more difficult to use for timing attacks. But only “more difficult” – with enough timing samples, the average can be taken and the attack again becomes viable.&lt;/p>
&lt;h3 id="constant-time-responses">
&lt;a class="heading-anchor" href="#constant-time-responses">
###
&lt;/a>
Constant-time responses
&lt;/h3>
&lt;p>We’re getting closer to the point of this post now.&lt;/p>
&lt;p>Forcing responses to take a fixed amount of time prevents timing analysis. If every response, regardless of input, takes the same amount of time, there’s nothing to differentiate and analyze.&lt;/p>
&lt;p>The constant time value should be chosen to exceed the possible natural response time. There are likely going to be outliers where the natural response time exceeds the constant time – you should log and alert these incidents, as they ruin the mitigation. But, generally, if they’re kept very infrequent they still won’t provide an attacker enough to work with.&lt;/p>
&lt;p>Another approach to constant-time excesses would be to have multiple increments of constants. Like, limit the response to 1 second; but if it naturally takes more than 1 second, limit it to 2 seconds; etc. I’m not sure if this is warranted or adds very much. I wouldn’t bother. You will also have to be very sure that, say, bad username doesn’t always end up in the first time increment while bad passwords always end up in the second.&lt;/p>
&lt;h4 id="exclusions">
&lt;a class="heading-anchor" href="#exclusions">
####
&lt;/a>
Exclusions
&lt;/h4>
&lt;p>Since we’re only trying to prevent an attacker from distinguishing between “bad username” and “good username but bad password”, then any situations that don’t reveal that can be excluded from having a constant-time response. For example:&lt;/p>
&lt;ul>
&lt;li>Successful login. The user knows that the username and password were both good, so a constant-time response achieves nothing except slowing down the valid-user experience.&lt;/li>
&lt;li>“400 Bad Request” responses. For example, if the username is too long or has invalid characters in it. No account lookup is done, so nothing is revealed.&lt;/li>
&lt;/ul>
&lt;p>It might be tempting to also exclude 500 server errors. In theory, something like a DB communication error shouldn’t reveal information about the username or password. But such errors can occur for many reasons, some of which may be repeatable by an attacker. It’s probably best to keep server errors constant-time, if possible. (Also, such errors should be extremely rare for benign users and shouldn’t significantly impact the experience of the service.)&lt;/p>
&lt;h2 id="exclude-network-time-from-constant-time-limiting">
&lt;a class="heading-anchor" href="#exclude-network-time-from-constant-time-limiting">
##
&lt;/a>
Exclude network time from constant-time limiting
&lt;/h2>
&lt;p>We’ve finally gotten to the point.&lt;/p>
&lt;p>When forcing a response to be constant time, the network portion of the request processing must be excluded. By this I mean the time taken to read the request from the client and the time taken to write the response. We’ll see that it’s both &lt;em>acceptable&lt;/em> and &lt;em>necessary&lt;/em> to do so.&lt;/p>
&lt;p>The login flow will end up looking like this:&lt;/p>
&lt;ol>
&lt;li>Read the request from the client.&lt;/li>
&lt;li>&lt;em>Record the response start time.&lt;/em>&lt;/li>
&lt;li>Parse the request. Check for basic validity.&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>If the check fails, go to step 7, indicating a 400 response.&lt;/li>
&lt;/ul>
&lt;ol start="4">
&lt;li>Look up the username in the DB. Retrieve the hashed password.&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>If the username is not found, go to step 7, indicating a 401 response.&lt;/li>
&lt;/ul>
&lt;ol start="5">
&lt;li>Hash the incoming password and compare against the stored one.&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>If the password does not match, go to step 7, indicating a 401 response.&lt;/li>
&lt;/ul>
&lt;ol start="6">
&lt;li>Do other work to set up the login session.&lt;/li>
&lt;li>&lt;em>Wait until the constant-time limit has passed since the start time.&lt;/em>&lt;/li>
&lt;li>Write the response to the client.&lt;/li>
&lt;/ol>
&lt;p>First of all, it is &lt;em>acceptable&lt;/em> to exclude the network transfers from the constant-time limit because they are completely unaffected by the validity of the input. Additionally, the attacker controls the network input (the request) and has full visibility of the network output (the response). There is nothing to hide here.&lt;/p>
&lt;p>Secondly, it is &lt;em>necessary&lt;/em> to exclude the network time.&lt;/p>
&lt;p>If the request read time is included in the constant-time consideration, the mitigation is effectively undermined. The attacker controls the client’s network speed, so they can trickle the request out until the constant-time limit is passed. Then they start measuring the response time from the point that the request writing is done. The server will then start processing the request. There will be no sleep in step #7 because the constant-time limit has already been exceeded, so the actual processing time will be plainly visible to the attacker.&lt;/p>
&lt;p>(If the “multiple increments of constants” approach is used, the attack is more complicated, but I believe it’s still viable. The attacker will need to tweak the request speed so that bad-username requests fall into the first increment while bad-password requests fall into the second. Anyway, this hardly matters, since it’s acceptable to exclude the request read time.)&lt;/p>
&lt;p>Excluding the response writing time is also necessary. As soon as the first byte of the response is written, the attacker is signaled that the processing is complete and they have the information they need so any constant-time sleeping needs to occur before the response writing is begun.&lt;/p>
&lt;h2 id="other-concerns">
&lt;a class="heading-anchor" href="#other-concerns">
##
&lt;/a>
Other concerns
&lt;/h2>
&lt;p>I worry about the attacker using a simultaneous request flood to slow down all other request processing enough that many or all login requests will exceed the constant-time limit and start revealing the true processing time. Sufficient capacity and/or scaling, combined with anti-denial-of-service measures should be sufficient to mitigate this. It will also be a pretty expensive – and therefore unlikely – approach for an attacker to take.&lt;/p>
&lt;h2 id="not-just-http">
&lt;a class="heading-anchor" href="#not-just-http">
##
&lt;/a>
Not just HTTP
&lt;/h2>
&lt;p>I wrote the above in terms of HTTP requests and responses, but it applies to any network protocol.&lt;/p>
&lt;h2 id="final-words">
&lt;a class="heading-anchor" href="#final-words">
##
&lt;/a>
Final words
&lt;/h2>
&lt;p>I’m writing this because when I was implementing this I screwed up by including the network transfer in the constant-time limit – it felt cleanest to implement it as middleware, but that was at the wrong level. I only realized the problem while re-reading some tangentially-related code. Hopefully this helps someone else not make the same mistake.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Hashed using something appropriate, like argon2, scrypt, or bcrypt. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">↩︎&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Make sure you have a backup Yubikey</title><link>https://adam-p.ca/blog/2021/06/backup-yubikey/</link><pubDate>Fri, 25 Jun 2021 00:00:00 +0000</pubDate><guid>https://adam-p.ca/blog/2021/06/backup-yubikey/</guid><description>&lt;p>For four years I carried a &lt;a href="https://support.yubico.com/hc/en-us/articles/360013714579-YubiKey-NEO">Yubikey NEO&lt;/a> (USB Type-A) in my pocket, on my keychain. And then it died (would no longer be recognized by any computer).&lt;/p>
&lt;p>&lt;a href="https://web.archive.org/web/20170212170854/https://www.yubico.com/products/yubikey-hardware/yubikey-neo">Yubikey’s durability claim&lt;/a> was:&lt;/p>
&lt;blockquote>
&lt;p>Crush-resistant and waterproof, YubiKey NEO is practically indestructible during normal use, weighs only 3g, and attaches to your keychain alongside your house and car keys&lt;/p>
&lt;/blockquote>
&lt;p>So, they didn’t explicitly say “carrying it on your keychain won’t kill it”, but they sure did imply it.&lt;/p>
&lt;p>The wording for their &lt;a href="https://www.yubico.com/ca/product/yubikey-5-nfc/">new USB Type-A products&lt;/a> is much more tepid:&lt;/p>
&lt;blockquote>
&lt;p>Design &amp; Durability: Water Resistant, Crush Resistant&lt;/p>
&lt;/blockquote>
&lt;p>Luckily, a few months after I bought the first one I bought a backup that I stored safely, so I wasn’t much put out. Here are the two keys side-by-side – click to see more detail than you need:&lt;/p>
&lt;a href="https://adam-p.ca/img/blog/yubikeys.jpg">
&lt;img src="https://adam-p.ca/img/blog/yubikeys.jpg" alt="two Yubikeys, one more worn than the other">
&lt;/a>
&lt;p>(After looking at that photo up close I realized that the problem could be that the two middle pins were touching. So I took a small screwdriver and carved them apart. And it works! But that doesn’t really undermine the point here, which is…)&lt;/p>
&lt;p>So make sure you have a backup Yubikey 2FA hardware token, or you could be very unhappy when it gets damaged beyond use. Then you can carry your day-to-day key in your pocket without worry.&lt;/p></description></item><item><title>Dev Story: Unicode URL length limit blues</title><link>https://adam-p.ca/blog/2021/06/unicode-url-length/</link><pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate><guid>https://adam-p.ca/blog/2021/06/unicode-url-length/</guid><description>&lt;p>I have enjoyed reading other people’s design and debugging train-of-thought posts, so after I spent two days wrestling with a code problem, I thought I’d write it up. It’s not technically exciting, but I think that describing it might be useful to someone – or my future self – someday. Or, at the very least, a little amusing.&lt;/p>
&lt;p>(Bonus: While writing this I discovered an error I made while doing the actual work. See if you can spot it before I reveal it…)&lt;/p>
&lt;h2 id="background">
&lt;a class="heading-anchor" href="#background">
##
&lt;/a>
Background
&lt;/h2>
&lt;h3 id="the-psiphon-for-windows-ui">
&lt;a class="heading-anchor" href="#the-psiphon-for-windows-ui">
###
&lt;/a>
The Psiphon for Windows UI
&lt;/h3>
&lt;p>Our &lt;a href="https://github.com/Psiphon-Inc/psiphon-windows">Psiphon for Windows&lt;/a> client uses an Internet Explorer-based HTML control as the GUI, talking to a C++ backend (which itself talks to the Go implementation of our censorship circumvention tech). It has been working reasonably well for the last 6 years. Before that we had a plain, grey-box, dialog-based win32 UI, but the idea of adding a settings UI and localizing everything prompted me to look for other approaches.&lt;/p>
&lt;p>We have strict executable size requirements, because many of our users have limited bandwidth, and because we run an auto-responder that emails our client software – the Windows and Android clients attached to a single email. That ruled out a lot of fancy UI approaches, but using the native web control ended up working okay. It meant supporting Internet Explorer 7 through 11 as the HTML/CSS/JS engine, so it wasn’t always a lot of fun, but manageable. (And Go recently dropped support for Windows XP, which meant we could raise our minimum IE version to 8, which helps.)&lt;/p>
&lt;p>For this story, the relevant part of the JS&lt;->C++ communication is that when the JavaScript wants the C++ to start a login sequence, it does something like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-js" data-lang="js">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">window&lt;/span>&lt;span style="color:#111">.&lt;/span>&lt;span style="color:#75af00">location&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#d88200">'psi:psicash?{"command":"login","username":"abc","password":"xyz"}'&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The C++ code gets a window message when the location is going to change and figures out what to do with the URL. (And has the ability to trigger JS functions.)&lt;/p>
&lt;h3 id="psicash-accounts">
&lt;a class="heading-anchor" href="#psicash-accounts">
###
&lt;/a>
PsiCash accounts
&lt;/h3>
&lt;p>Psiphon has an in-app credit system called PsiCash. Users get credit by buying it or by certain rewarded activities. They can spend PsiCash on Speed Boost, which uncaps the network speed and expands the allowed ports.&lt;/p>
&lt;p>For the last year I’ve been working on implementing PsiCash user accounts. Until now a user’s PsiCash credit has been tied to a device (or a device’s local storage), but accounts will let the user’s balance roam across devices, be restored after device failure, and so on.&lt;/p>
&lt;p>A PsiCash account has a username and password. The PsiCash server is the ultimate arbiter of what’s allowed for the username and password. We use the &lt;a href="https://www.rfc-editor.org/rfc/rfc8264.html">PRECIS spec&lt;/a> for what characters are allowed, which is a pretty wide set (which is important to this story).&lt;/p>
&lt;p>We wanted to let the usernames and passwords be pretty long, but we &lt;a href="https://dev.to/mitchpommers/password-max-length-limits-are-dumb-but-we-need-them-1dpp">needed to give them fixed limits&lt;/a>. We’re allowing 200 bytes for the username and 800 bytes for the password. For the widest UTF-8 characters, that’s 50 code points and 200 code points, respectively. (For now let’s say that a “code point” is basically what you think of as a “character” or “letter”, except not always.)&lt;/p>
&lt;p>So, the implementation of accounts is pretty far along at this point, and I’m writing up test cases, and I’m thinking, “I should double-check some of these tests…”&lt;/p>
&lt;h2 id="the-problems-begin">
&lt;a class="heading-anchor" href="#the-problems-begin">
##
&lt;/a>
The problems begin
&lt;/h2>
&lt;h3 id="too-many-bytes">
&lt;a class="heading-anchor" href="#too-many-bytes">
###
&lt;/a>
Too many bytes
&lt;/h3>
&lt;p>So I put a ton of letters into the username and password fields to see what will happen. And the JS &lt;code>window.onerror&lt;/code> handler catches this: &lt;strong>“The data area passed to a system call is too small”&lt;/strong>, and the C++ side doesn’t get the URL message.&lt;/p>
&lt;p>Googling for that error doesn’t help much. It can happen with &lt;a href="https://techcommunity.microsoft.com/t5/windows-dev-appconsult/desktop-bridge-8211-the-bridge-between-desktop-apps-and-the/ba-p/316488">Desktop Bridge&lt;/a> applications &lt;a href="https://support.microsoft.com/en-us/topic/kb4073393-fix-the-data-area-passed-to-a-system-call-is-too-small-error-when-you-start-a-desktop-bridge-application-on-a-sql-server-5ae0994d-023a-d32b-3aad-526500b53993">running on SQL Server&lt;/a>. There are hotfixes or Microsoft Management Console &lt;a href="https://www.minitool.com/news/the-data-area-passed-to-a-system-call-is-too-small.html">that can help&lt;/a>. None of those a) seem to apply, or b) are reasonable to ask our users to do.&lt;/p>
&lt;p>I found that passing 2020 or fewer bytes was fine, but passing 2022 or more bytes would trigger that error. But passing exactly 2021 bytes… was even worse. It would open a browser tab with a URL that started like &lt;code style="white-space:initial;word-break:break-all">res://ieframe.dll/unknownprotocol.htm#psi:psicash?%7B%22command%22%3A%22login%22%2C%22id%22%3A%22MC4yOTc5MjI5MTY4ODU3MjI4%22%2C%22password%22%3A%2201234567890…&lt;/code>. And there’s the user’s password in the browser address bar! (We’ll call this the “>2020 error”.)&lt;/p>
&lt;p>(This is surely due to &lt;a href="https://support.microsoft.com/en-us/topic/maximum-url-length-is-2-083-characters-in-internet-explorer-174e7c8a-6666-f4e0-6fd6-908b53c12246">IE having a URL length limit of 2048&lt;/a> for GET requests. I didn’t think of that at the time, and there was probably some URL overhead I wasn’t counting. Anyway, it doesn’t change the problem for me.)&lt;/p>
&lt;p>Throwing up obscure, non-actionable error messages is bad enough, but the browser tab thing is terrible. So this can’t be allowed to happen.&lt;/p>
&lt;h3 id="tangent-unicode">
&lt;a class="heading-anchor" href="#tangent-unicode">
###
&lt;/a>
Tangent: Unicode
&lt;/h3>
&lt;p>If you’re not familiar with Unicode and its encodings, you might want to &lt;a href="#unicode">skip down&lt;/a> and read the appendix about it. But here are some quick definitions of terms I’ll be using:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Unicode&lt;/strong>: The system of defining all the letters and characters and emoji and so on. Each Unicode entry has a 32-bit number assigned to it.&lt;/li>
&lt;li>&lt;strong>Code point&lt;/strong>: The 32-bit value that indicates a Unicode “character”.&lt;/li>
&lt;li>&lt;strong>UTF-8 and UTF-16&lt;/strong>: These are the common ways of actually encoding Unicode entries. UTF-8 uses between 1 and 4 single bytes, and UTF-16 uses 1 or 2 double bytes. JavaScript and Windows C++ (&lt;code>wchar_t&lt;/code>) use UTF-16. Almost everything else uses UTF-8.&lt;/li>
&lt;li>&lt;strong>Code unit&lt;/strong>: These are the individual chunks of an encoding – the single bytes of UTF-8 or the double bytes of UTF-16.&lt;/li>
&lt;/ul>
&lt;p>So a single Unicode code point may be encoded by up to 4 UTF-8 code units (4 bytes total) or 2 UTF-16 code units (4 bytes total).&lt;/p>
&lt;h3 id="limit-the-input">
&lt;a class="heading-anchor" href="#limit-the-input">
###
&lt;/a>
Limit the input
&lt;/h3>
&lt;p>I hadn’t been limiting the username and password input fields because it didn’t seem necessary, for reasons like:&lt;/p>
&lt;ul>
&lt;li>It’s important that the login interface allow at least as many characters as the server will allow in the creation of an account.&lt;/li>
&lt;li>The bytes vs graphemes distinction makes things a little murky.&lt;/li>
&lt;li>If we decided to raise the limit on the server side, it’d be nice if the clients just worked.&lt;/li>
&lt;li>If the user enters too many characters… Then they’re entering bad credentials, and that’s really up to them.&lt;/li>
&lt;/ul>
&lt;p>But allowing the user to hit the >2020 bytes error is unacceptable, so I needed to add input limiting. The &lt;code>&lt;input>&lt;/code> element’s &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Attributes/maxlength">&lt;code>maxlength&lt;/code> attribute&lt;/a> seemed like an easy-and-sufficient way to do that.&lt;/p>
&lt;p>&lt;code>maxlength&lt;/code> is supposed to limit by UTF-16 code &lt;em>unit&lt;/em>, and does so in modern browsers, which is kind of weird. That means that “🍕” and “𪘀” count as &lt;em>two&lt;/em> towards the length. Probably not coincidentally, that’s how JS’s &lt;code>String.length&lt;/code> works: &lt;code>"🍕".length === 2&lt;/code>. There’s no mention on MDN or CanIUse of IE deviating from this (that I can find), but it does – &lt;code>maxlength&lt;/code> counts code &lt;em>points&lt;/em>, not code &lt;em>units&lt;/em>. Which is great! It means I can set &lt;code>maxlength=50&lt;/code> and get the expected username limiting – no custom validators required.&lt;/p>
&lt;p>But it would still be nice to provide more space for input, because of the reasons I had for not limiting in the first place. So let’s do a little math:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-js" data-lang="js">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// We want to be sure we don't hit the ~2020 limit. Let's say 1900 is a safe maximum.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#75af00">absolute_byte_limit&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1900&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75af00">bytes_per_code_point&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">4&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75af00">allowed_code_points&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#75af00">absolute_byte_limit&lt;/span> &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#75af00">bytes_per_code_point&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">==>&lt;/span> &lt;span style="color:#ae81ff">475&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>So we have something like a 475 code point allowance to split between username and password. Let’s say 75 for the username and 400 for the password (we won’t be staying here, so it doesn’t really matter).&lt;/p>
&lt;h3 id="simple-change-quick-test-famous-last-words">
&lt;a class="heading-anchor" href="#simple-change-quick-test-famous-last-words">
###
&lt;/a>
Simple change, quick test (famous last words)
&lt;/h3>
&lt;p>So I use my numbered input of &lt;code>0123456789&lt;/code>, repeated, to fill the max lengths. Works as expected.&lt;/p>
&lt;p>Then I try with big long string of “𪘀” and hit the >2020 error. Ugh.&lt;/p>
&lt;p>The &lt;code>&lt;input maxlength="400">&lt;/code> limiter is working, so that’s not the problem.&lt;/p>
&lt;p>Inspecting the incoming URL on the C++ side reveals the problem: I’m calling &lt;code>encodeURIComponent&lt;/code> on the query parameters part (after the &lt;code>?&lt;/code>) of the URL. So each “𪘀” becomes “%F0%AA%98%80”. That means the 4 bytes of the UTF-8 (or UTF-16) code point becomes 12 bytes – there are 3 one-byte characters per byte of UTF-8.&lt;/p>
&lt;p>Let’s do the math again:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-js" data-lang="js">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75af00">absolute_byte_limit&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1900&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75af00">bytes_per_code_point&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">4&lt;/span> &lt;span style="color:#f92672">*&lt;/span> &lt;span style="color:#ae81ff">3&lt;/span> &lt;span style="color:#75715e">// 4x UTF-8 code units, 3 bytes per code unit
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#75af00">allowed_code_points&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#75af00">absolute_byte_limit&lt;/span> &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#75af00">bytes_per_code_point&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">==>&lt;/span> &lt;span style="color:#ae81ff">158.3&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>So… 158 allowed code points? But we need to allow at least 50+200 code points for the username+password. Time to advance to the next level of problem.&lt;/p>
&lt;p>(Disclosure: I have the juice to change the username and password limits. But I don’t want to and this seems like a weak reason to do so.)&lt;/p>
&lt;h3 id="the-best-encoding-is-no-or-little-encoding">
&lt;a class="heading-anchor" href="#the-best-encoding-is-no-or-little-encoding">
###
&lt;/a>
The best encoding is no (or little) encoding?
&lt;/h3>
&lt;p>The most obvious thing to try to alleviate the encoding bloat is to just remove &lt;code>encodeURIComponent&lt;/code>. And it works fine. It appears that the code points are going through as UTF-16 binary – taking up the minimum possible bytes – and both the JS and C++ sides were happy.&lt;/p>
&lt;p>Then I try a space in the password and it automatically gets encoded as &lt;code>%20&lt;/code> (the code point for the space character is &lt;code>U+0020&lt;/code> and so &lt;code>%20&lt;/code> is the URL-escaped UTF-8-encoded version of it).&lt;/p>
&lt;p>That’s a bit of a wrinkle, but fine. I could put the URL-decode call back into the C++ code. Except… what if there happens to also be the percent-and-two-numbers sequence naturally occurring in the password? We’ll unintentionally be altering it. For example: If the password is &lt;code>x%41y&lt;/code>, it would get URL-decoded to &lt;code>xAy&lt;/code>.&lt;/p>
&lt;p>After doing some research to satisfy myself that percent-encoding is the only thing going on in URLs, I decide that I only need to percent-encode the percent sign. So the password &lt;code>x y%20z&lt;/code> becomes &lt;code>x%20y%2520z&lt;/code> (&lt;code>%25&lt;/code> being the percent-encoded percent sign) in the URL. URL-decoding will reverse that value properly.&lt;/p>
&lt;p>So, yay? We’re back to a reasonable number of bytes. Even a password of 50 percent signs (the only thing getting escaped) will still only bloat to 150 bytes.&lt;/p>
&lt;h3 id="but-then-ie8-strikes">
&lt;a class="heading-anchor" href="#but-then-ie8-strikes">
###
&lt;/a>
But then IE8 strikes
&lt;/h3>
&lt;p>I do all my development on a Windows 10 machine, with IE 11. The installed IE version is what gets used for the web control in the app. IE 11 has a handy developer tool that lets you test in various even-older-IE modes. But it’s not always 100% accurate, so sometimes I fire up a &lt;a href="https://developer.microsoft.com/en-us/microsoft-edge/tools/vms/">Windows 7 VM with IE 8, 9, or 10&lt;/a> installed to test for real.&lt;/p>
&lt;p>The simple-ASCII-characters-only test works fine in the Win7+IE8 VM. The test with the maximum count of “𪘀” does not.&lt;/p>
&lt;p>Again, I inspect the URL coming into the backend (which is a bit harder now, because I don’t have a development environment in the VM). And it looks like IE8 is automatically encoding “𪘀” as &lt;code>\ud869\ude00&lt;/code> (the two-code-unit UTF-16 encoding). Which is &lt;em>again 12 bytes instead of 4&lt;/em>.&lt;/p>
&lt;p>I start getting pretty frustrated at this point.&lt;/p>
&lt;h3 id="when-all-else-fails-base64">
&lt;a class="heading-anchor" href="#when-all-else-fails-base64">
###
&lt;/a>
When all else fails, base64
&lt;/h3>
&lt;p>So how do I get only ASCII characters in the URL, without bloating by a factor of 3x? How about base64-encoding? That gives us ASCII and a size increase of 33%, which is tolerable.&lt;/p>
&lt;p>Let’s check the math:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-js" data-lang="js">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75af00">absolute_byte_limit&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1900&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75af00">bytes_per_code_point&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">4&lt;/span> &lt;span style="color:#75715e">// 2x UTF-16 code units
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#75af00">base64_bloat&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1.33&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75af00">allowed_code_points&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#75af00">absolute_byte_limit&lt;/span> &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#75af00">bytes_per_code_point&lt;/span> &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#75af00">base64_bloat&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">==>&lt;/span> &lt;span style="color:#ae81ff">357&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>That gives us an extra 100 code points to play with above our absolute limit of 50+200. Phew!&lt;/p>
&lt;p>So, I change the encoding to be &lt;code>btoa(JSON.stringify(payload))&lt;/code>, with appropriate decoding on the C++ side. Works as expected on Win10.&lt;/p>
&lt;p>Doesn’t work at all on Win7+IE8. Oh right, forgot, there is no &lt;code>btoa&lt;/code> in IE8. But we already have a polyfill for that, so I just swap it in and try again.&lt;/p>
&lt;p>And, again, the &lt;em>URL is still too long&lt;/em>. I’m losing my mind a little bit now.&lt;/p>
&lt;h3 id="re-polyfill-json">
&lt;a class="heading-anchor" href="#re-polyfill-json">
###
&lt;/a>
Re-polyfill JSON
&lt;/h3>
&lt;p>After weeping a little and doing some MessageBox-ing and digging, I realize that the culprit now is IE8’s &lt;code>JSON.stringify&lt;/code>.&lt;/p>
&lt;p>Here’s IE9, IE10, IE11, and every other browser:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-js" data-lang="js">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75af00">JSON&lt;/span>&lt;span style="color:#111">.&lt;/span>&lt;span style="color:#75af00">stringify&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">"𪘀"&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">==>&lt;/span> &lt;span style="color:#d88200">'"𪘀"'&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And here’s IE8:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-js" data-lang="js">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75af00">JSON&lt;/span>&lt;span style="color:#111">.&lt;/span>&lt;span style="color:#75af00">stringify&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">"𪘀"&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">==>&lt;/span> &lt;span style="color:#d88200">'"\ud869\ude00"'&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Well that’s JUST GREAT.&lt;/p>
&lt;p>I think for a few minutes about how encode objects without using JSON, but that’s dumb. And then I remember that, until recently, we used a &lt;a href="https://github.com/douglascrockford/JSON-js">JSON polyfill&lt;/a> because we still supported WinXP+IE7 (which doesn’t have JSON support). So I try out the polyfill code in the IE8 console and… it gives the desired output! Oh, thank goodness.&lt;/p>
&lt;p>So I modify the polyfill code to always replace the native JSON and conditionally include it for IE8.&lt;/p>
&lt;p>And test. And it works. Everywhere. For every input.&lt;/p>
&lt;h2 id="do-you-see-the-mistake-i-made">
&lt;a class="heading-anchor" href="#do-you-see-the-mistake-i-made">
##
&lt;/a>
Do you see the mistake I made?
&lt;/h2>
&lt;p>And later I decide that maybe this story would be amusing or educational for someone, so I should write up a blog post. And as I’m writing this blog post I realize that I got something wrong. This:&lt;/p>
&lt;blockquote>
&lt;p>it looks like IE8 is automatically encoding “𪘀” as &lt;code>\ud869\ude00&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>Nope. It was JSON doing that, not “automatic encoding”. Having encountered the space–>&lt;code>%20&lt;/code> automatic encoding, I think I was primed to lazily attribute more unexpected behaviour to magic.&lt;/p>
&lt;p>Now, with the JSON polyfill replacement, I could go back to just percent-encoding-percent and regain even more code point space in the URL.&lt;/p>
&lt;p>I don’t think I will, though. I can’t shake the question: “Is there anything besides percent-encoding that &lt;code>InternetCanonicalizeUrl(ICU_DECODE)&lt;/code> (the win32 URL decode function) will try to decode?” If there is, then some user’s password will be unusable, and it’ll be super hard to diagnose. At the bottom of every email we say, “Psiphon will never ask you for your password”, so we can’t possibly figure out what’s wrong with it!&lt;/p>
&lt;p>Fuzzing might be able to find other cases? Or maybe there’s source code for &lt;code>InternetCanonicalizeUrl&lt;/code> that I can inspect (and hope it’s the same across Win 7, 8, 8.1, and 10)? But I already spent &lt;em>way too long&lt;/em> on this and I can’t spend any more. Time to move on.&lt;/p>
&lt;h2 id="this-is-an-edited-rendition">
&lt;a class="heading-anchor" href="#this-is-an-edited-rendition">
##
&lt;/a>
This is an edited rendition
&lt;/h2>
&lt;p>This may read like a logical progression of problems, investigations, and (attempted) solutions, but it was so much messier than that. This was my primary task for &lt;em>two days&lt;/em> (not my &lt;em>only&lt;/em> task, but still).&lt;/p>
&lt;p>It was a painful cycle of:&lt;/p>
&lt;ol>
&lt;li>Think everything works.&lt;/li>
&lt;li>Test, expecting success.&lt;/li>
&lt;li>Get weird results.&lt;/li>
&lt;li>Debug, usually in VMs with the &lt;code>alert&lt;/code>-and-&lt;code>MessageBox&lt;/code> version of printfs.&lt;/li>
&lt;li>Search for explanations. Read MSDN pages, Wikipedia pages, and anything else that might make it make sense.&lt;/li>
&lt;li>Think of what to do to avoid the problem. Try stuff. Make it nominally work on Win10+IE11.&lt;/li>
&lt;li>Repeat.&lt;/li>
&lt;/ol>
&lt;p>(You know what didn’t help? Running out of disk space while trying to work with VMs.)&lt;/p>
&lt;p>There was also much, &lt;em>much&lt;/em> more profanity than I have allowed here.&lt;/p>
&lt;hr>
&lt;p>&lt;a name="unicode" href="#">&lt;/a>&lt;/p>
&lt;h2 id="appendix-unicode-stuff-as-i-know-it">
&lt;a class="heading-anchor" href="#appendix-unicode-stuff-as-i-know-it">
##
&lt;/a>
Appendix: Unicode stuff, as I know it
&lt;/h2>
&lt;p>I am not a Unicode pro, and quite a bit of what I know I learned during this work. I’ll give a quick-and-dirty description so we can be on the same page.&lt;/p>
&lt;p>So, Unicode is a big list of, like, letters and characters and stuff. Each one gets a 32-bit number assigned to it (although there are only 24-bits actually used). “A” is &lt;code>0x00000041&lt;/code> (65 in decimal), “あ” is &lt;code>0x00003042&lt;/code> (12354), “🍕” is &lt;code>0x0001F355&lt;/code> (127829), “𪘀” is &lt;code>0x0002A600&lt;/code> (173568). These numbers are often written like &lt;code>U+0041&lt;/code>, without so many leading zeros.&lt;/p>
&lt;p>Those 32-bit Unicode numbers are “code points”. Some represent “graphemes” (rendered entities), but some are accents and whatnot that are to be combined with other code points.&lt;/p>
&lt;p>(Tangent within a tangent: Some things that you might think of as a single “character”, like the “keycap digit one” emoji “1️⃣”, are actually “grapheme clusters”. In the case of “1️⃣”, it’s actually a combination of three code points: the usual ASCII number “1”, the “VARIATION SELECTOR-16” (&lt;code>U+FE0F&lt;/code>), and the “COMBINING ENCLOSING KEYCAP” (&lt;code>U+20E3&lt;/code>).)&lt;/p>
&lt;p>So, think of “Unicode” as an abstract list of code points. Then we need to actually encode those code points.&lt;/p>
&lt;p>The most direct approach is called UTF-32. It uses 32 bits to encode the 32-bit code point. But the vast majority of code points in common use don’t need all 32 bits, so this is a pretty inefficient encoding.&lt;/p>
&lt;p>The most common encoding is UTF-8. It uses between one and four 8-bit “code units” to encode a code point. It has the very nice property of encoding English letters, numbers, and punctuation exactly the same as they are in ASCII. If you’re encoding text, use UTF-8.&lt;/p>
&lt;p>But JavaScript and Windows use UTF-16. It uses one or two 16-bit code units to encode a code point. Some quick &lt;a href="https://en.wikipedia.org/wiki/UTF-16#History">Wikipedia reading&lt;/a> suggests that, once upon a time, 16 bits were thought to be enough for Unicode code points. JavaScript and Windows probably adopted that early (UCS2) spec, and then were stuck with it for legacy reasons. UTF-16 is compatible with that old spec, and here we are.&lt;/p></description></item><item><title>The short happy life of the Breached extension</title><link>https://adam-p.ca/blog/2019/05/breached/</link><pubDate>Mon, 27 May 2019 00:00:00 +0000</pubDate><guid>https://adam-p.ca/blog/2019/05/breached/</guid><description>&lt;p>In October 2017, Troy Hunt of &lt;a href="https://haveibeenpwned.com/">Have I Been Pwned&lt;/a> held a &lt;a href="https://www.troyhunt.com/do-something-awesome-with-have-i-been-pwned-and-win-a-lenovo-thinkpad/">contest&lt;/a> inviting people to do something cool with the HIBP API. I decided a) that I would kind of like the special edition ThinkPad he was giving away, and b) that I could probably whip something up pretty quickly.&lt;/p>
&lt;p>I decided to create a browser extension that would simply pull HIBP breach information and show a browser notification – with the ability to view extra info – when the user visited a site that had been breached. And so was born the &lt;a href="https://github.com/adam-p/breached">Breached extension&lt;/a>. (Spoiler: I didn’t win.)&lt;/p>
&lt;blockquote>
&lt;p>For non-technical readers: A “breach”, in this context, is when a hacker obtains the user database of a website. A breach generally includes email addresses, passwords (in some form), maybe credit cards, and other stuff you don’t want a hacker to have. So “breach information” about a website tells you that a breach occurred, when it happened, what data was stolen, and how much of it. Which is the kind of thing you should know about before using that website!&lt;/p>
&lt;/blockquote>
&lt;p>A month after I released the extension, someone created an issue pointing out that Mozilla &lt;a href="https://github.com/adam-p/breached/issues/5">“started working on integrating haveibeenpwned.com warnings into Firefox”&lt;/a>. (As I promised there I did (nominally) reach out, but it didn’t go anywhere.) A year later that project turned into &lt;a href="https://monitor.firefox.com/">Firefox Monitor&lt;/a>.&lt;/p>
&lt;p>So, Breached is basically redundant on Firefox. In a sense I’m bummed, but I’m more flattered – it was obviously a pretty okay idea!&lt;/p>
&lt;p>Monitor has a different &lt;a href="https://blog.mozilla.org/security/2018/11/14/when-does-firefox-alert-for-breached-sites/">notification policy&lt;/a>: They only show alerts for sites that have been breached within the last two months. (If I’m reading that right. I think the 12-month part is a one-off and is likely just to increase the number of people that will ever see a notification). Breached’s policy is… just, like, show them all. Because I didn’t think about that while coding it. I think Monitor’s time-limit is better, since many sites will have sorted themselves out and don’t deserve a black mark for all time, so I &lt;a href="https://github.com/adam-p/breached/issues/6">might also add a time limit&lt;/a>. (I don’t find the notifications noisy, but maybe I just don’t visit enough shady sites.)&lt;/p>
&lt;p>In case you’re wondering what Monitor notifications look like, here’s one:&lt;/p>
&lt;p class="text-center">
&lt;img src="https://adam-p.ca/img/blog/firefox-monitor-breach.png" alt="Firefox Monitor breach notification"
style="max-width: 600px;">
&lt;/p>
&lt;p>And here is Breached’s notification:&lt;/p>
&lt;p class="text-center">
&lt;img src="https://adam-p.ca/img/blog/breached-notification.png" alt="Breached's breach notification"
style="max-width: 400px;">
&lt;/p>
&lt;p>And the additional-detail popup:&lt;/p>
&lt;p class="text-center">
&lt;img src="https://adam-p.ca/img/blog/breached-popup.png" alt="Breached's additional detail popup">
&lt;/p>
&lt;p>Well, at least Breached is still relevant on Chrome! For now…&lt;/p></description></item><item><title>First Post: someone's last post</title><link>https://adam-p.ca/inco/2017/01/first-post/</link><pubDate>Thu, 05 Jan 2017 00:00:00 +0000</pubDate><guid>https://adam-p.ca/inco/2017/01/first-post/</guid><description>&lt;p>I often come across digital and physical interface designs that are either great (coherent) or terrible (incoherent). For some mysterious reason I’ve been wanting to capture such interfaces and share them.&lt;/p>
&lt;p>To start, a curious headstone.&lt;/p>
&lt;p>The goal of the design of a headstone is to be read by the living. For a few decades some of the people who read it will have known the deceased, but many won’t, and after a while no one will have personally known them. The design of this headstone ensures that it will get more reading-attention than an ordinary headstone. I certainly stood in front of it longer than any other, while figuring out how to decipher and read it.&lt;/p>
&lt;p>It’s just the right degree of indecipherable. You can instantly tell that there’s something there to read and that it probably won’t require you to bust out pen and paper to do frequency analysis or ROT-13. It’s just hard enough to keep you standing there for a few minutes, and no harder.&lt;/p>
&lt;p>&lt;img src="https://adam-p.ca/img/inco/headstone.jpg" alt="headstone">&lt;/p></description></item><item><title>Markdown Here: Splitting the Firefox and Thunderbird Extension</title><link>https://adam-p.ca/blog/2016/07/mdh-dividing-firefox-and-thunderbird/</link><pubDate>Thu, 14 Jul 2016 00:00:00 +0000</pubDate><guid>https://adam-p.ca/blog/2016/07/mdh-dividing-firefox-and-thunderbird/</guid><description>&lt;p>[This started as notes to myself to help clarify the problem and solution. It’s probably more suited to a Github issue than a blog post, and it may get copied into one.]&lt;/p>
&lt;h1 id="the-story-so-far">
&lt;a class="heading-anchor" href="#the-story-so-far">
#
&lt;/a>
The story so far
&lt;/h1>
&lt;p>The Firefox and Thunderbird versions of &lt;a href="https://markdown-here.com/">Markdown Here&lt;/a> both used nearly the same code – an old-style XUL extension. Tb is only capable of using a XUL extension, while Fx supports at least three extension types: &lt;a href="https://developer.mozilla.org/en-US/Add-ons/Overlay_Extensions/XUL_School">XUL-based&lt;/a>, &lt;a href="https://developer.mozilla.org/en-US/Add-ons/SDK">Add-on SDK&lt;/a> (aka Jetpack, aka jpm), and &lt;a href="https://developer.mozilla.org/en-US/Add-ons/WebExtensions">WebExtensions&lt;/a>. WebExtensions is the newest, and is essentially an implementation of Chrome’s extension API.&lt;/p>
&lt;p>I’ve kept my eye on WebExtensions because it’s tempting to be able to use identical code across the many major browsers: Chrome, Firefox, Opera, and Edge(?). It seemed premature to do any real work towards using it as it’s not yet fully released, and there would be no perceived benefit to users (it’s a more-locked-down extension API, so there would be an imperceptible sercurity benefit).&lt;/p>
&lt;p>But… Firefox’s multi-process &lt;a href="https://wiki.mozilla.org/Electrolysis">Electrolysis&lt;/a> (E10s) update is nearing release, and &lt;a href="https://github.com/adam-p/markdown-here/issues/207">it breaks MDH&lt;/a>. There are two ways to go about fixing this:&lt;/p>
&lt;ol>
&lt;li>Figure out what’s wrong in the XUL extension and correct it.&lt;/li>
&lt;li>Switch to WebExtensions.&lt;/li>
&lt;/ol>
&lt;p>I tried to figure out why E10s is breaking the XUL extension, but without any success. Maybe I could figure it out if I tried longer/harder? Or maybe not.&lt;/p>
&lt;p>I did some testing (and fixing) to make WebExtensions work, and it seems like the best option.&lt;/p>
&lt;h1 id="the-big-problem-splitting-up-fx-and-tb">
&lt;a class="heading-anchor" href="#the-big-problem-splitting-up-fx-and-tb">
#
&lt;/a>
The big problem: Splitting up Fx and Tb
&lt;/h1>
&lt;p>Until now, the Firefox and Thunderbird extensions were literally the same extension – I upload a single file to Mozilla and check off the “Firefox” and “Thunderbird” boxes. There is only one extension ID for both. But Thunderbird only supports XUL extensions, so it’ll probably be using the XUL version forever.&lt;/p>
&lt;p>We clearly have a problem: Fx and Tb will have to use fundamentally different extensions, but right now there’s only one extension for both. We’re going to have to split the userbase. And it’s going to be ugly.&lt;/p>
&lt;p>Specifically, it’s going to be very ugly for either the Firefox users &lt;em>or&lt;/em> the Thunderbird users. One platform will receive a message saying, “If you want Markdown Here to keep working for you, uninstall the one you have and go install this other one.” (The other platform will see no difference.) Ugh. That’s going to cost us some users for sure.&lt;/p>
&lt;p>According to the &lt;a href="https://addons.mozilla.org/en-US/firefox/addon/markdown-here/statistics/usage/applications/?last=30">Mozilla stats for MDH&lt;/a>, there are two-thirds as many Thunderbird daily users of MDH as Firefox daily users. That makes it somewhat preferable to make life difficult for Tb users rather than Fx users.&lt;/p>
&lt;p>However, I think it’s Firefox users who will have to jump through hoops. The XUL extension will still work in Firefox to a sufficient degree to show a message to the user. The WebExtensions extension will not work at all in Thunderbird. So existing Thunderbird users simply cannot be given the WebExtensions version, therefore the WebExtensions version must be the new, separate extension. Therefore it’s the Firefox users who must suffer.&lt;/p>
&lt;p>(A couple of caveats: 1. I have asked in the &lt;a href="https://discourse.mozilla-community.org/t/best-way-to-split-thunderbird-and-firefox-users-because-webextensions/9717">Mozilla extension dev forum&lt;/a> for suggestions for how to do this gracefully; so for I’ve received one reply, but it’s even more painful. 2. I have a terrible feeling that there actually is a graceful way to do this and I’m just not seeing it.)&lt;/p>
&lt;h1 id="the-plan-such-as-it-is">
&lt;a class="heading-anchor" href="#the-plan-such-as-it-is">
#
&lt;/a>
The plan, such as it is
&lt;/h1>
&lt;ol>
&lt;li>
&lt;p>Get the WebExtensions work done. (Most of the effort now is going to be building the separate versions in a sane way.)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Release the brand new WebExtensions version, with a separate ID. (And update the website to point to it, etc.)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Release a new XUL version that does nothing but show Firefox users a message telling them what they need to do to upgrade. (And explaining and apologizing profusely.)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Watch how many users are lost.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>The E10s rollout is supposed to be gradual, so I plan on continuing to support Firefox in the XUL version for the time being, and continuing to show the switch-message when Firefox is detected.&lt;/p>
&lt;p>&lt;strong>Outstanding question:&lt;/strong> Continue using Addons.Mozilla.Org (aka AMO, aka the main Firefox extension site) or self-host. I’ve gone through &lt;a href="https://github.com/adam-p/markdown-here/issues/21">ridiculous hassle&lt;/a> getting MDH approved in the past, and I don’t want to do it again. Ever. (This is exacerbated by the fact that I’m planning on replacing the Markdown rendering library, and I fear push-back from the AMO reviewers for using a lot of third-party code that’s not pre-approved.) The downside of self-hosting is that MDH won’t show up in AMO searches, which is surely where a lot of users go when looking for a Firefox extension (versus googling).&lt;/p>
&lt;p>(The current Firefox and Thunderbird extension is also used by Pale Moon, which is a Firefox fork, and Postbox and Ice Dove, which are Thunderbird forks. There aren’t enough users of them to change any of the rationale, and it &lt;a href="https://forum.palemoon.org/viewtopic.php?t=6660">looks like&lt;/a> Pale Moon &lt;a href="https://forum.palemoon.org/viewtopic.php?t=12216">won’t be using&lt;/a> E10s. So there’s Firefox, and then everything else.)&lt;/p></description></item><item><title>Android Non-Vulnerability: Steal a Device and Keep it Unlocked</title><link>https://adam-p.ca/blog/2016/05/smart-lock-mock-location/</link><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid>https://adam-p.ca/blog/2016/05/smart-lock-mock-location/</guid><description>&lt;p>While poking around in my Android phone’s developer options, I realized that &lt;strong>if you steal a phone that’s currently unlocked because it’s in a “trusted place”, then you can force it to remain unlocked forever&lt;/strong>. (And then I got schooled about that not being a problem.)&lt;/p>
&lt;h2 id="security-feature-smart-lock-with-trusted-places">
&lt;a class="heading-anchor" href="#security-feature-smart-lock-with-trusted-places">
##
&lt;/a>
Security Feature: Smart Lock with Trusted Places
&lt;/h2>
&lt;p>&lt;a href="https://support.google.com/nexus/answer/6093922?hl=en">Android’s Smart Lock&lt;/a> allows users to configure conditions under which to keep the phone unlocked. One of the conditions is location – you can set trusted locations where your phone shouldn’t prompt for a PIN/pattern/password when unlocking.&lt;/p>
&lt;p>This is a pretty great feature. It’s difficult to convince people that the security gained by using a PIN outweighs the inconvenience of constantly entering it. Smart Lock helps mitigate the inconvenience by not requiring the user to constantly enter the PIN at home or at the office.&lt;/p>
&lt;h2 id="developer-feature-mock-location">
&lt;a class="heading-anchor" href="#developer-feature-mock-location">
##
&lt;/a>
Developer Feature: Mock Location
&lt;/h2>
&lt;p>If you’re developing a location-aware app, you might want to trick the phone (and your app) into believing that it’s somewhere you’re not (in a restaurant, near a bus stop, etc.). The Android developer options provide the ability to set a “mock location app”. This is a separate app that allows you to configure your desired fake location. When the app is set as the mock location app, the phone pretends to be in the location specified by the app.&lt;/p>
&lt;h2 id="trusted-place--mock-location--perma-unlock">
&lt;a class="heading-anchor" href="#trusted-place--mock-location--perma-unlock">
##
&lt;/a>
Trusted Place + Mock Location = Perma-Unlock
&lt;/h2>
&lt;p>So if Eve steals Alice’s phone from her desk at work, and wants to keep it unlocked until she has more time to peruse it, she can do this:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Install a mock location app. Set it to the current location.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Enable Developer Options.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Select the mock location app in the developer options.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>And then Eve walks away, confident that Alice’s phone will remain unlocked.&lt;/p>
&lt;p>Note that none of those steps requires Eve to type in the phone’s PIN. If any of them did, this attack would be nullified. (Adding a Smart Lock trusted location does require a PIN, but fooling the phone into thinking that it’s always in the current trusted location doesn’t.)&lt;/p>
&lt;p>My recommended solution to the Android team: Require a PIN at step 3. If Alice is a developer, there might already be a mock location app installed and the phone will probably already have the developer options enabled. Step 3 seems like the best intervention point.&lt;/p>
&lt;p>I also don’t like that Smart Lock (and Android Device Manager) respect the mock location. It seems to me that they should be “above” that.&lt;/p>
&lt;h3 id="bonus-attack-android-device-manager">
&lt;a class="heading-anchor" href="#bonus-attack-android-device-manager">
###
&lt;/a>
Bonus attack: Android Device Manager
&lt;/h3>
&lt;p>Alice realizes that her phone is gone! She jumps on her computer and checks &lt;a href="https://support.google.com/accounts/answer/3265955?hl=en">Android Device Manager&lt;/a>! Except… it reports that the phone is still at the office, because it also uses the mock location being reported by the phone.&lt;/p>
&lt;p>Hopefully Alice will do a remote lock (or wipe) anyway, or maybe the location confusion slows her down for a while.&lt;/p>
&lt;h2 id="vulnerability-disclosure">
&lt;a class="heading-anchor" href="#vulnerability-disclosure">
##
&lt;/a>
Vulnerability Disclosure
&lt;/h2>
&lt;p>I filed a security issue with the Android team (&lt;a href="https://code.google.com/p/android/issues/detail?id=204776">#204776&lt;/a>, but it’s not publicly visible). The response was that it is “working as intended”.&lt;/p>
&lt;blockquote>
&lt;p>Once someone has access to an unlocked phone, they are able to do anything with it (attempt to root the device, install other malware, etc).&lt;/p>
&lt;p>We appreciate the report but this is working as intended.&lt;/p>
&lt;/blockquote>
&lt;p>I see what they’re saying. In theory, the attacker could enable app side-loading, and then install some kind of data-snarfer service, and then give it sufficient permission to exfiltrate everything it can access. The data-snarfer could run even while the phone is locked.&lt;/p>
&lt;p>Or the attacker could just keep touching the screen to keep it unlocked.&lt;/p>
&lt;p>(Rooting typically requires a bootloader unlock, which wipes the device. But that’s irrelevant if rooting isn’t necessary to effect an equivalent attack.)&lt;/p>
&lt;p>I’d really like to thank the Android team for taking the time to reply to my not-super-exciting bug report.&lt;/p>
&lt;h2 id="grand-conclusion">
&lt;a class="heading-anchor" href="#grand-conclusion">
##
&lt;/a>
Grand Conclusion
&lt;/h2>
&lt;p>There isn’t one. The “Trusted Place + Mock Location” combo isn’t a real problem – it’s just a little distasteful. If someone steals your phone and it’s unlocked, you’d better hope they’re only after the hardware.&lt;/p>
&lt;p>This is especially distressing for tablets. Unlike our phone, most of us don’t carry our tablet everywhere, so it’s much more likely to be stolen from our home. Entering your PIN on your tablet every time is almost as annoying as on your phone, so Smart Lock seems like a good choice. And that means that it’s almost certain that your tablet will be stolen unlocked. (If it gets stolen. I have no idea what the likelihood of that is.)&lt;/p>
&lt;hr>
&lt;h2 id="update">
&lt;a class="heading-anchor" href="#update">
##
&lt;/a>
Update
&lt;/h2>
&lt;p>In Android O (a year after I wrote this) Google &lt;a href="https://www.androidpolice.com/2017/06/01/android-o-feature-spotlight-enabling-developer-options-requires-devices-passcode/">added a PIN prompt&lt;/a> when enabling developer options. So, that negates this problem.&lt;/p>
&lt;p>Unless you’re already a developer…&lt;/p></description></item><item><title>Why and How to Use a Contributor License Agreement</title><link>https://adam-p.ca/blog/2015/02/contributor-license-agreement/</link><pubDate>Tue, 17 Feb 2015 00:00:00 +0000</pubDate><guid>https://adam-p.ca/blog/2015/02/contributor-license-agreement/</guid><description>
&lt;h2 id="background-and-motivation">
&lt;a class="heading-anchor" href="#background-and-motivation">
##
&lt;/a>
Background and Motivation
&lt;/h2>
&lt;p>I received a &lt;a href="https://github.com/adam-p/markdown-here/pull/232">pull request&lt;/a> for Markdown Here that was great: it found a bug, fixed it, and included tests for the fix. However, the PR submitter didn’t write the tests using the existing framework, so I figured I’d massage his test code into the proper form.&lt;/p>
&lt;p>And then I noticed that he included a &lt;a href="https://github.com/adam-p/markdown-here/commit/52cd013413ff4645ed124cef68b5fc9044d65a96#diff-555e8e637d661924e36cdddfba81a23aR9">copyright line&lt;/a> in the test file. It says “MIT License”, which is the license used for the rest of the project, but that got me thinking about what that might mean…&lt;/p>
&lt;p>Wikipedia suggests that the &lt;a href="https://en.wikipedia.org/wiki/MIT_License">MIT License&lt;/a> would require me to include his copyright+license notice wherever I use his code. Not a big deal, but annoying. And maybe a slippery slope – what if I get a bunch more code submissions?&lt;/p>
&lt;p>So I did some research into “Contributor License Agreements” and found that there are a couple more things to be concerned about:&lt;/p>
&lt;p>If you ever want to change the project license, you have to get the agreement of all contributors. That includes dual-licensing. Good info about that (and CLAs in general), with specific KDE example: &lt;a href="https://julien.ponge.org/blog/in-defense-of-contributor-license-agreements/">“In Defense of License Agreements”&lt;/a>.&lt;/p>
&lt;p>Patent something something. Contributors retain the patent rights unless explicitly granted in the CLA.&lt;/p>
&lt;p>And just to be clear: Contributors have the copyright on their code &lt;em>by default&lt;/em>, regardless of whether they put the © notice on it. I’m not sure about the license.&lt;/p>
&lt;p>So I decided to require MDH contributors to “sign” a CLA. Seemed kind of irresponsible not to.&lt;/p>
&lt;h2 id="picking-a-cla">
&lt;a class="heading-anchor" href="#picking-a-cla">
##
&lt;/a>
Picking a CLA
&lt;/h2>
&lt;p>Spending your weekend reading CLAs is a drag.&lt;/p>
&lt;p>I decided to use &lt;a href="http://www.harmonyagreements.org/">Harmony Agreements&lt;/a> to &lt;a href="http://selector.harmonyagreements.org/">generate&lt;/a> the agreement. (I chose “any license” for the “outbound license option”.) The agreement it provides seems pretty good and pretty standard.&lt;/p>
&lt;p>(Well… when I first generated a agreement I chose the “copyright assignment” version instead of “copyright license”. I even committed it and got the pull-request submitter to sign it. But then I re-read it and realized it was a) not very standard, b) maybe not enforceable, and c) kind of heinous. So I changed to the “copyright license” form. The difference is something like “you’re transferring absolute power to me and you lose the ability to use your own code” versus “you’re letting me do whatever I want with your code but it’s still yours” – exclusive vs. non-exclusive license, kind of thing.)&lt;/p>
&lt;h2 id="signing">
&lt;a class="heading-anchor" href="#signing">
##
&lt;/a>
“Signing”
&lt;/h2>
&lt;p>Ugh.&lt;/p>
&lt;p>Some projects – like Apache – require you to print out the agreement, sign it, and mail, fax, or scan-and-email it back to them. Another one (I forget which) uses some Adobe e-signing plugin where you draw your signature on the screen. Google requires you to be signed in, but it’s just a button press; ditto Twitter (signed in with Twitter, I mean). &lt;a href="https://www.clahub.com/">CLAHub&lt;/a> requires a Github sign-in (with optional typing of “I AGREE”). I think I also saw some projects that just require a filled-in form.&lt;/p>
&lt;p>CLAHub is really cool. It gives you a nice link for your CLA, collects agreements, and runs a bot that watches your project’s PRs, checks against the CLAs, and comments on the PR as to whether the PR-user has agreed yet or not. Except… there’s a notice saying it’s not ready for prime-time, the bot is broken, and the blog hasn’t been updated in a year. Also, entrusting a (flaky?) third party with the agreements seems dangerous (although they can be downloaded, so blah).&lt;/p>
&lt;p>The article I linked above mentions “some [projects] collect agreements through a simple web form (Google Doc is a fine choice)”. So I created a Google Form with the CLA and a form for the contributor to provide contact info and indicate agreement. But…&lt;/p>
&lt;p>Then I started thinking about non-repudiability. So I did a little reading about &lt;a href="https://en.wikipedia.org/wiki/Electronic_signature">electronic signatures&lt;/a> (not to be confused with digital signatures). Which is a horrible rabbit hole. Anyway, it made me seriously doubt that a row in a Google Spreadsheet with someone’s maybe-correct address and the words “I AGREE” really constitutes a legal signature. (Even in the Google and Twitter cases – how would they prove that they didn’t just fiddle some bits to make it look like I signed?)&lt;/p>
&lt;p>And if you’re going to do this annoying CLA crap then you might as well hope that it means something, right?&lt;/p>
&lt;p>Then I found &lt;a href="https://github.com/Medium/opensource/blob/master/sign-cla.md">Medium’s open source project&lt;/a>. It requires contributors to commit a file along with their pull request stating that they agree to the CLA. That seemed… totally reasonable. The agreement is in-band with the code. In a way that I can’t manipulate without invalidating. And much, much less annoying for the contributor than printing/signing/faxing.&lt;/p>
&lt;p>So that’s how I did it. See MDH’s &lt;a href="https://github.com/adam-p/markdown-here/blob/master/CONTRIBUTING.md#contributor-license-agreement">CONTRIBUTING.md&lt;/a>. (That’s the &lt;a href="https://github.com/blog/1184-contributing-guidelines">filename to use&lt;/a>.)&lt;/p>
&lt;h2 id="backlash-warning">
&lt;a class="heading-anchor" href="#backlash-warning">
##
&lt;/a>
Backlash warning
&lt;/h2>
&lt;p>Doing the CLA read-and-agree dance is more effort than not doing it. And some people are offended at the idea of doing it (see the tweet at the top of the “In Defense Of” post.) So, it seems unavoidable that a project with a CLA will get fewer contributions than one without – some people just won’t get past that hurdle.&lt;/p>
&lt;p>But it still seems necessary.&lt;/p>
&lt;hr>
&lt;p>[Note: This is actually an email I wrote to my co-workers after going through this CLA exercise with Markdown Here. That’s why the tone is a bit informal and “blah” is used as if it means something.]&lt;/p></description></item><item><title>Test post: Markdown Here in Disqus</title><link>https://adam-p.ca/blog/2013/10/mdh-disqus-test/</link><pubDate>Fri, 18 Oct 2013 00:00:00 +0000</pubDate><guid>https://adam-p.ca/blog/2013/10/mdh-disqus-test/</guid><description>&lt;p>This is just a stub test post to allow me to try out Markdown Here in Disqus comments.&lt;/p>
&lt;p>Right now MDH won’t work with Disqus in Chrome because of cross-origin restrictions. See: &lt;a href="https://github.com/adam-p/markdown-here/issues/124">https://github.com/adam-p/markdown-here/issues/124&lt;/a>&lt;/p>
&lt;p>&lt;strong>Update&lt;/strong>: The Disqus edit box is &lt;code>contenteditable&lt;/code>, and MDH will render in it, but all formatting seems to get stripped out when you actually post the comment. Seems like the rich-edit-ness is probably just to support Disqus’s add-an-image feature.&lt;/p>
&lt;p>&lt;strong>Final update&lt;/strong>: I’m not longer using Disqus, so there’s nothing here to try. Sorry!&lt;/p></description></item><item><title>Safari Extensions Gallery: half-baked</title><link>https://adam-p.ca/blog/2013/06/safari-extensions-gallery-half-baked/</link><pubDate>Tue, 25 Jun 2013 00:00:00 +0000</pubDate><guid>https://adam-p.ca/blog/2013/06/safari-extensions-gallery-half-baked/</guid><description>&lt;p>Trying to get &lt;a href="https://markdown-here.com">Markdown Here&lt;/a> listed in the &lt;a href="https://extensions.apple.com">Safari Extensions Gallery&lt;/a> is by far the worst browser extension “store” experience I’ve had so far. Shockingly bad.&lt;/p>
&lt;h2 id="no-hosting">
&lt;a class="heading-anchor" href="#no-hosting">
##
&lt;/a>
No hosting
&lt;/h2>
&lt;p>First of all, but least of all: There’s no hosting. Unlike the Chrome and Mozilla stores, the Safari store doesn’t host the extension for you – it’s really more of a listing of links to wherever you host your extension files. That’s not terrible, but:&lt;/p>
&lt;ul>
&lt;li>It’s costing me a little bit of money each month to host them.&lt;/li>
&lt;li>I don’t get nice install/usage stats like I do with Chrome and &lt;a href="https://addons.mozilla.org/en-US/firefox/addon/markdown-here/statistics/?last=365">Mozilla&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="no-communication">
&lt;a class="heading-anchor" href="#no-communication">
##
&lt;/a>
No communication
&lt;/h2>
&lt;p>Submitting the extension was basically the same as everywhere else. But this is the confirmation email:&lt;/p>
&lt;blockquote>
&lt;p>Dear Adam Pritchard,
Thank you for submitting your Safari Extension.
Apple reviews all submissions and reserves the right to omit, edit, or reject any submission. Please note you will not receive any further notifications.
We appreciate your interest in Safari.
Sincerely,
Apple Developer&lt;/p>
&lt;/blockquote>
&lt;p>(From &lt;a href="mailto:noreply@adc.apple.com">noreply@adc.apple.com&lt;/a>)&lt;/p>
&lt;p>So… You’re going to review my extension, and I’ll never know if it’s been accepted or rejected or what? And there’s no mechanism for me to get an update?&lt;/p>
&lt;p>Sure enough, 5 weeks has gone by now. There’ve been no status update emails, and there’s no review status info anywhere on the developer website (that I can find). And I’m not the only one – there are &lt;a href="https://devforums.apple.com/thread/182373?tstart=0">other people&lt;/a> on the forums in the &lt;a href="https://devforums.apple.com/thread/187144?tstart=0">same boat&lt;/a>.&lt;/p>
&lt;p>To be clear, this is not at all what the Chrome and Mozilla extension approval process is like. I’ve had my &lt;a href="https://github.com/adam-p/markdown-here/issues/21">fair share of problems&lt;/a> with the Mozilla approval process, but I had a queue number, an IRC channel, and reviewers I could communicate with. (The Chrome store has no apparent approval process, because I’m using the standard API. Which is similar to the Safari extension API I’m using. So… why is there any non-negligible review at all?)&lt;/p>
&lt;h2 id="no-search">
&lt;a class="heading-anchor" href="#no-search">
##
&lt;/a>
No search
&lt;/h2>
&lt;p>To top it off, the Safari Extensions Gallery itself… has no search?!? Let’s pick a not-front-page extension at random… how about the “Entertainment” category and then the Turboglue extension. (Sorry, there’s no way to give you a link to that!). Now try to find it some other way. The search box in the upper-right of that page? “No results were found.” And… I can’t find another search box. (Unlike Firefox, there’s no in-browser extension search.) How about a Google site search? That wouldn’t really be an acceptable work-around even if it worked… but &lt;a href="https://www.google.com/search?q=site%3Aextensions.apple.com+Turboglue">it doesn’t anwyay&lt;/a>.&lt;/p>
&lt;p>(Maybe related: Here’s a &lt;a href="https://devforums.apple.com/thread/179972">forum post&lt;/a> by a dev who has had his/her extension approved, but can’t actually find it in the Gallery.)&lt;/p>
&lt;p>I’m not sure how this could be worse. Unfriendly to developers &lt;em>and&lt;/em> unfriendly to users. And it’s not like Safari extensions are &lt;em>new&lt;/em> – they’ve been around about the &lt;a href="https://en.wikipedia.org/wiki/Safari_%28web_browser%29#Safari_5">same amount of time&lt;/a> &lt;a href="https://en.wikipedia.org/wiki/Google_Chrome#Chrome_Web_Store">as Chrome extensions&lt;/a>.&lt;/p>
&lt;p>&lt;img src="https://adam-p.ca/img/blog/safari-gallery-new.png" alt="Safari Extensions Gallery still marked as new" title="Safari Extensions Gallery still marked as new">&lt;/p>
&lt;p>(Screen-grabbed from the bottom of &lt;a href="https://developer.apple.com/programs/safari/">this page&lt;/a>. Age of the Gallery derived from &lt;a href="https://lifehacker.com/5598524/whats-useful-in-the-safari-extensions-gallery">this article&lt;/a>.)&lt;/p>
&lt;h2 id="sweet-lemons">
&lt;a class="heading-anchor" href="#sweet-lemons">
##
&lt;/a>
Sweet lemons
&lt;/h2>
&lt;p>Safari extensions are clearly not Apple’s primary concern, and maybe that’s okay. It’s not a hardcore-extension-geek browser like Firefox; their browser isn’t also an OS, unlike Chrome; and they have guaranteed, bundled-with-OS market share, like Internet Explorer (I don’t know much about IE’s extension support, but it’s clearly not as robust as Firefox and Chrome). And they do give me a way to provide a &lt;a href="https://markdown-here.com/get.html">Safari extension to my users&lt;/a> and update it automatically.&lt;/p>
&lt;hr>
&lt;h2 id="bonus-whinging">
&lt;a class="heading-anchor" href="#bonus-whinging">
##
&lt;/a>
Bonus whinging
&lt;/h2>
&lt;p>It’s necessary to generate and register a signing certificate before you can even start to develop a Safari extension (IIRC – but definitely before publishing). There’s no such stumbling block in Firefox and Chrome. Maybe this is due to the absence of hosting? I’m not entirely sure how painful it’s going to be to set up a new OS X development machine.&lt;/p>
&lt;p>Check out the ghetto method of opening the Markdown Here options page in Safari. It’s probably not &lt;em>just&lt;/em> my dumbness, since I stole the approach from AdBlock.&lt;/p>
&lt;p>&lt;img src="https://adam-p.ca/img/blog/safari-mdh-prefs-checkbox.png" alt="Markdown Here prefs in Safari">&lt;/p>
&lt;hr>
&lt;h2 id="update-accepted-6-months-later">
&lt;a class="heading-anchor" href="#update-accepted-6-months-later">
##
&lt;/a>
Update: Accepted 6 months later
&lt;/h2>
&lt;p>Two days ago (2013-11-04) I received an email indicating that Markdown Here had been accepted into the Safari Extensions Gallery. Phew!&lt;/p>
&lt;p>I think the acceptance makes the whole thing even weirder. If they were just sending submissions directly to the trash I could understand, but… Complete silence and then acceptance after 6 months? Are they backed up half a year? Is my extension so complex that it took them that long to review? Wha?&lt;/p></description></item><item><title>No One Knows to Click on a Page Action</title><link>https://adam-p.ca/blog/2013/03/pageaction-interaction/</link><pubDate>Sun, 24 Mar 2013 00:00:00 +0000</pubDate><guid>https://adam-p.ca/blog/2013/03/pageaction-interaction/</guid><description>&lt;p>&lt;strong>Page actions&lt;/strong> – the buttons in a browser’s address bar – are a &lt;strong>surprising UI failure&lt;/strong>.&lt;/p>
&lt;p>When adding a button for a browser extension, a choice must be made whether to make it a “page action” or a “browser action” (button on the toolbar). But &lt;strong>browsers have failed to communicate the interactiveness&lt;/strong> of page actions, and &lt;strong>almost no one – techy or layman – realizes that they’re clickable&lt;/strong>.&lt;/p>
&lt;hr>
&lt;p>To complement the context menu item and hotkey, and to fulfil &lt;a href="https://github.com/adam-p/markdown-here/issues/34">a user feature request&lt;/a>, I decided to add a button to the &lt;a href="https://www.markdown-here.com">&lt;strong>Markdown Here&lt;/strong>&lt;/a> browser extension. It turned out that simply deciding &lt;em>where&lt;/em> to put the button was a big part of the effort…&lt;/p>
&lt;h2 id="page-action-vs-browser-action">
&lt;a class="heading-anchor" href="#page-action-vs-browser-action">
##
&lt;/a>
Page Action vs. Browser Action
&lt;/h2>
&lt;p>I’m going to use the Chrome extension development terminology:&lt;/p>
&lt;dl>
&lt;dt>Page actions...&lt;/dt>
&lt;dd>are the buttons and status indicators located in the address/omni/awesome bar. (&lt;a href="https://developer.chrome.com/extensions/pageAction.html">See &lt;code>pageAction&lt;/code> API info&lt;/a>.)&lt;/dd>
&lt;dt>Browser actions...&lt;/dt>
&lt;dd>are buttons on the browser toolbar. (&lt;a href="https://developer.chrome.com/extensions/browserAction.html">See &lt;code>browserAction&lt;/code> API info&lt;/a>.)&lt;/dd>
&lt;/dl>
&lt;p>&lt;img src="https://adam-p.ca/img/blog/firefox-button.png" alt="Firefox page and browser buttons">&lt;/p>
&lt;p>In the screenshot above you can see the two styles co-existing in Firefox, which suggests there’s no real implementation decision to make – just provide both, and let the user decide which style they like. That’s true in Firefox (although there’s still the lesser decision of whether or not to add the toolbar button by default), but in Chrome you can either have a page action &lt;em>or&lt;/em> a browser action, not both.&lt;/p>
&lt;p>The choice initially seemed pretty obvious: use a page action. From Chrome’s &lt;a href="https://developer.chrome.com/extensions/browserAction.html#tips">documentation for browser actions&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>Don’t use browser actions for features that make sense for only a few pages. Use page actions instead.&lt;/p>
&lt;/blockquote>
&lt;p>Markdown Here’s button is only applicable to some rich-edit compose elements (email, mostly), so that admonition seems to apply pretty directly. Like many people, I don’t like occasional-use buttons cluttering up my toolbar, so I initially implemented the button as a page action.&lt;/p>
&lt;h2 id="apparently-imperceptible-affordance">
&lt;a class="heading-anchor" href="#apparently-imperceptible-affordance">
##
&lt;/a>
Apparently Imperceptible Affordance
&lt;/h2>
&lt;p>…And then I showed the cool new button to my significant other, who said something along the lines of “I can click that?” Which is a pretty damning statement, for a button.&lt;/p>
&lt;p>I must admit that I had some suspicions about the obviousness of page actions’ clickability. I’m fairly sure it took me a while to realize I could click them, and I’m a) pretty technically savvy, and b) pretty hover-over-everything-that-looks-interesting curious. But what if a user is &lt;em>not&lt;/em> both of those things…?&lt;/p>
&lt;p>So I asked around. I asked in the &lt;a href="https://groups.google.com/forum/#!topic/markdown-here/NjQRYcD1mgY/discussion">Markdown Here Google Group&lt;/a>, the &lt;a href="https://ux.stackexchange.com/questions/33987/browser-extensions-page-action-or-browser-action">UX StackExchange&lt;/a>, and on &lt;a href="https://plus.google.com/u/0/112228900913862544865/posts/9HbUjid2UvV">Google+&lt;/a>. These are the sorts of responses I got:&lt;/p>
&lt;ul>
&lt;li>“This [is] purely anecdotal, but I work in the web industry, and use [C]hrome everyday, and didn’t realise the page actions were clickable. I agree with you that they look more like signifiers than they do clickable buttons.”&lt;/li>
&lt;li>“But I agree that they don’t function well as buttons, perhaps this is by the design of the icon (not “raising” the element to give it depth).”&lt;/li>
&lt;li>“pageAction in the abstract is a great idea, but I always find its use a little jarring. And I agree it’s not button-like at all, more just informational.”&lt;/li>
&lt;/ul>
&lt;p>(Yes, there were some people who knew that page actions are clickable. But the fact that many computer/tech/web/UX-savvy people &lt;em>didn’t&lt;/em> know is the more significant observation.)&lt;/p>
&lt;p>I also asked around among people at the office (coders) and among non-programmer friends, and the vast majority of both groups didn’t know they could interact with page actions. At best they thought of them as status indicators, and at worst they couldn’t remember ever having noticed them before. &lt;em>Ugh&lt;/em>.&lt;/p>
&lt;h3 id="missing-cues">
&lt;a class="heading-anchor" href="#missing-cues">
###
&lt;/a>
Missing Cues
&lt;/h3>
&lt;p>It’s hard to blame users for this lack of &lt;a href="https://en.wikipedia.org/wiki/Affordance">affordance&lt;/a> recognition. At least, not yet.&lt;/p>
&lt;p>Page actions do not display any of the typical this-is-a-clickable-thing traits. For the most part, page actions:&lt;/p>
&lt;ul>
&lt;li>are not raised or underlined, like a standard button or a link, so most people won’t hover over them, but even if the user does hover, page actions…&lt;/li>
&lt;li>do not change at all when hovered over – no outline, no colour change, no raise-up, no clicky-hand mouse cursor.&lt;/li>
&lt;/ul>
&lt;p>Some page actions have a verb-based tooltip if you hover long enough. &lt;em>Some&lt;/em>. &lt;em>If&lt;/em>. &lt;em>Long enough&lt;/em>.&lt;/p>
&lt;p>It’s a little shocking how poorly the interactiveness is communicated to the user.&lt;/p>
&lt;h3 id="maybe-our-future-selves-will-get-it">
&lt;a class="heading-anchor" href="#maybe-our-future-selves-will-get-it">
###
&lt;/a>
Maybe our future selves will get it?
&lt;/h3>
&lt;!-- dropping into HTML to float the image, since it's so vertical -->
&lt;a href="https://adam-p.ca/img/blog/windows8-clickable.png">
&lt;img src="https://adam-p.ca/img/blog/windows8-clickable.png" alt="Windows 8 clickable text" class="pull-right" style="max-height: 20em; margin-left: 2em;">
&lt;/a>
&lt;p>Above I coyly dropped “At least, not yet.” There is a trend in UI design toward everything on-screen being interactive unless explicitly disabled-looking. Windows 8 has gone this way, as has Chrome and, to a slightly lesser extent, Firefox. There’s very, very little text or window chrome that’s non-interactive.&lt;/p>
&lt;p>But even if you accept the “everything is interactive” ideal, page actions are still different than most other elements, since there’s no hover effect. And page actions are further hampered by the minimalistic design aesthetic that Chrome and Firefox seem to have adopted for them – a monochrome outline icon that can easily be read as disabled.&lt;/p>
&lt;p>Maybe once users have fully embraced/internalized the idea that there are no extraneous UI elements, they won’t need hover effects and raised borders. Maybe there’ll be a great awakening to the utility of page actions. But until then…&lt;/p>
&lt;h2 id="how-to-rescue-page-actions">
&lt;a class="heading-anchor" href="#how-to-rescue-page-actions">
##
&lt;/a>
How to rescue page actions
&lt;/h2>
&lt;p>Page actions need to look less like small, monochrome, passive, static icons. They need some standard button cues, both initially and on hover; they should employ one or more of: raisèd-ness, colour, border, more visual strength.&lt;/p>
&lt;p>(I suspect that even the Chrome-style toolbar buttons – like the three-line settings button – are also below most laypeople’s threshold to recognize the click affordance. I’ve seen that in action in my own family-tech-support experience. Those buttons also lack most historical click cues. But let’s tilt at one windmill at a time…)&lt;/p>
&lt;h3 id="tangent-chrome-needs-to-allow-both-page-and-browser-actions">
&lt;a class="heading-anchor" href="#tangent-chrome-needs-to-allow-both-page-and-browser-actions">
###
&lt;/a>
Tangent: Chrome needs to allow both page and browser actions
&lt;/h3>
&lt;p>Finally, Chrome should allow extensions to provide both page actions and browser actions.&lt;/p>
&lt;p>In the screenshot at the top of this post, you see can that &lt;a href="https://getpocket.com/">Pocket’s&lt;/a> Firefox extension uses both button styles: the page action is for saving the current page, while the browser action is for showing your saved pages. Similarly for the bookmarks buttons: page action for bookmarking the page, browser action for viewing bookmarks.&lt;/p>
&lt;p>(Markdown Here also has a button in each place, but it’s not as compelling a use case, since it’s just a convenience to work around the page action affordance opacity. Both buttons toggle Markdown rendering; the page action only shows when focus is in a valid target; you can hide the toolbar button if you’re one of the few page-action-savvy users. But, still, I wish I could provide the same flexibility to my Chrome users that I do to my Firefox users.)&lt;/p>
&lt;p>In Chrome, Pocket only has a browser action (which, oddly enough, acts only like its Firefox page action), and bookmarks only have a page action (and a whole toolbar). I can’t think of any reason for Chrome to prevent extensions from providing both, and there are certainly good use cases for allowing them.&lt;/p>
&lt;h2 id="so-its-back-to-a-browser-action">
&lt;a class="heading-anchor" href="#so-its-back-to-a-browser-action">
##
&lt;/a>
So it’s back to a browser action
&lt;/h2>
&lt;p>I finally switched the Markdown Here toggle button in Chrome to be a browser action. Even though it clearly, spiritually, should be a page action, I just can’t ignore the fact that most users will not recognize it as clickable in that form.&lt;/p>
&lt;p>I have had &lt;a href="https://github.com/adam-p/markdown-here/issues/45">one complaint&lt;/a> about the button location, but the user seemed satisfied that I made the rational choice after I explained it.&lt;/p>
&lt;hr>
&lt;a name="update-20171028"/>
&lt;h2 id="update-2017-10-28">
&lt;a class="heading-anchor" href="#update-2017-10-28">
##
&lt;/a>
Update 2017-10-28
&lt;/h2>
&lt;p>Last year &lt;strong>Chrome turned page actions into browser actions&lt;/strong>. The &lt;a href="https://productforums.google.com/d/msg/chrome/wOUFbsKqPg0/K9FBzJh7BAAJ">description of the change&lt;/a> suggests that this was a security decision. Evil stealth extensions were being installed, so now all extensions have to have toolbar buttons to expose them to users. (FWIW, I don’t think this is a good solution to the problem. I’m sure the majority of users don’t pay much attention to random toolbar buttons.) It makes little sense for page action-centric extensions to have both a sometimes-visible address bar button and an always-visible toolbar button, so the Chrome team did away with the address bar button.&lt;/p>
&lt;p>Firefox hasn’t made a similar change. You’d think that if there’s solid security rationale for one browser, that it’d apply to all browsers.&lt;/p>
&lt;p>This makes the documentation for &lt;code>pageAction&lt;/code> pretty confusing. Check out &lt;a href="https://developer.chrome.com/extensions/pageAction">Chrome’s&lt;/a> versus &lt;a href="https://developer.mozilla.org/en-US/Add-ons/WebExtensions/API/pageAction">Mozilla’s&lt;/a> documentation. Chrome’s documentation is kind of incoherent now. There seems to be no reason at all to use page actions, but the doc still suggests that you do.&lt;/p>
&lt;p>I felt that page actions were kind of unusable before, but… this is worse?&lt;/p>
&lt;p>The reason I was looking at this again is because I created another extension, called &lt;a href="https://github.com/adam-p/breached">Breached&lt;/a>, where a page action would, in theory, make the most sense. The button is enabled/shown when the user visits a site that has suffered a breach in the past, exposing user accounts. This is a pretty rare thing (for many people… who don’t use Yahoo Mail), so dedicating a perma-visible toolbar button to it is pretty wasteful. Still, I initially went with a browser action for the reasons given in this post. I even added a notification (shown once per site), so the user could hide the toolbar button and still notice when they visit a breached site.&lt;/p>
&lt;p>Then a &lt;a href="https://addons.mozilla.org/en-US/firefox/addon/breached/reviews/940746/">reviewer&lt;/a> requested that it be a page action. I thought about it again and realized that the notification also helps with the “no one knows to click on a page action” problem by telling the user they can click on it. And the always-visible button still bugged me (and, let’s face it, most users won’t know how to hide buttons). So I changed it to a page action. And… discovered that there’s no visible difference in Chrome. Except page actions don’t support badge text, which I was using to show the number of breaches.&lt;/p>
&lt;p>Anyway, I’m leaving Breached as a page action, for the sake of Firefox users.&lt;/p>
&lt;hr>
&lt;p>Postscript: First blog post ever! Yay! Thanks to &lt;a href="https://caseywatts.github.com/">Casey Watts&lt;/a> for suggesting that I write it.&lt;/p></description></item></channel></rss>